<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Model fitting | Manual for building tree volume and biomass allometric equations</title>
  <meta name="description" content="6 Model fitting | Manual for building tree volume and biomass allometric equations" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Model fitting | Manual for building tree volume and biomass allometric equations" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Model fitting | Manual for building tree volume and biomass allometric equations" />
  
  
  

<meta name="author" content="Nicolas Picard, Laurent Saint-André, Matieu Henry" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="5-explo.html"/>
<link rel="next" href="7-util.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html"><b>Manual for building tree volume and biomass allometric equations </b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover page</a></li>
<li class="chapter" data-level="" data-path="note-on-the-version-2.html"><a href="note-on-the-version-2.html"><i class="fa fa-check"></i>Note on the version 2</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="preamble.html"><a href="preamble.html"><i class="fa fa-check"></i>Preamble</a></li>
<li class="chapter" data-level="1" data-path="1-biol.html"><a href="1-biol.html"><i class="fa fa-check"></i><b>1</b> The foundations of biomass estimations</a><ul>
<li class="chapter" data-level="1.1" data-path="1-biol.html"><a href="1-biol.html#biology-eichhorns-rule-site-index-etc."><i class="fa fa-check"></i><b>1.1</b> “Biology”: Eichhorn’s rule, site index, etc.</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-biol.html"><a href="1-biol.html#even-aged-monospecific-stands"><i class="fa fa-check"></i><b>1.1.1</b> Even-aged, monospecific stands</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-biol.html"><a href="1-biol.html#uneven-aged-andor-multispecific-stands"><i class="fa fa-check"></i><b>1.1.2</b> Uneven-aged and/or multispecific stands</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-biol.html"><a href="1-biol.html#selecting-a-method"><i class="fa fa-check"></i><b>1.2</b> Selecting a method</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-biol.html"><a href="1-biol.html#estimating-the-biomass-of-a-biome"><i class="fa fa-check"></i><b>1.2.1</b> Estimating the biomass of a biome</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-biol.html"><a href="1-biol.html#estimating-the-biomass-of-a-forest-or-a-set-of-forests"><i class="fa fa-check"></i><b>1.2.2</b> Estimating the biomass of a forest or a set of forests</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-biol.html"><a href="1-biol.html#measuring-the-biomass-of-a-tree"><i class="fa fa-check"></i><b>1.2.3</b> Measuring the biomass of a tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-samp.html"><a href="2-samp.html"><i class="fa fa-check"></i><b>2</b> Sampling and stratification</a><ul>
<li class="chapter" data-level="2.1" data-path="2-samp.html"><a href="2-samp.html#simple"><i class="fa fa-check"></i><b>2.1</b> Sampling for a simple linear regression</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-samp.html"><a href="2-samp.html#predicting-the-volume-of-a-particular-tree"><i class="fa fa-check"></i><b>2.1.1</b> Predicting the volume of a particular tree</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-samp.html"><a href="2-samp.html#peup"><i class="fa fa-check"></i><b>2.1.2</b> Predicting the volume of a stand</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-samp.html"><a href="2-samp.html#sampling-to-construct-volume-tables"><i class="fa fa-check"></i><b>2.2</b> Sampling to construct volume tables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-samp.html"><a href="2-samp.html#size"><i class="fa fa-check"></i><b>2.2.1</b> Number of trees</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-samp.html"><a href="2-samp.html#vent"><i class="fa fa-check"></i><b>2.2.2</b> Sorting trees</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-samp.html"><a href="2-samp.html#stratif"><i class="fa fa-check"></i><b>2.2.3</b> Stratification</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-samp.html"><a href="2-samp.html#sel"><i class="fa fa-check"></i><b>2.2.4</b> Selecting trees</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-samp.html"><a href="2-samp.html#sampeup"><i class="fa fa-check"></i><b>2.3</b> Sampling for stand estimations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-samp.html"><a href="2-samp.html#sampling-unit"><i class="fa fa-check"></i><b>2.3.1</b> Sampling unit</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-samp.html"><a href="2-samp.html#relation-between-coefficient-of-variation-and-plot-size"><i class="fa fa-check"></i><b>2.3.2</b> Relation between coefficient of variation and plot size</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-samp.html"><a href="2-samp.html#selecting-plot-size"><i class="fa fa-check"></i><b>2.3.3</b> Selecting plot size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ter.html"><a href="3-ter.html"><i class="fa fa-check"></i><b>3</b> In the field</a><ul>
<li class="chapter" data-level="3.1" data-path="3-ter.html"><a href="3-ter.html#pese"><i class="fa fa-check"></i><b>3.1</b> Weighing all compartments directly in the field</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-ter.html"><a href="3-ter.html#in-the-field"><i class="fa fa-check"></i><b>3.1.1</b> In the field</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-ter.html"><a href="3-ter.html#lab1"><i class="fa fa-check"></i><b>3.1.2</b> In the laboratory</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-ter.html"><a href="3-ter.html#calculations"><i class="fa fa-check"></i><b>3.1.3</b> Calculations</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-ter.html"><a href="3-ter.html#direct-weighing-for-certain-compartments-and-volume-and-density-measurements-for-others"><i class="fa fa-check"></i><b>3.2</b> Direct weighing for certain compartments and volume and density measurements for others</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-ter.html"><a href="3-ter.html#in-the-field-case-of-semi-destructive-measurements"><i class="fa fa-check"></i><b>3.2.1</b> In the field: case of semi-destructive measurements</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-ter.html"><a href="3-ter.html#lab2"><i class="fa fa-check"></i><b>3.2.2</b> In the laboratory</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-ter.html"><a href="3-ter.html#calculations-1"><i class="fa fa-check"></i><b>3.2.3</b> Calculations</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-ter.html"><a href="3-ter.html#partial-weighings-in-the-field"><i class="fa fa-check"></i><b>3.3</b> Partial weighings in the field</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-ter.html"><a href="3-ter.html#trees-less-than-20-cm-in-diameter"><i class="fa fa-check"></i><b>3.3.1</b> Trees less than 20 cm in diameter</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-ter.html"><a href="3-ter.html#trees-more-than-20-cm-in-diameter"><i class="fa fa-check"></i><b>3.3.2</b> Trees more than 20 cm in diameter</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-ter.html"><a href="3-ter.html#measuring-roots"><i class="fa fa-check"></i><b>3.4</b> Measuring roots</a></li>
<li class="chapter" data-level="3.5" data-path="3-ter.html"><a href="3-ter.html#matos"><i class="fa fa-check"></i><b>3.5</b> Recommended equipment</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-ter.html"><a href="3-ter.html#heavy-machinery-and-vehicles"><i class="fa fa-check"></i><b>3.5.1</b> Heavy machinery and vehicles</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-ter.html"><a href="3-ter.html#general-equipment"><i class="fa fa-check"></i><b>3.5.2</b> General equipment</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-ter.html"><a href="3-ter.html#computer-entering-field-data"><i class="fa fa-check"></i><b>3.5.3</b> Computer-entering field data</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-ter.html"><a href="3-ter.html#laboratory-equipment"><i class="fa fa-check"></i><b>3.5.4</b> Laboratory equipment</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-ter.html"><a href="3-ter.html#equip"><i class="fa fa-check"></i><b>3.6</b> Recommended field team composition</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-don.html"><a href="4-don.html"><i class="fa fa-check"></i><b>4</b> Entering and formatting data</a><ul>
<li class="chapter" data-level="4.1" data-path="4-don.html"><a href="4-don.html#data-entry"><i class="fa fa-check"></i><b>4.1</b> Data entry</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-don.html"><a href="4-don.html#data-entry-errors"><i class="fa fa-check"></i><b>4.1.1</b> Data entry errors</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-don.html"><a href="4-don.html#meta-information"><i class="fa fa-check"></i><b>4.1.2</b> Meta-information</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-don.html"><a href="4-don.html#nested-levels"><i class="fa fa-check"></i><b>4.1.3</b> Nested levels</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-don.html"><a href="4-don.html#data-cleansing"><i class="fa fa-check"></i><b>4.2</b> Data cleansing</a></li>
<li class="chapter" data-level="4.3" data-path="4-don.html"><a href="4-don.html#data-formatting"><i class="fa fa-check"></i><b>4.3</b> Data formatting</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-explo.html"><a href="5-explo.html"><i class="fa fa-check"></i><b>5</b> Graphical exploration of the data</a><ul>
<li class="chapter" data-level="5.1" data-path="5-explo.html"><a href="5-explo.html#exploring-the-mean-relation"><i class="fa fa-check"></i><b>5.1</b> Exploring the mean relation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-explo.html"><a href="5-explo.html#plus"><i class="fa fa-check"></i><b>5.1.1</b> When there is more than one effect variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-explo.html"><a href="5-explo.html#determining-whether-or-not-a-relation-is-adequate"><i class="fa fa-check"></i><b>5.1.2</b> Determining whether or not a relation is adequate</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-explo.html"><a href="5-explo.html#catalog-of-primitives"><i class="fa fa-check"></i><b>5.1.3</b> Catalog of primitives</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-explo.html"><a href="5-explo.html#exploring-variance"><i class="fa fa-check"></i><b>5.2</b> Exploring variance</a></li>
<li class="chapter" data-level="5.3" data-path="5-explo.html"><a href="5-explo.html#exploring-doesnt-mean-selecting"><i class="fa fa-check"></i><b>5.3</b> Exploring doesn’t mean selecting</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-fit.html"><a href="6-fit.html"><i class="fa fa-check"></i><b>6</b> Model fitting</a><ul>
<li class="chapter" data-level="6.1" data-path="6-fit.html"><a href="6-fit.html#fitting-a-linear-model"><i class="fa fa-check"></i><b>6.1</b> Fitting a linear model</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-fit.html"><a href="6-fit.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.1.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-fit.html"><a href="6-fit.html#multiple-regression"><i class="fa fa-check"></i><b>6.1.2</b> Multiple regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-fit.html"><a href="6-fit.html#pond"><i class="fa fa-check"></i><b>6.1.3</b> Weighted regression</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-fit.html"><a href="6-fit.html#lme"><i class="fa fa-check"></i><b>6.1.4</b> Linear regression with variance model</a></li>
<li class="chapter" data-level="6.1.5" data-path="6-fit.html"><a href="6-fit.html#trans"><i class="fa fa-check"></i><b>6.1.5</b> Transforming variables</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-fit.html"><a href="6-fit.html#nlm"><i class="fa fa-check"></i><b>6.2</b> Fitting a non-linear model</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-fit.html"><a href="6-fit.html#exponent-known"><i class="fa fa-check"></i><b>6.2.1</b> Exponent known</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-fit.html"><a href="6-fit.html#estimating-the-exponent"><i class="fa fa-check"></i><b>6.2.2</b> Estimating the exponent</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-fit.html"><a href="6-fit.html#algo"><i class="fa fa-check"></i><b>6.2.3</b> Numerical optimization</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-fit.html"><a href="6-fit.html#select"><i class="fa fa-check"></i><b>6.3</b> Selecting variables and models</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-fit.html"><a href="6-fit.html#selecting-variables"><i class="fa fa-check"></i><b>6.3.1</b> Selecting variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-fit.html"><a href="6-fit.html#selmod"><i class="fa fa-check"></i><b>6.3.2</b> Selecting models</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-fit.html"><a href="6-fit.html#choosing-a-fitting-method"><i class="fa fa-check"></i><b>6.3.3</b> Choosing a fitting method</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-fit.html"><a href="6-fit.html#stratification-and-aggregation-factors"><i class="fa fa-check"></i><b>6.4</b> Stratification and aggregation factors</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-fit.html"><a href="6-fit.html#stdat"><i class="fa fa-check"></i><b>6.4.1</b> Stratifying data</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-fit.html"><a href="6-fit.html#cmpt"><i class="fa fa-check"></i><b>6.4.2</b> Tree compartments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-util.html"><a href="7-util.html"><i class="fa fa-check"></i><b>7</b> Uses and prediction</a><ul>
<li class="chapter" data-level="7.1" data-path="7-util.html"><a href="7-util.html#val"><i class="fa fa-check"></i><b>7.1</b> Validating a model</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-util.html"><a href="7-util.html#Ival"><i class="fa fa-check"></i><b>7.1.1</b> Validation criteria</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-util.html"><a href="7-util.html#cross-validation"><i class="fa fa-check"></i><b>7.1.2</b> Cross validation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-util.html"><a href="7-util.html#BVpred"><i class="fa fa-check"></i><b>7.2</b> Predicting the volume or biomass of a tree</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-util.html"><a href="7-util.html#plm"><i class="fa fa-check"></i><b>7.2.1</b> Prediction: linear model</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-util.html"><a href="7-util.html#pnls"><i class="fa fa-check"></i><b>7.2.2</b> Prediction: case of a non-linear model</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-util.html"><a href="7-util.html#papp"><i class="fa fa-check"></i><b>7.2.3</b> Approximated confidence intervals</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-util.html"><a href="7-util.html#invtra"><i class="fa fa-check"></i><b>7.2.4</b> Inverse variables transformation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-util.html"><a href="7-util.html#predicting-the-volume-or-biomass-of-a-stand"><i class="fa fa-check"></i><b>7.3</b> Predicting the volume or biomass of a stand</a></li>
<li class="chapter" data-level="7.4" data-path="7-util.html"><a href="7-util.html#BEF"><i class="fa fa-check"></i><b>7.4</b> Expanding and converting volume and biomass models</a></li>
<li class="chapter" data-level="7.5" data-path="7-util.html"><a href="7-util.html#arbi"><i class="fa fa-check"></i><b>7.5</b> Arbitrating between different models</a><ul>
<li class="chapter" data-level="7.5.1" data-path="7-util.html"><a href="7-util.html#comparing-using-validation-criteria"><i class="fa fa-check"></i><b>7.5.1</b> Comparing using validation criteria</a></li>
<li class="chapter" data-level="7.5.2" data-path="7-util.html"><a href="7-util.html#choosing-a-model"><i class="fa fa-check"></i><b>7.5.2</b> Choosing a model</a></li>
<li class="chapter" data-level="7.5.3" data-path="7-util.html"><a href="7-util.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>7.5.3</b> Bayesian model averaging</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusions-and-recommendations.html"><a href="conclusions-and-recommendations.html"><i class="fa fa-check"></i>Conclusions and recommendations</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="" data-path="list-of-red-lines.html"><a href="list-of-red-lines.html"><i class="fa fa-check"></i>List of red lines</a></li>
<li class="chapter" data-level="" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="lexicon-of-mathematical-symbols.html"><a href="lexicon-of-mathematical-symbols.html"><i class="fa fa-check"></i>Lexicon of mathematical symbols</a><ul>
<li class="chapter" data-level="" data-path="lexicon-of-mathematical-symbols.html"><a href="lexicon-of-mathematical-symbols.html#latin-symbols"><i class="fa fa-check"></i>Latin symbols</a></li>
<li class="chapter" data-level="" data-path="lexicon-of-mathematical-symbols.html"><a href="lexicon-of-mathematical-symbols.html#greek-symbols"><i class="fa fa-check"></i>Greek symbols</a></li>
<li class="chapter" data-level="" data-path="lexicon-of-mathematical-symbols.html"><a href="lexicon-of-mathematical-symbols.html#non-alphabetical-symbols"><i class="fa fa-check"></i>Non-alphabetical symbols</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="http://www.globallometree.org/" target="blank"><b>More on GlobAllomeTree</b></a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank"><i>Published with bookdown</i></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Manual for building tree volume and biomass allometric equations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fit" class="section level1">
<h1><span class="header-section-number">6</span> Model fitting</h1>
<div class="watermark">
DRAFT
</div>
<p>Fitting a model consists in estimating its parameters from data. This assumes, therefore, that data are already available and formatted, and that the mathematical expression of the model to be fitted is known. For example, fitting the power model <span class="math inline">\(B=aD^b\)</span> consists in estimating coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> from a dataset that gives the values of <span class="math inline">\(B_i\)</span> and <span class="math inline">\(D_i\)</span> from the biomass and dbh of <span class="math inline">\(n\)</span> trees (<span class="math inline">\(i=1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(n\)</span>). The response variable (also in the literature called the output variable or the dependent variable) is the variable fitted by the model. There is only one. In this guide, the response variable is always a volume or a biomass. Effect variables are the variables used to predict the response variable. There may be several, and their number is denoted by <span class="math inline">\(p\)</span>. Care must be taken not to confuse effect variables and model entry data. The model <span class="math inline">\(B=a(D^2H)^b\)</span> possesses a single effect variable (<span class="math inline">\(D^2H\)</span>) but two entries (dbh <span class="math inline">\(D\)</span> and height <span class="math inline">\(H\)</span>). Conversely, the model <span class="math inline">\(B=a_0+a_1D+a_2D^2\)</span> possesses two effect variables (<span class="math inline">\(D\)</span> and <span class="math inline">\(D^2\)</span>) but only a single entry (dbh <span class="math inline">\(D\)</span>). Each effect variable is associated with a coefficient to be calculated. To this must be added, if necessary, the y-intercept or a multiplier such that the total number of coefficients to be calculated in a model with <span class="math inline">\(p\)</span> effect variables is <span class="math inline">\(p\)</span> or <span class="math inline">\(p+1\)</span>.</p>
<p>An observation consists of the data forming the response variable (volume or biomass) and the effect variables for a tree. If we again consider the model <span class="math inline">\(B=aD^b\)</span>, an observation consists of the doublet (<span class="math inline">\(B_i\)</span>, <span class="math inline">\(D_i\)</span>). The number of observations is therefore <span class="math inline">\(n\)</span>. An observation stems from a measurement in the field. The prediction provided by the model is the value it predicts for the response variable from the data available for the effect variables. A prediction stems from a calculation. For example, the prediction provided by the model <span class="math inline">\(B=aD^b\)</span> for a tree of dbh <span class="math inline">\(D_i\)</span> is <span class="math inline">\(\hat{B}_i=aD_i^b\)</span>. There are as many predictions as there are observations. A key concept in model fitting is the residual. The residual, or residual error, is the difference between the observed value of the response variable and its prediction. Again for the same example, the residual of the <span class="math inline">\(i\)</span>th observation is: <span class="math inline">\(\varepsilon_i=B_i-\hat{B}_i=B_i-aD_i^b\)</span>. There are as many residuals as there are observations. The lower the residuals, the better the fit. Also, the statistical properties of the model stem from the properties that the residuals are assumed to check <em>a priori</em>, in particular the form of their distribution. The type of a model’s fitting is therefore directly dependent upon the properties of its residuals.</p>
<p>In all the models we will be considering here, the observations will be assumed to be independent or, which comes to the same thing, the residuals will be assumed to be independent: for each <span class="math inline">\(i\neq j\)</span>, <span class="math inline">\(\varepsilon_i\)</span> is assumed to be independent of <span class="math inline">\(\varepsilon_j\)</span>. This independence property is relatively easy to ensure through the sampling protocol. Typically, care must be taken to ensure that the characteristics of a tree measured in a given place do not affect the characteristics of another tree in the sample. Selecting trees that are sufficiently far apart is generally enough to ensure this independence. If the residuals are not independent, the model can be modified to take account of this. For example, a spatial dependency structure could be introduced into the residuals to take account of a spatial auto-correlation between the measurements. We will not be considering these models here as they are far more complex to use.</p>
<p>In all the models we will be looking at, we assume also that the residuals have a normal distribution with zero expectation. The zero mean of the residuals is in fact a property that stems automatically from model fitting, and ensures that the model’s predictions are not biased. It is the residuals that are assumed to have a normal distribution, not the observations. This hypothesis in fact causes little constraint for volume or biomass data. In the unlikely case where the distribution of the residuals is far from normal, efforts could be made to fit other model types, e.g. the generalized linear model, but this will not be addressed here in this guide. The hypotheses that the residuals are independent and follow a normal distribution are the first two hypotheses underlying model fitting. We will see a third hypothesis later. It should be checked that these two hypotheses are actually valid. To the extent that these hypotheses concern model residuals, not the observations, they cannot be tested until they have been calculated, i.e. until the model has been fitted. These hypotheses are therefore checked a posteriori, after model fitting. The models we will look at here are also robust with regard to these hypotheses, i.e. the predictive quality of the fitted models is acceptable even when the independence and the normal distribution of the residuals are not perfectly satisfied. For this reason, we will not look to test these two hypotheses very formally. In practice we will simply perform a visual verification of the plots.</p>
<div id="fitting-a-linear-model" class="section level2">
<h2><span class="header-section-number">6.1</span> Fitting a linear model</h2>
<p>The linear model is the simplest of all models to fit. The word <em>linear</em> means here that the model is <em>linearly</em> dependent on its coefficients. For example, <span class="math inline">\(Y=a+bX^2\)</span> and <span class="math inline">\(Y=a+b\ln(X)\)</span> are linear models because the response variable <span class="math inline">\(Y\)</span> is linearly dependent upon coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, even if <span class="math inline">\(Y\)</span> is not linearly dependent on the effect variable $X $. Conversely, <span class="math inline">\(Y=aX^b\)</span> is not a linear model because <span class="math inline">\(Y\)</span> is not linearly dependent on coefficient <span class="math inline">\(b\)</span>. Another property of the linear model is that the residual is additive. This is underlined by explicitly including the residual <span class="math inline">\(\varepsilon\)</span> in the model’s formula. For example, a linear regression of <span class="math inline">\(Y\)</span> against <span class="math inline">\(X\)</span> will be written: <span class="math inline">\(Y=a+bX+\varepsilon\)</span>.</p>
<div id="simple-linear-regression" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Simple linear regression</h3>
<p>A simple linear regression is the simplest of the linear models. It assumes (<em>i</em>) that there is only one effect variable <span class="math inline">\(X\)</span>, (<em>ii</em>) that the relation between the response variable <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is a straight line:
<span class="math display">\[
Y=a+bX+\varepsilon
\]</span>
where <span class="math inline">\(a\)</span> is the y-intercept of the line and <span class="math inline">\(b\)</span> its slope, and (<em>iii</em>) that the residuals are of constant variance: <span class="math inline">\(\mathrm{Var}(\varepsilon)=\sigma^2\)</span>. For example, the model
<span class="math display" id="eq:pow">\[\begin{equation}
\ln(B)=a+b\ln(D)+\varepsilon\tag{6.1}
\end{equation}\]</span>
is a typical simple linear regression, with response variable <span class="math inline">\(Y=\ln(B)\)</span> and effect variable <span class="math inline">\(X=\ln(D)\)</span>. It corresponds to a power model for biomass: <span class="math inline">\(B=\exp(a)D^b\)</span>. This model is often used to fit a single-entry biomass model. Another example is the double-entry biomass model:
<span class="math display" id="eq:powH">\[\begin{equation}
\ln(B)=a+b\ln(D^2H)+\varepsilon\tag{6.2}
\end{equation}\]</span>
The hypothesis whereby the residuals are of constant variance is added to the two independence and normal distribution hypotheses (we also speak of homoscedasticity). These three hypotheses may be summarized by writing:
<span class="math display">\[
\varepsilon\;\mathop{\sim}_{\mathrm{i.i.d.}}\;\mathcal{N}(0,\ \sigma)
\]</span>
where <span class="math inline">\(\mathcal{N}(\mu,\ \sigma)\)</span> designates a normal distribution of expectation <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, the tilde “<span class="math inline">\(\sim\)</span>” means “is distributed in accordance with”, and “i.i.d.” is the abbreviation of “independently and identically distributed”.</p>
<div class="figure" style="text-align: center"><span id="fig:reglin"></span>
<img src="source/figures/reglin.png" alt="Actual observations (points) and plot of the regression (thick line) and residuals (thin lines)." width="40%" />
<p class="caption">
Figure 6.1: Actual observations (points) and plot of the regression (thick line) and residuals (thin lines).
</p>
</div>
<div id="estimating-coefficients" class="section level4 unnumbered">
<h4>Estimating coefficients</h4>
<p>Figure <a href="6-fit.html#fig:reglin">6.1</a> shows the observations and the plot of predicted values. The best fit is that which minimizes the residual error. There are several ways of quantifying this residual error. From a mathematical standpoint, this is equivalent with choosing a norm to measure <span class="math inline">\(\varepsilon\)</span> and various norms could be used. The norm that is commonly used is the <span class="math inline">\(L_2\)</span> norm, which is equivalent with quantifying the residual difference between the actual observations and the predictions by summing the squares of the residuals, which is also called the sum of squares (SS):
<span class="math display">\[
\mathrm{SSE}(a,b)=\sum_{i=1}^n\varepsilon_i^2=\sum_{i=1}^n(Y_i-\hat{Y}_i)^2
=\sum_{i=1}^n(Y_i-a-bX_i)^2
\]</span>
The best fit is therefore that which minimizes SS. In other words, the estimations <span class="math inline">\(\hat{a}\)</span> and <span class="math inline">\(\hat{b}\)</span> of the coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> that minimize the sum of squares:
<span class="math display">\[
(\hat{a},\ \hat{b})=\arg\min_{(a,\ b)}\mathrm{SSE}(a,b)
\]</span>
This minimum may be obtained by calculating the partial derivatives of SS in relation to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, and by looking for the values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> that cancel these partial derivatives. Simple calculations yield the following results: <span class="math inline">\(\hat{b}=\widehat{\mathrm{Cov}}(X,\ Y)/S_X^2\)</span> and <span class="math inline">\(\hat{a}=\bar{Y}-\hat{b}\bar{X}\)</span>, where <span class="math inline">\(\bar{X}=(\sum_{i=1}^nX_i)/n\)</span> is the empirical mean of the effect variable, <span class="math inline">\(\bar{Y}=(\sum_{i=1}^nY_i)/n\)</span> is the empirical mean of the response variable,
<span class="math display">\[
S_X^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2
\]</span>
is the empirical variance of the effect variable, and
<span class="math display">\[
\widehat{\mathrm{Cov}}(X,\ Y)=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})
\]</span>
is the empirical covariance between the effect variable and the response variable. The estimation of the residual variance is:
<span class="math display">\[
\hat{\sigma}^2=\frac{1}{n-2}\sum_{i=1}^n(Y_i-\hat{a}-\hat{b}X_i)^2
=\frac{\mathrm{SSE}(\hat{a},\ \hat{b})}{n-2}
\]</span>
Because this method of estimating the coefficients is based on minimizing the sum of squares, it is called the <em>least squares</em> method (sometimes it is called the “ordinary least squares” method to differentiate it from the weighted least squares method we will see in § <a href="6-fit.html#pond">6.1.3</a>). This estimation method has the advantage of providing an explicit expression of the estimated coefficients.</p>
</div>
<div id="interpreting-the-results-of-a-regression" class="section level4 unnumbered">
<h4>Interpreting the results of a regression</h4>
<p>When fitting a simple linear regression, several outputs need to be analyzed. The determination coefficient, more commonly called <span class="math inline">\(R^2\)</span>, measures the quality of the fit. <span class="math inline">\(R^2\)</span> is directly related to the residual variance since:
<span class="math display">\[
R^2=1-\frac{\hat{\sigma}^2(n-2)/n}{S_Y^2}
\]</span>
where <span class="math inline">\(S_Y^2=[\sum_{i=1}^n(Y_i-\bar{Y})^2]/n\)</span> is the empirical variance of <span class="math inline">\(Y\)</span>. The difference <span class="math inline">\(S_Y^2-\hat{\sigma}^2(n-2)/n\)</span> between the variance of <span class="math inline">\(Y\)</span> and the residual variance corresponds to the variance explained by the model. The determination coefficient <span class="math inline">\(R^2\)</span> can be interpreted as being the ratio between the variance explained by the model and the total variance. It is between 0 and 1, and the closer it is to 1, the better the quality of the fit. In the case of a simple linear regression, and only in this case, <span class="math inline">\(R^2\)</span> is also the square of the linear correlation coefficient (also called Pearson’s coefficient) between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We have already seen in chapter <a href="5-explo.html#explo">5</a> (particularly in Figure <a href="5-explo.html#fig:badR2">5.2</a>) that the interpretation of <span class="math inline">\(R^2\)</span> has its limitations.</p>
<p>In addition to estimating values for coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, model fitting also provides the standard deviations of these estimations (i.e. the standard deviations of estimators <span class="math inline">\(\hat{a}\)</span> and <span class="math inline">\(\hat{b}\)</span>), and the results of significance tests on these coefficients. A test is performed on the y-intercept <span class="math inline">\(a\)</span>, which tests the null hypothesis that <span class="math inline">\(a=0\)</span>, and likewise a test is performed on the slope <span class="math inline">\(b\)</span>, which tests the hypothesis that <span class="math inline">\(b=0\)</span>.</p>
<p>Finally, the result given by the overall significance test for the model is also analyzed. This test is based on breaking down the total variance of <span class="math inline">\(Y\)</span> into the sum of the variance explained by the model and the residual variance. Like in an analysis of variance, the test used is Fisher’s test which, as test statistic, uses a weighted ratio of the explained variance over the residual variance. In the case of a simple linear regression, and only in this case, the test for the overall significance of the model gives the same result as the test on the null hypothesis <span class="math inline">\(b=0\)</span>. This can be grasped intuitively: a line linking <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> is significant only if its slope is not zero.</p>
</div>
<div id="checking-hypotheses" class="section level4 unnumbered">
<h4>Checking hypotheses</h4>
<p>Model fitting may be brought to a conclusion by checking that the hypotheses put forward for the residuals are in fact satisfied. We will not consider here the hypothesis that the residuals are independent for this has already been satisfied thanks to the sampling plan adopted. If there is a natural order in the observations, we could possibly use the Durbin-Watson test to test if the residuals are indeed independent <span class="citation">(Durbin and Watson <a href="bibliography.html#ref-durbin71" role="doc-biblioref">1971</a>)</span>. The hypothesis that the residuals are normally distributed can be checked visually by inspecting the quantile–quantile graph. This graph plots the empirical quantiles of the residuals against the theoretical quantiles of the standard normal distribution. If the hypothesis that the residuals are normally distributed is satisfied, then the points are approximately aligned along a straight line, as in Figure <a href="6-fit.html#fig:res1">6.2</a> (right plot).</p>
<div class="figure" style="text-align: center"><span id="fig:res1"></span>
<img src="source/figures/resid1.png" alt="Residuals plotted against fitted values (left) and quantile–quantile plot (right) when the normal distribution and constant variance of the residuals hypotheses are satisfied." width="80%" />
<p class="caption">
Figure 6.2: Residuals plotted against fitted values (left) and quantile–quantile plot (right) when the normal distribution and constant variance of the residuals hypotheses are satisfied.
</p>
</div>
<p>In the case of fitting volume or biomass models, the most important hypothesis to satisfy is that of the constant variance of the residuals. This can be checked visually by plotting the cluster of points for the residuals <span class="math inline">\(\varepsilon_i=Y_i-\hat{Y}_i\)</span> in function to predicted values <span class="math inline">\(\hat{Y}_i=\hat{a}+\hat{b}X_i\)</span>.</p>
<p>If the variance of the residuals is indeed constant, this cluster of points should not show any particular trend and no particular structure. This for instance is the case for the plot shown on the left in Figure <a href="6-fit.html#fig:res1">6.2</a>. By contrast, if the cluster of points shows some form of structure, the hypothesis should be questioned. This for instance is the case in Figure <a href="6-fit.html#fig:res2">6.3</a> where the cluster of points for the residuals plotted against fitted values forms a funnel shape. This shape is typical of a residual variance that increases with the effect variable (which we call heteroscedasticity). If this is the case, a model other than a simple linear regression must be fitted.</p>
<div class="figure" style="text-align: center"><span id="fig:res2"></span>
<img src="source/figures/resid2.png" alt="Plot of residuals against fitted values when the residuals have a non constant variance (heteroscedasticity)." width="40%" />
<p class="caption">
Figure 6.3: Plot of residuals against fitted values when the residuals have a non constant variance (heteroscedasticity).
</p>
</div>
<p>In the case of biological data such as tree volume or biomass, heteroscedasticity is the rule and homoscedasticity the exception. This means simply that the greater tree biomass (or volume), the greater the variability of this biomass (or volume). This increase in the variability of biomass with increasing size is a general principle in biology. Thus, when fitting biomass or volume models, simple linear regression using biomass as response variable (<span class="math inline">\(Y=B\)</span>) is generally of little use. The log transformation (i.e. <span class="math inline">\(Y=\ln(B)\)</span>) resolves this problem and therefore the linear regressions we use for adjusting models nearly always use log-transformed data. We will return at length to this fundamental point later.</p>
<div class="filrouge">
<ol start="7" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:rllnBvD" class="exercise"><strong>Red line 6.1  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Simple linear regression between <span class="math inline">\(\ln(B)\)</span> and <span class="math inline">\(\ln(D)\)</span></strong></span></p>
</div>
<p>The exploratory analysis (red line <a href="5-explo.html#exr:feln">5.4</a>) showed that the relation between the log of the biomass and the log of the dbh was linear, with a variance of <span class="math inline">\(\ln(B)\)</span> that was approximately constant. A simple linear regression may therefore be fitted to predict <span class="math inline">\(\ln(B)\)</span>from <span class="math inline">\(\ln(D)\)</span>:
<span class="math display">\[
\ln(B)=a+b\ln(D)+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]</span>
The regression is fitted using the ordinary least squares method. Given that we cannot apply the log transformation to a value of zero, zero biomass data (see red line <a href="4-don.html#exr:read">4.1</a>) must first be withdrawn from the dataset:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb14-1"><a href="6-fit.html#cb14-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,])</span>
<span id="cb14-2"><a href="6-fit.html#cb14-2"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>The residual standard deviation is <span class="math inline">\(\hat{\sigma}=0.462\)</span>, <span class="math inline">\(R^2\)</span> is 0.9642 and the model is highly significant (Fisher’s test: <span class="math inline">\(F_{1,39}=1051\)</span>, p-value <span class="math inline">\(&lt;2.2\times10^{-16}\)</span>). The values of the coefficients are given in the table below:</p>
<pre class="Rout"><code>##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -8.42722    0.27915  -30.19   &lt;2e-16 ***
## I(log(dbh))  2.36104    0.07283   32.42   &lt;2e-16 ***</code></pre>
<p>The first column in the table gives the values of the coefficients. The model is therefore: <span class="math inline">\(\ln(B)=-8.42722+2.36104\ln(D)\)</span>. The second column gives the standard deviations of the coefficient estimators. The third column gives the result of the test on the null hypothesis that the coefficient is zero. Finally, the fourth column gives the p-value of this test. In the present case, both the slope and the y-intercept are significantly different from zero. We must now check graphically that the hypotheses of the linear regression are satisfied:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb16-1"><a href="6-fit.html#cb16-1"></a><span class="kw">plot</span>(m, <span class="dt">which =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</span></code></pre></div>
<p>The result is shown in Figure <a href="6-fit.html#fig:fBvDres">6.4</a>. Even though the quantile–quantile plot of the residuals appears to have a slight structure, we will consider that the hypotheses of the simple linear regression are suitably satisfied.</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fBvDres"></span>
<img src="source/figures/fBvDres.png" alt="Residuals plotted against fitted values (left) and quantile–quantile plot (right) of the residuals of the simple linear regression of \(\ln(B)\) against \(\ln(D)\) fitted for the 42 trees measured by Henry et al. (2010) in Ghana." width="80%" />
<p class="caption">
Figure 6.4: Residuals plotted against fitted values (left) and quantile–quantile plot (right) of the residuals of the simple linear regression of <span class="math inline">\(\ln(B)\)</span> against <span class="math inline">\(\ln(D)\)</span> fitted for the 42 trees measured by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span> in Ghana.
</p>
</div>
<div class="filrouge">
<ol start="8" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:rllnBvD2H" class="exercise"><strong>Red line 6.2  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Simple linear regression between <span class="math inline">\(\ln(B)\)</span> and <span class="math inline">\(\ln(D^2H)\)</span></strong></span></p>
</div>
<p>The exploratory analysis (red line <a href="5-explo.html#exr:feln2">5.5</a>) showed that the relation between the log of the biomass and the log of <span class="math inline">\(D^2H\)</span> was linear, with a variance of <span class="math inline">\(\ln(B)\)</span> that was approximately constant. We can therefore fit a simple linear regression to predict <span class="math inline">\(\ln(B)\)</span> from <span class="math inline">\(\ln(D^2H)\)</span>:
<span class="math display">\[
\ln(B)=a+b\ln(D^2H)+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]</span>
The regression is fitted by the ordinary least squares method. Given that we cannot apply the log transformation to a value of zero, zero biomass data (see red line <a href="4-don.html#exr:read">4.1</a>) must first be withdrawn from the dataset:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb17-1"><a href="6-fit.html#cb17-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,])</span>
<span id="cb17-2"><a href="6-fit.html#cb17-2"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>The residual standard deviation is <span class="math inline">\(\hat{\sigma}=0.4084\)</span>, <span class="math inline">\(R^2\)</span> is 0.972 and the model is highly significant (Fisher’s test: <span class="math inline">\(F_{1,39}=1356\)</span>, p-value <span class="math inline">\(&lt;2.2\times10^{-16}\)</span>). The values of the coefficients are as follows:</p>
<pre class="Rout"><code>##                      Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          -8.99427    0.26078  -34.49   &lt;2e-16 ***
## I(log(dbh^2 * heig))  0.87238    0.02369   36.82   &lt;2e-16 ***</code></pre>
<p>The first column in the table gives the values of the coefficients. The model is therefore: <span class="math inline">\(\ln(B)=-8.99427+0.87238\ln(D^2H)\)</span>. The second column gives the standard deviations of the coefficient estimators. The third column gives the result of the test on the null hypothesis that “the coefficient is zero”. Finally, the fourth column gives the p-value of this test. In the present case, both the slope and the y-intercept are significantly different from zero.</p>
<p>We must now check graphically that the hypotheses of the linear regression are satisfied:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb19-1"><a href="6-fit.html#cb19-1"></a><span class="kw">plot</span>(m, <span class="dt">which =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</span></code></pre></div>
<p>The result is shown in Figure <a href="6-fit.html#fig:fBvD2Hres">6.5</a>. Even though the plot of the residuals against the fitted values appears to have a slight structure, we will consider that the hypotheses of the simple linear regression are suitably satisfied.</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fBvD2Hres"></span>
<img src="source/figures/fBvD2Hres.png" alt="Residuals plotted against fitted values (left) and quantile–quantile plot (right) of the residuals of the simple linear regression of \(\ln(B)\) against \(\ln(D^2H)\) fitted for the 42 trees measured by Henry et al. (2010) in Ghana." width="80%" />
<p class="caption">
Figure 6.5: Residuals plotted against fitted values (left) and quantile–quantile plot (right) of the residuals of the simple linear regression of <span class="math inline">\(\ln(B)\)</span> against <span class="math inline">\(\ln(D^2H)\)</span> fitted for the 42 trees measured by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span> in Ghana.
</p>
</div>
</div>
</div>
<div id="multiple-regression" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Multiple regression</h3>
<p>Multiple regression is the extension of simple linear regression to the case where there are several effect variables, and is written:
<span class="math display" id="eq:regmul">\[\begin{equation}
Y=a_0+a_1X_1+a_2X_2+\ldots+a_pX_p+\varepsilon\tag{6.3}
\end{equation}\]</span>
where <span class="math inline">\(Y\)</span> is the response variable, <span class="math inline">\(X_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span> the <span class="math inline">\(p\)</span> effect variables, <span class="math inline">\(a_0\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(a_p\)</span> the coefficients to be estimated, and <span class="math inline">\(\varepsilon\)</span> the residual error. Counting the y-intercept <span class="math inline">\(a_0\)</span>, there are <span class="math inline">\(p+1\)</span> coefficients to be estimated. Like for simple linear regression, the variance of the residuals is assumed to be constant and equal to <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[
\varepsilon\;\mathop{\sim}_{\mathrm{i.i.d.}}\;\mathcal{N}(0,\ \sigma)
\]</span>
The following biomass models are examples of multiple regressions:
<span class="math display" id="eq:powsd" id="eq:powd" id="eq:powHsd" id="eq:powHd" id="eq:powHs">\[\begin{eqnarray}
\ln(B) &amp;=&amp; a_0+a_1\ln(D^2H)+a_2\ln(\rho)+\varepsilon\tag{6.4}\\ %
\ln(B) &amp;=&amp; a_0+a_1\ln(D)+a_2\ln(H)+\varepsilon\tag{6.5}\\ %
\ln(B) &amp;=&amp; a_0+a_1\ln(D)+a_2\ln(H)+a_3\ln(\rho)+\varepsilon\tag{6.6}\\ %
\ln(B) &amp;=&amp; a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+\varepsilon\tag{6.7}\\ %
\ln(B) &amp;=&amp; a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(\rho)+\varepsilon\tag{6.8}%
\end{eqnarray}\]</span>
where <span class="math inline">\(\rho\)</span> is wood density. In all these examples the response variable is the log of the biomass: <span class="math inline">\(Y=\ln(B)\)</span>. As model <a href="6-fit.html#eq:powHs">(6.4)</a> generalizes <a href="6-fit.html#eq:powH">(6.2)</a> by adding dependency on wood specific density: typically <a href="6-fit.html#eq:powHs">(6.4)</a> should be preferred to <a href="6-fit.html#eq:powH">(6.2)</a> when the dataset is multispecific. Model <a href="6-fit.html#eq:powHd">(6.5)</a> generalizes <a href="6-fit.html#eq:powH">(6.2)</a> by considering that the exponent associated with height <span class="math inline">\(H\)</span> is not necessarily half the exponent associated with dbh. It therefore introduces a little more flexibility into the form of the relation between biomass and <span class="math inline">\(D^2H\)</span>. Model <a href="6-fit.html#eq:powHsd">(6.6)</a> generalizes <a href="6-fit.html#eq:powH">(6.2)</a> by considering that there are both several species and that biomass is not quite a power of <span class="math inline">\(D^2H\)</span>. Model <a href="6-fit.html#eq:powd">(6.7)</a> generalizes <a href="6-fit.html#eq:pow">(6.1)</a> by considering that the relation between <span class="math inline">\(\ln(B)\)</span> and <span class="math inline">\(\ln(D)\)</span> is not exactly linear. It therefore offers a little more flexibility in the form of this relation. Model <a href="6-fit.html#eq:powsd">(6.8)</a> is the extension of <a href="6-fit.html#eq:powd">(6.7)</a> to take account of the presence of several species in a dataset.</p>
<div id="estimating-coefficients-1" class="section level4 unnumbered">
<h4>Estimating coefficients</h4>
<p>In the same manner as for simple linear regression, the estimation of the coefficients is based on the least squares method. Estimators <span class="math inline">\(\hat{a}_0\)</span>, <span class="math inline">\(\hat{a}_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\hat{a}_p\)</span> are the values of coefficients <span class="math inline">\(a_0\)</span>, <span class="math inline">\(a_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(a_p\)</span> that minimize the sum of squares:
<span class="math display">\[
\mathrm{SSE}(a_0,\ a_1,\ \ldots,\ a_p)=\sum_{i=1}^n\varepsilon_i^2
=\sum_{i=1}^n(Y_i-\hat{Y}_i)^2
=\sum_{i=1}^n(Y_i-a_0-a_1X_{i1}-\ldots-a_pX_{ip})^2
\]</span>
where <span class="math inline">\(X_{ij}\)</span> is the value of the <span class="math inline">\(j\)</span>th effect variable for the <span class="math inline">\(i\)</span>th observation (<span class="math inline">\(i=1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(n\)</span> and <span class="math inline">\(j=1\)</span>  <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(p\)</span>). Once again, estimations of the coefficients may be obtained by calculating the partial derivatives of SS in relation to the coefficients, and by looking for the values of the coefficients that cancel these partial derivatives. These computations are barely more complex than for simple linear regression, on condition that they are arranged in a matrix form. Let <span class="math inline">\(\mathbf{X}\)</span> be the matrix with <span class="math inline">\(n\)</span> lines and <span class="math inline">\(p\)</span> columns, called the design matrix, containing the values observed for the effect variables:
<span class="math display">\[
\mathbf{X}=\left[
\begin{array}{cccc}
1      &amp; X_{11} &amp; \cdots &amp; X_{1p}\\ %
\vdots &amp; \vdots &amp;        &amp; \vdots\\ %
1      &amp; X_{n1} &amp; \cdots &amp; X_{np}\\ %
\end{array}
\right]
\]</span>
Let <span class="math inline">\(\mathbf{Y}={}^{\mathrm{t}}{[Y_1,\ \ldots,\ Y_n]}\)</span> be the vector of the <span class="math inline">\(n\)</span> values observed for the response variable, and <span class="math inline">\(\mathbf{a}={}^{\mathrm{t}}{[a_0,\ \ldots,\ a_p]}\)</span> be the vector of the <span class="math inline">\(p+1\)</span> coefficients to be estimated. Thus
<span class="math display">\[
\mathbf{X}\mathbf{a}=\left[
\begin{array}{c}
a_0+a_1X_{11}+\ldots+a_pX_{1p}\\ %
\vdots\\ %
a_0+a_1X_{n1}+\ldots+a_pX_{np}\\ %
\end{array}
\right]
\]</span>
is none other than the vector <span class="math inline">\(\hat{\mathbf{Y}}\)</span> of the <span class="math inline">\(n\)</span> values fitted by the response variable model. Using these matrix notations, the sum of squares is written:
<span class="math display">\[
\mathrm{SSE}(\mathbf{a})={}^{\mathrm{t}}{(\mathbf{Y}-\hat{\mathbf{Y}})}
(\mathbf{Y}-\hat{\mathbf{Y}})={}^{\mathrm{t}}{(\mathbf{Y}-\mathbf{X}\mathbf{a})}
(\mathbf{Y}-\mathbf{X}\mathbf{a})
\]</span>
And using matrix differential calculus <span class="citation">(Magnus and Neudecker <a href="bibliography.html#ref-magnus07" role="doc-biblioref">2007</a>)</span>, we finally obtain:
<span class="math display">\[
\hat{\mathbf{a}}=\arg\min_{\mathbf{a}}\mathrm{SSE}(\mathbf{a})
=({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}{}^{\mathrm{t}}{\mathbf{X}}\mathbf{Y}
\]</span>
The estimation of the residual variance is:
<span class="math display">\[
\hat{\sigma}^2=\frac{\mathrm{SSE}(\hat{\mathbf{a}})}{n-p-1}
\]</span>
Like for simple linear regression, this estimation method has the advantage of providing an explicit expression of the coefficients estimated. As simple linear regression is a special case of multiple regression (case where <span class="math inline">\(p=1\)</span>), we can check that the matrix expressions for estimating the coefficients and <span class="math inline">\(\hat{\sigma}\)</span> actually — when <span class="math inline">\(p=1\)</span> — give again the expressions given previously with a simple linear regression.</p>
</div>
<div id="irm" class="section level4 unnumbered">
<h4>Interpreting the results of a multiple regression</h4>
<p>In the same manner as for simple linear regression, fitting a multiple regression provides a determination coefficient <span class="math inline">\(R^2\)</span> corresponding to the proportion of the variance explained by the model, values <span class="math inline">\(\hat{\mathbf{a}}\)</span> for model coefficients <span class="math inline">\(a_0\)</span>, <span class="math inline">\(a_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(a_p\)</span>, standard deviations for these estimations, the results of significance tests on these coefficients (there are <span class="math inline">\(p+1\)</span> — one for each coefficient — null hypotheses <span class="math inline">\(a_i=0\)</span> for <span class="math inline">\(i=0\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(p\)</span>), and the result of the test on the overall significance of the model.</p>
<p>As previously, <span class="math inline">\(R^2\)</span> has a value of between 0 and 1. The higher the value, the better the fit. However, it should be remembered that the value of <span class="math inline">\(R^2\)</span> increases automatically with the number of effect variables used. For instance, if we are predicting <span class="math inline">\(Y\)</span> using a polynomial with <span class="math inline">\(p\)</span> orders in <span class="math inline">\(X\)</span>,
<span class="math display">\[
Y=a_0+a_1X+a_2X^2+\ldots+a_pX^p
\]</span>
<span class="math inline">\(R^2\)</span> will automatically increase with the number of orders <span class="math inline">\(p\)</span>. This may give the illusion that the higher the number of orders <span class="math inline">\(p\)</span> in a polynomial, the better its fit. This of course is not the case. If the number <span class="math inline">\(p\)</span> of orders is too high, this will result in model over-parameterization. In other words, <span class="math inline">\(R^2\)</span> is not a valid criterion on which model selection may be based. We will return to this point in section <a href="6-fit.html#select">6.3</a>.</p>
</div>
<div id="checking-hypotheses-1" class="section level4 unnumbered">
<h4>Checking hypotheses</h4>
<p>Like simple linear regression, multiple regression is based on three hypotheses: that the residuals are independent, that they follow a normal distribution and that their variance is constant. These hypotheses may be checked in exactly the same manner as for the simple linear regression. To check that the residuals follow a normal distribution, we can plot a quantile–quantile graph and verify visually that the cluster of points forms a straight line. To check that the variance of the residuals is constant, we can plot the residuals against the fitted values and verify visually that the cluster of points does not show any particular trend. The same restriction as for the simple linear regression applies to volume or biomass data that nearly always (or always) show heteroscedasticity. For this reason, multiple regression is generally applicable for fitting models only on log-transformed data.</p>
<div class="filrouge">
<ol start="9" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:flnDpol" class="exercise"><strong>Red line 6.3  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Polynomial regression between <span class="math inline">\(\ln(B)\)</span> and <span class="math inline">\(\ln(D)\)</span></strong></span></p>
</div>
<p>The exploratory analysis (red line <a href="5-explo.html#exr:feln">5.4</a>) showed that the relation between the log of the biomass and the log of the dbh was linear. We can ask ourselves the question of whether this relation is truly linear, or has a more complex shape. To do this, we must construct a polynomial regression with <span class="math inline">\(p\)</span> orders, i.e. a multiple regression of <span class="math inline">\(\ln(B)\)</span> against <span class="math inline">\(\ln(D)\)</span>, <span class="math inline">\([\ln(D)]^2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\([\ln(D)]^p\)</span>:
<span class="math display">\[
\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+\ldots+a_p[\ln(D)]^p+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]</span>
The regression is fitted by the ordinary least squares method. As the log transformation stabilizes the residual variance, the hypotheses on which the multiple regression is based are in principle satisfied. For a second-order polynomial, the polynomial regression is fitted by the following code:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb20-1"><a href="6-fit.html#cb20-1"></a>m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">2</span>), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,])</span>
<span id="cb20-2"><a href="6-fit.html#cb20-2"></a><span class="kw">print</span>(<span class="kw">summary</span>(m2))</span></code></pre></div>
<p>This yields:</p>
<pre class="Rout"><code>##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -8.322190   1.031359  -8.069 9.25e-10 ***
## I(log(dbh))    2.294456   0.633072   3.624 0.000846 ***
## I(log(dbh)^2)  0.009631   0.090954   0.106 0.916225</code></pre>
<p>with <span class="math inline">\(R^2=\)</span> <code>0.9642</code>. And as for a third-order polynomial regression:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb22-1"><a href="6-fit.html#cb22-1"></a>m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb22-2"><a href="6-fit.html#cb22-2"></a>  <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">3</span>),</span>
<span id="cb22-3"><a href="6-fit.html#cb22-3"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb22-4"><a href="6-fit.html#cb22-4"></a>  )</span>
<span id="cb22-5"><a href="6-fit.html#cb22-5"></a><span class="kw">print</span>(<span class="kw">summary</span>(m3))</span></code></pre></div>
<p>it yields:</p>
<pre class="Rout"><code>##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   -5.46413    3.80855  -1.435    0.160
## I(log(dbh))   -0.42448    3.54394  -0.120    0.905
## I(log(dbh)^2)  0.82073    1.04404   0.786    0.437
## I(log(dbh)^3) -0.07693    0.09865  -0.780    0.440</code></pre>
<p>with <span class="math inline">\(R^2=\)</span> <code>0.9648</code>. Finally, a fourth-order polynomial regression:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb24-1"><a href="6-fit.html#cb24-1"></a>m4 &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb24-2"><a href="6-fit.html#cb24-2"></a>  <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">4</span>),</span>
<span id="cb24-3"><a href="6-fit.html#cb24-3"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb24-4"><a href="6-fit.html#cb24-4"></a>  )</span>
<span id="cb24-5"><a href="6-fit.html#cb24-5"></a><span class="kw">print</span>(<span class="kw">summary</span>(m4))</span></code></pre></div>
<p>yields:</p>
<pre class="Rout"><code>##               Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)   -26.7953    15.7399  -1.702   0.0973 .
## I(log(dbh))    26.3990    19.5353   1.351   0.1850  
## I(log(dbh)^2) -11.2782     8.7301  -1.292   0.2046  
## I(log(dbh)^3)   2.2543     1.6732   1.347   0.1863  
## I(log(dbh)^4)  -0.1628     0.1166  -1.396   0.1714</code></pre>
<p>with <span class="math inline">\(R^2=\)</span> 0.9648. Adding higher order terms above 1 therefore does not improve the model. The coefficients associated with these terms were not significantly different from zero. But the model’s <span class="math inline">\(R^2\)</span> continued to increase with the number <span class="math inline">\(p\)</span> of orders in the polynomial. <span class="math inline">\(R^2\)</span> is not therefore a reliable criterion for selecting the order of the polynomial. The plots fitted by these different polynomials may be superimposed on the biomass-dbh cluster of points: with object <code>m</code> indicating the linear regression of <span class="math inline">\(\ln(B)\)</span> against <span class="math inline">\(\ln(D)\)</span> fitted in red line <a href="6-fit.html#exr:rllnBvD">6.1</a>,</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb26-1"><a href="6-fit.html#cb26-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]) <span class="co">## Red line 7</span></span>
<span id="cb26-2"><a href="6-fit.html#cb26-2"></a></span>
<span id="cb26-3"><a href="6-fit.html#cb26-3"></a><span class="kw">with</span>(dat, <span class="kw">plot</span>(</span>
<span id="cb26-4"><a href="6-fit.html#cb26-4"></a>  <span class="dt">x =</span> dbh, <span class="dt">y =</span> Btot, <span class="dt">xlab =</span> <span class="st">&quot;Dbh (cm)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Biomass (t)&quot;</span>, <span class="dt">log =</span> <span class="st">&quot;xy&quot;</span></span>
<span id="cb26-5"><a href="6-fit.html#cb26-5"></a>  ))</span>
<span id="cb26-6"><a href="6-fit.html#cb26-6"></a>D &lt;-<span class="st"> </span><span class="dv">10</span><span class="op">^</span><span class="kw">seq</span>(<span class="kw">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">1</span>], <span class="kw">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">2</span>], <span class="dt">length =</span> <span class="dv">200</span>)</span>
<span id="cb26-7"><a href="6-fit.html#cb26-7"></a><span class="kw">lines</span>(D, <span class="kw">exp</span>(<span class="kw">predict</span>(m , <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> D))))</span>
<span id="cb26-8"><a href="6-fit.html#cb26-8"></a><span class="kw">lines</span>(D, <span class="kw">exp</span>(<span class="kw">predict</span>(m2, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> D))))</span>
<span id="cb26-9"><a href="6-fit.html#cb26-9"></a><span class="kw">lines</span>(D, <span class="kw">exp</span>(<span class="kw">predict</span>(m3, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> D))))</span>
<span id="cb26-10"><a href="6-fit.html#cb26-10"></a><span class="kw">lines</span>(D, <span class="kw">exp</span>(<span class="kw">predict</span>(m4, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> D))))</span></code></pre></div>
<p>The plots are shown in Figure <a href="6-fit.html#fig:fDpol">6.6</a>: the higher the order of the polynomial, the more deformed the plot in order to fit the data, with increasingly unrealistic extrapolations outside the range of the data (typical of model over-parameterization).</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fDpol"></span>
<img src="source/figures/fDpol.png" alt="Biomass against dbh for 42 trees in Ghana measured by Henry et al. (2010) (points), and predictions (plots) by a polynomial regression of \(\ln(B)\) against \(\ln(D)\): (A) first-order polynomial (straight line); (B) second-order polynomial (parabolic); (C) third-order polynomial; (D) fourth-order polynomial." width="80%" />
<p class="caption">
Figure 6.6: Biomass against dbh for 42 trees in Ghana measured by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span> (points), and predictions (plots) by a polynomial regression of <span class="math inline">\(\ln(B)\)</span> against <span class="math inline">\(\ln(D)\)</span>: (A) first-order polynomial (straight line); (B) second-order polynomial (parabolic); (C) third-order polynomial; (D) fourth-order polynomial.
</p>
</div>
<div class="filrouge">
<ol start="10" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:flnDlnH" class="exercise"><strong>Red line 6.4  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Multiple regression between <span class="math inline">\(\ln(B)\)</span>, <span class="math inline">\(\ln(D)\)</span> and <span class="math inline">\(\ln(H)\)</span></strong></span></p>
</div>
<p>The graphical exploration (red lines <a href="5-explo.html#exr:feBvD2H">5.2</a> and <a href="5-explo.html#exr:feln2">5.5</a>) showed that the combined variable <span class="math inline">\(D^2H\)</span> was linked to biomass by a power relation (i.e. a linear relation on a log scale): <span class="math inline">\(B=a(D^2H)^b\)</span>. We can, however, wonder whether the variables <span class="math inline">\(D^2\)</span> and <span class="math inline">\(H\)</span> have the same exponent <span class="math inline">\(b\)</span>, or whether they have different exponents: <span class="math inline">\(B=a\times(D^2)^{b_1}H^{b_2}\)</span>. Working on the log-transformed data (which in passing stabilizes the residual variance), means fitting a multiple regression of <span class="math inline">\(\ln(B)\)</span> against <span class="math inline">\(\ln(D)\)</span> and <span class="math inline">\(\ln(H)\)</span>:
<span class="math display">\[
\ln(B)=a+b_1\ln(D)+b_2\ln(H)+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]</span>
The regression is fitted by the ordinary least squares method. Fitting this multiple regression:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb27-1"><a href="6-fit.html#cb27-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(heig)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,])</span>
<span id="cb27-2"><a href="6-fit.html#cb27-2"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>yields:</p>
<pre class="Rout"><code>##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -8.9050     0.2855 -31.190  &lt; 2e-16 ***
## I(log(dbh))    1.8654     0.1604  11.632 4.35e-14 ***
## I(log(heig))   0.7083     0.2097   3.378   0.0017 **</code></pre>
<p>with a residual standard deviation of 0.4104 and <span class="math inline">\(R^2=\)</span> 0.9725 (0.971). The model is highly significant (Fisher’s test: <span class="math inline">\(F_{2,38}=671.5\)</span>, p-value <span class="math inline">\(&lt;2.2\times10^{-16}\)</span>). The model, all of whose coefficients are significantly different from zero, is written: <span class="math inline">\(\ln(B)=-8.9050+1.8654\ln(D)+0.7083\ln(H)\)</span>. By applying the exponential function to return to the starting data, the model becomes: <span class="math inline">\(B=1.357\times10^{-4}D^{1.8654}H^{0.7083}\)</span>. The exponent associated with height is a little less than half that associated with dbh, and is a little less than the exponent of 0.87238 found for the combined variable <span class="math inline">\(D^2H\)</span> (see red line <a href="6-fit.html#exr:rllnBvD2H">6.2</a>). An examination of the residuals:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb29-1"><a href="6-fit.html#cb29-1"></a><span class="kw">plot</span>(m, <span class="dt">which =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</span></code></pre></div>
<p>shows nothing in particular (Figure <a href="6-fit.html#fig:flnDHres">6.7</a>).</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:flnDHres"></span>
<img src="source/figures/flnDlnHres.png" alt="Residuals plotted against fitted values (left) and quantile–quantile plot (right) for residuals of the multiple regression of \(\ln(B)\) against \(\ln(D)\) and \(\ln(H)\) fitted for the 42 trees measured by Henry et al. (2010) in Ghana." width="80%" />
<p class="caption">
Figure 6.7: Residuals plotted against fitted values (left) and quantile–quantile plot (right) for residuals of the multiple regression of <span class="math inline">\(\ln(B)\)</span> against <span class="math inline">\(\ln(D)\)</span> and <span class="math inline">\(\ln(H)\)</span> fitted for the 42 trees measured by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span> in Ghana.
</p>
</div>
</div>
</div>
<div id="pond" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Weighted regression</h3>
<p>Let us now suppose that we want to fit directly a polynomial model of biomass <span class="math inline">\(B\)</span> against <span class="math inline">\(D\)</span>. For example, for a second-order polynomial:
<span class="math display" id="eq:pol">\[\begin{equation}
B=a_0+a_1D+a_2D^2+\varepsilon\tag{6.9}
\end{equation}\]</span>
As mentioned earlier, the variability of the biomass nearly always (if not always<span class="math inline">\(\ldots\)</span>) increases with tree dbh <span class="math inline">\(D\)</span>. In other words, the variance of <span class="math inline">\(\varepsilon\)</span> increases with <span class="math inline">\(D\)</span>, in contradiction with the homoscedasticity hypothesis required by multiple regression. Therefore, model <a href="6-fit.html#eq:pol">(6.9)</a> cannot be fitted by multiple regression. Log transformation stabilizes the residual variance (we will return to this in section <a href="6-fit.html#trans">6.1.5</a>). If we take <span class="math inline">\(\ln(B)\)</span> as the response variable, the model becomes:
<span class="math display" id="eq:logpol">\[\begin{equation}
\ln(B)=\ln(a_0+a_1D+a_2D^2)+\varepsilon\tag{6.10}
\end{equation}\]</span>
It is reasonable to assume that the variance of the residuals in such a model is constant. But unfortunately, this is no longer a linear model as the dependency of the response variables on the coefficients <span class="math inline">\(a_0\)</span>, <span class="math inline">\(a_1\)</span> and <span class="math inline">\(a_2\)</span> is not linear. Therefore, model <a href="6-fit.html#eq:logpol">(6.10)</a> cannot be fitted by a linear model. We will see later (§ <a href="6-fit.html#nlm">6.2</a>) how to fit this non-linear model.</p>
<p>A weighted regression can be used to fit a model such as <a href="6-fit.html#eq:pol">(6.9)</a> where the variance of the residuals is not constant, while nevertheless using the formalism of the linear model. It may be regarded as extending multiple regression to the case where the variance of the residuals is not constant. Weighted regression was developed in forestry between the 1960s and the 1980s, particularly thanks to the work of <span class="citation">Cunia (<a href="bibliography.html#ref-cunia64" role="doc-biblioref">1964</a>)</span>; <span class="citation">Cunia (<a href="bibliography.html#ref-cunia87b" role="doc-biblioref">1987</a><a href="bibliography.html#ref-cunia87b" role="doc-biblioref">c</a>)</span>. It was widely used to fit linear tables <span class="citation">(Whraton and Cunia <a href="bibliography.html#ref-whraton87" role="doc-biblioref">1987</a>; Brown, Gillespie, and Lugo <a href="bibliography.html#ref-brown89" role="doc-biblioref">1989</a>; Parresol <a href="bibliography.html#ref-parresol99" role="doc-biblioref">1999</a>)</span>, before being replaced by more efficient fitting methods we will see in section <a href="6-fit.html#lme">6.1.4</a>.</p>
<p>A weighted regression is written in an identical fashion to the multiple regression <a href="6-fit.html#eq:regmul">(6.3)</a>:
<span class="math display">\[
Y=a_0+a_1X_1+a_2X_2+\ldots+a_pX_p+\varepsilon
\]</span>
except that it is no longer assumed that the variance of the residuals is constant. Each observation now has its own residual variance <span class="math inline">\(\sigma_i^2\)</span>:
<span class="math display">\[
\varepsilon_i\sim\mathcal{N}(0,\ \sigma_i)
\]</span>
Each observation is associated with a positive <em>weight</em> <span class="math inline">\(w_i\)</span> (hence the term “weighted” to describe this regression), which is inversely proportional to the residual variance:
<span class="math display">\[
w_i\propto1/\sigma_i^2
\]</span>
The proportionality coefficient between <span class="math inline">\(w_i\)</span> and <span class="math inline">\(1/\sigma_i^2\)</span> does not need to be specified as the method is insensitive to any renormalization of the weights (as we shall see in the next section). Associating each observation with a weight inversely proportional to its variance is quite natural. If an observation has a very high residual variance, this means that it has very high intrinsic variability, and it is therefore quite natural that it has less weight in model fitting. As we cannot estimate <span class="math inline">\(n\)</span> weights from <span class="math inline">\(n\)</span> observations, we must model the weighting. When dealing with biological data such as biomass or volume, heteroscedasticity of the residuals corresponds nearly always to a power relation between the residual variance and the size of the trees. We may therefore assume that among the <span class="math inline">\(p\)</span> effect variables of the weighted regression, there is one (typically tree dbh) such that <span class="math inline">\(\sigma_i\)</span> is a power relation of this variable. Without loss of generality, we can put forward that this variable is <span class="math inline">\(X_1\)</span>, such that:
<span class="math display">\[
\sigma_i=k\ X_{i1}^c
\]</span>
where <span class="math inline">\(k&gt;0\)</span> and <span class="math inline">\(c\geq0\)</span>. In consequence:
<span class="math display">\[
w_i\propto X_{i1}^{-2c}
\]</span>
The exponent <span class="math inline">\(c\)</span> cannot be estimated in the same manner as <span class="math inline">\(a_0\)</span>, <span class="math inline">\(a_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(a_p\)</span>, but must be fixed in advance. This is the main drawback of this fitting method. We will see later how to select a value for exponent <span class="math inline">\(c\)</span>. By contrast, the multiplier <span class="math inline">\(k\)</span> does not need to be estimated as the weights <span class="math inline">\(w_i\)</span> are defined only to within a multiplier factor. In practice therefore we can put forward <span class="math inline">\(w_i=X_{i1}^{-2c}\)</span>.</p>
<div id="estimating-coefficients-2" class="section level4 unnumbered">
<h4>Estimating coefficients</h4>
<p>The least squares method is adjusted to take account of the weighting of the observations. We therefore speak of the weighted least squares method. For a fixed exponent <span class="math inline">\(c\)</span>, estimations of coefficients <span class="math inline">\(a_0\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(a_p\)</span> have values that minimize the weighted sum of squares:
<span class="math display">\[
\mathrm{SSE}(a_0,\ a_1,\ \ldots,\ a_p)=\sum_{i=1}^nw_i\ \varepsilon_i^2
=\sum_{i=1}^nw_i(Y_i-a_0-a_1X_{i1}-\ldots-a_pX_{ip})^2
\]</span>
or as a matrix:
<span class="math display">\[
\mathrm{SSE}(\mathbf{a})={}^{\mathrm{t}}{(\mathbf{Y}-\hat{\mathbf{Y}})}\mathbf{W}
(\mathbf{Y}-\hat{\mathbf{Y}})={}^{\mathrm{t}}{(\mathbf{Y}-\mathbf{X}\mathbf{a})}
\mathbf{W} (\mathbf{Y}-\mathbf{X}\mathbf{a})
\]</span>
where <span class="math inline">\(\mathbf{W}\)</span> is the <span class="math inline">\(n\times n\)</span> diagonal matrix with <span class="math inline">\(w_i\)</span> on its diagonal:
<span class="math display">\[
\mathbf{W}=\left[
\begin{array}{ccc}
w_1 &amp;&amp; \mathbf{0}\\ %
&amp; \ddots &amp; \\ %
\mathbf{0} &amp;&amp; w_n
\end{array}
\right]
\]</span>
The least SS is obtained for <span class="citation">(Magnus and Neudecker <a href="bibliography.html#ref-magnus07" role="doc-biblioref">2007</a>)</span>:
<span class="math display">\[
\hat{\mathbf{a}}=\arg\min_{\mathbf{a}}\mathrm{SSE}(\mathbf{a})
=({}^{\mathrm{t}}{\mathbf{X}}\mathbf{W}\mathbf{X})^{-1}{}^{\mathrm{t}}{\mathbf{X}}\mathbf{W}\mathbf{Y}
\]</span>
This minimum does not change when all the weights <span class="math inline">\(w_i\)</span> are multiplied by the same scalar, clearly proving that the method is not sensitive to normalization of weights. We can check that the estimation yielded by the weighted least squares method applied to observations <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span> gives the same result as that yielded by the ordinary least squares method applied to observations <span class="math inline">\(\sqrt{w_i}\ X_{ij}\)</span> and <span class="math inline">\(\sqrt{w_i}\ Y_i\)</span>. Like previously, this fitting method has the advantage that the estimations of the coefficients have an explicit expression.</p>
</div>
<div id="interpreting-results-and-checking-hypotheses" class="section level4 unnumbered">
<h4>Interpreting results and checking hypotheses</h4>
<p>The results of the weighted regression are interpreted in exactly the same fashion as for the multiple regression. The same may also be said of the residuals-related hypotheses, except that the residuals are replaced by the weighted residuals <span class="math inline">\(\varepsilon&#39;_i=\sqrt{w_i}\ \varepsilon_i=\varepsilon_i/X_i^c\)</span>. The graph of the weighted residuals <span class="math inline">\(\varepsilon_i&#39;\)</span> against the fitted values must not show any particular trend (like in Figure <a href="6-fit.html#fig:res3">6.8</a>B). If the cluster of points given by plotting the residuals against the fitted values takes on a funnel-like shape open toward the right (as in Figure <a href="6-fit.html#fig:res3">6.8</a>A), then the value of exponent <span class="math inline">\(c\)</span> is too low (the lowest possible value being zero). If the cluster of points takes on a funnel-like shape closed toward the right (as in Figure <a href="6-fit.html#fig:res3">6.8</a>C), then the value of exponent <span class="math inline">\(c\)</span> is too high.</p>
<div class="figure" style="text-align: center"><span id="fig:res3"></span>
<img src="source/figures/resid3.png" alt="Plot of weighted residuals against fitted values for a weighted regression: (A) the value of exponent \(c\) is too low for the weighting; (B) the value of exponent \(c\) is appropriate; (C) the value of exponent \(c\) is too high. It should be noted that as the value of c increases, the rank of the values for the weighted values \(\varepsilon/X^c\) decreases." width="80%" />
<p class="caption">
Figure 6.8: Plot of weighted residuals against fitted values for a weighted regression: (A) the value of exponent <span class="math inline">\(c\)</span> is too low for the weighting; (B) the value of exponent <span class="math inline">\(c\)</span> is appropriate; (C) the value of exponent <span class="math inline">\(c\)</span> is too high. It should be noted that as the value of c increases, the rank of the values for the weighted values <span class="math inline">\(\varepsilon/X^c\)</span> decreases.
</p>
</div>
</div>
<div id="chx" class="section level4 unnumbered">
<h4>Choosing the right weighting</h4>
<p>A crucial point in weighted regression is the prior selection of a value for exponent c that defines the weighting. Several methods can be used to determine <span class="math inline">\(c\)</span>. The first consists in proceeding by trial and error based on the appearance of the plot of the residuals against the fitted values. As the appearance of the plot provides information on the pertinence of the value of <span class="math inline">\(c\)</span> (Figure <a href="6-fit.html#fig:res3">6.8</a>), several values of <span class="math inline">\(c\)</span> can simply be tested until the cluster of points formed by plotting the weighted residuals against the fitted values no longer shows any particular trend.</p>
<p>As linear regression is robust with regard to the hypothesis that the variance of the residuals is constant, there is no need to determine <span class="math inline">\(c\)</span> with great precision. In most cases it is enough to test integers of <span class="math inline">\(c\)</span>. In practice, the weighted regression may be fitted for <span class="math inline">\(c\)</span> values of 0, 1, 2, 3 or 4 (it is rarely useful to go above 4), and retain the integer value that yields the best appearance for the cluster of points in the plot of the weighted residuals against the fitted values. This simple method is generally amply sufficient.</p>
<p>If we are looking to obtain a more precise value for exponent <span class="math inline">\(c\)</span>, we can calculate approximately the conditional variance of response variable <span class="math inline">\(Y\)</span> as we know <span class="math inline">\(X_1\)</span>:</p>
<ol style="list-style-type: decimal">
<li>divide <span class="math inline">\(X_1\)</span> into <span class="math inline">\(K\)</span> classes centered on <span class="math inline">\(X_{1k}\)</span> (<span class="math inline">\(k=1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(K\)</span>);</li>
<li>calculate the empirical variance, <span class="math inline">\(\sigma^2_k\)</span>, of <span class="math inline">\(Y\)</span> for the observations in class <span class="math inline">\(k\)</span> (avec <span class="math inline">\(k=1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(K\)</span>);</li>
<li>plot the linear regression of <span class="math inline">\(\ln(\sigma_k)\)</span> against <span class="math inline">\(\ln(X_{1k})\)</span>.</li>
</ol>
<p>The slope of this regression is an estimation of <span class="math inline">\(c\)</span>.</p>
<p>The third way of estimating <span class="math inline">\(c\)</span> consists in looking for the value of <span class="math inline">\(c\)</span> that minimizes <span class="citation">Furnival (<a href="bibliography.html#ref-furnival61" role="doc-biblioref">1961</a>)</span> index.</p>
<div class="filrouge">
<ol start="11" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:frlpD2H" class="exercise"><strong>Red line 6.5  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Weighted linear regression between <span class="math inline">\(B\)</span> and <span class="math inline">\(D^2H\)</span></strong></span></p>
</div>
<p>The exploratory analysis of the relation between biomass and <span class="math inline">\(D^2H\)</span> showed (red line <a href="5-explo.html#exr:feBvD2H">5.2</a>) that this relation is linear, but that the variance of the biomass increases with <span class="math inline">\(D^2H\)</span>. We can therefore fit a weighted linear regression of biomass <span class="math inline">\(B\)</span> against <span class="math inline">\(D^2H\)</span>:
<span class="math display">\[
B=a+bD^2H+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]</span>
The linear regression is fitted by the weighted least squares method, meaning that we must beforehand know the value of exponent <span class="math inline">\(c\)</span>. Let us first estimate coefficient <span class="math inline">\(c\)</span> for the weighting of the observations. To do this we must divide the observations into dbh classes and calculate the standard deviation of the biomass in each dbh class:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb30-1"><a href="6-fit.html#cb30-1"></a>D &lt;-<span class="st"> </span><span class="kw">quantile</span>(dat<span class="op">$</span>dbh, (<span class="dv">0</span><span class="op">:</span><span class="dv">5</span>) <span class="op">/</span><span class="st"> </span><span class="dv">5</span>)</span>
<span id="cb30-2"><a href="6-fit.html#cb30-2"></a>i &lt;-<span class="st"> </span><span class="kw">findInterval</span>(dat<span class="op">$</span>dbh, D, <span class="dt">rightmost.closed =</span> <span class="ot">TRUE</span>)</span>
<span id="cb30-3"><a href="6-fit.html#cb30-3"></a>sdB &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">D =</span> (D[<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>D[<span class="op">-</span><span class="dv">6</span>]) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">sdB =</span> <span class="kw">tapply</span>(dat<span class="op">$</span>Btot, i, sd))</span></code></pre></div>
<p>Object <code>D</code> contains the bounds of the dbh classes that are calculated such to have 5 classes containing approximately the same number of observations: Object <code>i</code> contains the number of the dbh class to which each observation belongs. Figure <a href="6-fit.html#fig:fpond">6.9</a>, obtained by the command:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb31-1"><a href="6-fit.html#cb31-1"></a><span class="kw">with</span>(sdB, <span class="kw">plot</span>(</span>
<span id="cb31-2"><a href="6-fit.html#cb31-2"></a>  <span class="dt">x =</span> D, </span>
<span id="cb31-3"><a href="6-fit.html#cb31-3"></a>  <span class="dt">y =</span> sdB, </span>
<span id="cb31-4"><a href="6-fit.html#cb31-4"></a>  <span class="dt">log =</span> <span class="st">&quot;xy&quot;</span>, </span>
<span id="cb31-5"><a href="6-fit.html#cb31-5"></a>  <span class="dt">xlab =</span> <span class="st">&quot;Diameter (cm)&quot;</span>, </span>
<span id="cb31-6"><a href="6-fit.html#cb31-6"></a>  <span class="dt">ylab =</span> <span class="st">&quot;Biomass standard deviation (t)&quot;</span></span>
<span id="cb31-7"><a href="6-fit.html#cb31-7"></a>  ))</span></code></pre></div>
<p>shows a plot of the standard deviation of the biomass against the median dbh of each dbh class, on a log scale. The points are roughly aligned along a straight line, confirming that the power model is appropriate for modeling the residual variance. The linear regression of the log of the standard deviation of the biomass against the log of the median dbh for each class, fitted by the command:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb32-1"><a href="6-fit.html#cb32-1"></a><span class="kw">summary</span>(<span class="kw">lm</span>(<span class="kw">log</span>(sdB) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(D)), <span class="dt">data =</span> sdB))</span></code></pre></div>
<p>yields:</p>
<pre class="Rout"><code>##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  -7.3487     0.7567  -9.712  0.00232 **
## I(log(D))     2.0042     0.1981  10.117  0.00206 **</code></pre>
<p>The slope of the regression corresponds to <span class="math inline">\(c\)</span>, and is <span class="math inline">\(2\)</span>. Thus, the standard deviation <span class="math inline">\(\sigma\)</span> of the biomass is approximately proportional to <span class="math inline">\(D^2\)</span>, and we will select a weighting of the observations that is inversely proportional to <span class="math inline">\(D^4\)</span>. The weighted regression of biomass <span class="math inline">\(B\)</span> against <span class="math inline">\(D^2H\)</span> with this weighting, and fitted by the command:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb34-1"><a href="6-fit.html#cb34-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(Btot <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig), <span class="dt">data =</span> dat, <span class="dt">weights =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">4</span>)</span>
<span id="cb34-2"><a href="6-fit.html#cb34-2"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>yields:</p>
<pre class="Rout"><code>##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     1.181e-03  2.288e-03   0.516    0.608    
## I(dbh^2 * heig) 2.742e-05  1.527e-06  17.957   &lt;2e-16 ***</code></pre>
<p>An examination of the result of this fitting shows that the y-intercept is not significantly different from zero. This leads us to fit a new weighted regression of biomass <span class="math inline">\(B\)</span> against <span class="math inline">\(D^2H\)</span> without an intercept:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb36-1"><a href="6-fit.html#cb36-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(Btot <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig), <span class="dt">data =</span> dat, <span class="dt">weights =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">4</span>)</span>
<span id="cb36-2"><a href="6-fit.html#cb36-2"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##                  Estimate Std. Error t value Pr(&gt;|t|)    
## I(dbh^2 * heig) 2.747e-05  1.511e-06   18.19   &lt;2e-16 ***</code></pre>
<p>The model is therefore: <span class="math inline">\(B=2.747\times10^{-5}D^2H\)</span>, with <span class="math inline">\(R^2\)</span> = 0.8897 and a residual standard deviation <span class="math inline">\(k=0.0003513\)</span> tonnes cm<sup>-2</sup>. The model is highly significant (Fisher’s test: <span class="math inline">\(F_{1,41}=330.8\)</span>, p-value <span class="math inline">\(&lt;2.2\times10^{-16}\)</span>). As this model was fitted directly on non-transformed data, it should be noted that it was unnecessary to withdraw the observations of zero biomass (unlike the situation in red line <a href="6-fit.html#exr:rllnBvD2H">6.2</a>). Figure <a href="6-fit.html#fig:fD2Hpres">6.10</a>A, obtained by the command:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb38-1"><a href="6-fit.html#cb38-1"></a><span class="kw">plot</span>(</span>
<span id="cb38-2"><a href="6-fit.html#cb38-2"></a>  <span class="dt">x =</span> <span class="kw">fitted</span>(m),  </span>
<span id="cb38-3"><a href="6-fit.html#cb38-3"></a>  <span class="dt">y =</span> <span class="kw">residuals</span>(m) <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">2</span>, </span>
<span id="cb38-4"><a href="6-fit.html#cb38-4"></a>  <span class="dt">xlab =</span> <span class="st">&quot;Fitted values&quot;</span>, </span>
<span id="cb38-5"><a href="6-fit.html#cb38-5"></a>  <span class="dt">ylab =</span> <span class="st">&quot;Weighted residuals&quot;</span></span>
<span id="cb38-6"><a href="6-fit.html#cb38-6"></a>  )</span></code></pre></div>
<p>gives a plot of the weighted residuals against the fitted values. By way of a comparison, Figure <a href="6-fit.html#fig:fD2Hpres">6.10</a>B shows a plot of the weighted residuals against the fitted values when the weighting is too low (with weights inversely proportional to <span class="math inline">\(D^2\)</span>):</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb39-1"><a href="6-fit.html#cb39-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(Btot <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig), <span class="dt">data =</span> dat, <span class="dt">weights =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb39-2"><a href="6-fit.html#cb39-2"></a><span class="kw">plot</span>(</span>
<span id="cb39-3"><a href="6-fit.html#cb39-3"></a>  <span class="dt">x =</span> <span class="kw">fitted</span>(m), </span>
<span id="cb39-4"><a href="6-fit.html#cb39-4"></a>  <span class="dt">y =</span> <span class="kw">residuals</span>(m) <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh, </span>
<span id="cb39-5"><a href="6-fit.html#cb39-5"></a>  <span class="dt">xlab =</span> <span class="st">&quot;Fitted values&quot;</span>, </span>
<span id="cb39-6"><a href="6-fit.html#cb39-6"></a>  <span class="dt">ylab =</span> <span class="st">&quot;Weighted residuals&quot;</span></span>
<span id="cb39-7"><a href="6-fit.html#cb39-7"></a>  )</span></code></pre></div>
<p>whereas <a href="6-fit.html#fig:fD2Hpres">6.10</a>C shows a plot of the weighted residuals against the fitted values when the weighting is too high (with weights inversely proportional to <span class="math inline">\(D^5\)</span>):</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb40-1"><a href="6-fit.html#cb40-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(Btot <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig), <span class="dt">data =</span> dat, <span class="dt">weights =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">5</span>)</span>
<span id="cb40-2"><a href="6-fit.html#cb40-2"></a><span class="kw">plot</span>(</span>
<span id="cb40-3"><a href="6-fit.html#cb40-3"></a>  <span class="dt">x =</span> <span class="kw">fitted</span>(m), </span>
<span id="cb40-4"><a href="6-fit.html#cb40-4"></a>  <span class="dt">y =</span> <span class="kw">residuals</span>(m) <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="fl">2.5</span>, </span>
<span id="cb40-5"><a href="6-fit.html#cb40-5"></a>  <span class="dt">xlab =</span> <span class="st">&quot;Fitted values&quot;</span>, </span>
<span id="cb40-6"><a href="6-fit.html#cb40-6"></a>  <span class="dt">ylab =</span> <span class="st">&quot;Weighted residuals&quot;</span></span>
<span id="cb40-7"><a href="6-fit.html#cb40-7"></a>  )</span></code></pre></div>
<p>Thus, the coefficient <span class="math inline">\(c=2\)</span> for the weighting indeed proves to be that which is suitable.</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fpond"></span>
<img src="source/figures/fpond.png" alt="Plot of standard deviation of biomass calculated in five dbh classes against median dbh of the class (on a log scale), for 42 trees measured in Ghana by Henry et al. (2010)." width="40%" />
<p class="caption">
Figure 6.9: Plot of standard deviation of biomass calculated in five dbh classes against median dbh of the class (on a log scale), for 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fD2Hpres"></span>
<img src="source/figures/fD2Hpres.png" alt="Plot of weighted residuals against fitted values for a weighted regression of biomass against \(D^2H\) for 42 trees measured in Ghana by Henry et al. (2010): (A) the weighting is inversely proportional to \(D^4\); (B) the weighting is inversely proportional to \(D^2\); (C) the weighting is inversely proportional to \(D^5\)." width="80%" />
<p class="caption">
Figure 6.10: Plot of weighted residuals against fitted values for a weighted regression of biomass against <span class="math inline">\(D^2H\)</span> for 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>: (A) the weighting is inversely proportional to <span class="math inline">\(D^4\)</span>; (B) the weighting is inversely proportional to <span class="math inline">\(D^2\)</span>; (C) the weighting is inversely proportional to <span class="math inline">\(D^5\)</span>.
</p>
</div>
<div class="filrouge">
<ol start="12" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fDpara" class="exercise"><strong>Red line 6.6  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Weighted polynomial regression between <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span></strong></span></p>
</div>
<p>The exploratory analysis (red line <a href="5-explo.html#exr:feBvD">5.1</a>) showed that the relation between biomass and dbh was parabolic, with the variance of the biomass increasing with dbh. Log-transformation rendered the relation between biomass and dbh linear, but we can also model the relation between biomass and dbh directly by a parabolic relation:
<span class="math display">\[
B=a_0+a_1D+a_2D^2+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]</span>
In red line <a href="6-fit.html#exr:frlpD2H">6.5</a> we saw that the value <span class="math inline">\(c=2\)</span> of the exponent was suitable for modeling the conditional standard deviation of the biomass when we know the dbh. We will therefore fit the multiple regression using the weighted least squares method with weighting of the observations proportional to <span class="math inline">\(1/D^4\)</span>:</p>
<p>which yields:</p>
<pre class="Rout"><code>##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.127e-02  6.356e-03   1.772  0.08415 .  
## dbh         -7.297e-03  2.140e-03  -3.409  0.00153 ** 
## I(dbh^2)     1.215e-03  9.014e-05  13.478 2.93e-16 ***</code></pre>
<p>with a residual standard deviation <span class="math inline">\(k=\)</span>  tonnes~cm<sup>-2</sup> and <span class="math inline">\(R^2=\)</span> . The y-intercept is not significantly different from zero. We can therefore again fit a parabolic relation but without the intercept:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb42-1"><a href="6-fit.html#cb42-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(Btot <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span>dbh <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(dbh<span class="op">^</span><span class="dv">2</span>), <span class="dt">data =</span> dat, <span class="dt">weights =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">4</span>)</span>
<span id="cb42-2"><a href="6-fit.html#cb42-2"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##            Estimate Std. Error t value Pr(&gt;|t|)    
## dbh      -3.840e-03  9.047e-04  -4.245 0.000126 ***
## I(dbh^2)  1.124e-03  7.599e-05  14.789  &lt; 2e-16 ***</code></pre>
<p>with a residual standard deviation <span class="math inline">\(k=\)</span>  tonnes cm<sup>-2</sup> and <span class="math inline">\(R^2=\)</span> . The model is highly significant (Fisher’s test: <span class="math inline">\(F_{2,40}=124.4\)</span>, p-value <span class="math inline">\(&lt; 2.2\times10^{-16}\)</span>) and is written: <span class="math inline">\(B=-3.840\times10^{-3}D+1.124\times10^{-3}D^2\)</span>. Figure <a href="6-fit.html#fig:fresDD2">6.11</a> obtained by the command:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb44-1"><a href="6-fit.html#cb44-1"></a><span class="kw">plot</span>(</span>
<span id="cb44-2"><a href="6-fit.html#cb44-2"></a>  <span class="dt">x =</span> <span class="kw">fitted</span>(m), </span>
<span id="cb44-3"><a href="6-fit.html#cb44-3"></a>  <span class="dt">y =</span> <span class="kw">residuals</span>(m) <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">2</span>,</span>
<span id="cb44-4"><a href="6-fit.html#cb44-4"></a>  <span class="dt">xlab =</span> <span class="st">&quot;fitted values&quot;</span>, </span>
<span id="cb44-5"><a href="6-fit.html#cb44-5"></a>  <span class="dt">ylab =</span> <span class="st">&quot;Weighted residuals&quot;</span></span>
<span id="cb44-6"><a href="6-fit.html#cb44-6"></a>  )</span></code></pre></div>
<p>gives a plot of the weighted residuals against the fitted values.</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fresDD2"></span>
<img src="source/figures/fresDD2.png" alt="Plot of weighted residuals against fitted values for a weighted regression of biomass against \(D\) and \(D^2\) for 42 trees measured in Ghana by Henry et al. (2010)." width="40%" />
<p class="caption">
Figure 6.11: Plot of weighted residuals against fitted values for a weighted regression of biomass against <span class="math inline">\(D\)</span> and <span class="math inline">\(D^2\)</span> for 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>.
</p>
</div>
</div>
</div>
<div id="lme" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Linear regression with variance model</h3>
<p>An alternative to the weighted regression is to use explicitly a model for the variance of the residuals. As previously, it is realistic to assume that there is an effect variable (without loss of generality, the first) such that the residual standard deviation is a power function of this variable:
<span class="math display" id="eq:modvar">\[\begin{equation}
\mathrm{Var}(\varepsilon)=(kX_1^c)^2\tag{6.11}
\end{equation}\]</span>
where <span class="math inline">\(k&gt;0\)</span> and <span class="math inline">\(c\geq0\)</span>. The model is therefore written:
<span class="math display" id="eq:modmoy">\[\begin{equation}
Y=a_0+a_1X_1+a_2X_2+\ldots+a_pX_p+\varepsilon\tag{6.12}
\end{equation}\]</span>
where:
<span class="math display">\[
\varepsilon\sim\mathcal{N}(0,\ kX_1^c)
\]</span>
In its form the model is little different from the weighted regression. Regarding its content, it has one fundamental difference: the coefficients <span class="math inline">\(k\)</span> and <span class="math inline">\(c\)</span> are now model parameters that need to be estimated, in the same manner as coefficients <span class="math inline">\(a_0\)</span>, <span class="math inline">\(a_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(a_p\)</span>. Because of these <span class="math inline">\(k\)</span> and <span class="math inline">\(c\)</span> parameters that need to be estimated, the least squares method can no longer be used to estimate model coefficients. Another estimation method has to be used, the maximum likelihood method. Strictly speaking, the model defined by <a href="6-fit.html#eq:modvar">(6.11)</a> and <a href="6-fit.html#eq:modmoy">(6.12)</a> is not a linear model. It is far closer conceptually to the non-linear model we will see in section <a href="6-fit.html#nlm">6.2</a>. We will not go any further here in presenting the non-linear model: the model fitting method defined by <a href="6-fit.html#eq:modvar">(6.11)</a> and <a href="6-fit.html#eq:modmoy">(6.12)</a> will be presented as a special case of the non-linear model in section <a href="6-fit.html#nlm">6.2</a>.</p>
<div class="filrouge">
<ol start="13" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fD2Hvar" class="exercise"><strong>Red line 6.7  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Polynomial regression between <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span> with variance model</strong></span></p>
</div>
<p>Pre-empting section <a href="6-fit.html#nlm">6.2</a>, we will fit a linear regression of biomass against <span class="math inline">\(D^2H\)</span> by specifying a power model on the residual variance:
<span class="math display">\[
B=a+bD^2H+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]</span>
We will see later (§ <a href="6-fit.html#nlm">6.2</a>) that this model is fitted by maximum likelihood. This regression is in spirit very similar to the previously-constructed weighted regression of biomass against <span class="math inline">\(D^2H\)</span> (red line <a href="6-fit.html#exr:frlpD2H">6.5</a>), except that exponent <span class="math inline">\(c\)</span> used to define the weighting of the observations is now a parameter that must be estimated in its own right rather than a coefficient that is fixed in advance. The linear regression with variance model is fitted as follows:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb45-1"><a href="6-fit.html#cb45-1"></a><span class="kw">library</span>(nlme)</span>
<span id="cb45-2"><a href="6-fit.html#cb45-2"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(Btot <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig), <span class="dt">data =</span> dat))</span>
<span id="cb45-3"><a href="6-fit.html#cb45-3"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>)</span>
<span id="cb45-4"><a href="6-fit.html#cb45-4"></a><span class="kw">summary</span>(<span class="kw">nlme</span>(</span>
<span id="cb45-5"><a href="6-fit.html#cb45-5"></a>  <span class="dt">model   =</span> Btot <span class="op">~</span><span class="st"> </span>a <span class="op">+</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig, </span>
<span id="cb45-6"><a href="6-fit.html#cb45-6"></a>  <span class="dt">data    =</span> <span class="kw">cbind</span>(dat, <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>), </span>
<span id="cb45-7"><a href="6-fit.html#cb45-7"></a>  <span class="dt">fixed   =</span> a <span class="op">+</span><span class="st"> </span>b <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, </span>
<span id="cb45-8"><a href="6-fit.html#cb45-8"></a>  <span class="dt">start   =</span> start, </span>
<span id="cb45-9"><a href="6-fit.html#cb45-9"></a>  <span class="dt">groups  =</span> <span class="op">~</span>g, </span>
<span id="cb45-10"><a href="6-fit.html#cb45-10"></a>  <span class="dt">weights =</span> <span class="kw">varPower</span>(<span class="dt">form=</span><span class="op">~</span>dbh)</span>
<span id="cb45-11"><a href="6-fit.html#cb45-11"></a>  ))</span></code></pre></div>
<p>and yields (we will return in section <a href="6-fit.html#nlm">6.2</a> to the meaning of the start object <code>start</code>):</p>
<pre class="Rout"><code>##          Value    Std.Error DF    t-value      p-value
## a 1.286802e-03 2.421161e-03 40  0.5314813 5.980247e-01
## b 2.735025e-05 1.499931e-06 40 18.2343395 5.501856e-21</code></pre>
<p>with an estimated value of exponent <span class="math inline">\(c=\)</span> 1.9777361. Like for the weighted linear regression (red line <a href="6-fit.html#exr:frlpD2H">6.5</a>), the y-intercept proves not to be significantly different from zero. We can therefore refit the model without the intercept:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb47-1"><a href="6-fit.html#cb47-1"></a><span class="kw">summary</span>(<span class="kw">nlme</span>(</span>
<span id="cb47-2"><a href="6-fit.html#cb47-2"></a>  <span class="dt">model   =</span> Btot <span class="op">~</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig, </span>
<span id="cb47-3"><a href="6-fit.html#cb47-3"></a>  <span class="dt">data    =</span> <span class="kw">cbind</span>(dat, <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>), </span>
<span id="cb47-4"><a href="6-fit.html#cb47-4"></a>  <span class="dt">fixed   =</span> b <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, </span>
<span id="cb47-5"><a href="6-fit.html#cb47-5"></a>  <span class="dt">start   =</span> start[<span class="st">&quot;b&quot;</span>], </span>
<span id="cb47-6"><a href="6-fit.html#cb47-6"></a>  <span class="dt">groups  =</span> <span class="op">~</span>g, </span>
<span id="cb47-7"><a href="6-fit.html#cb47-7"></a>  <span class="dt">weights =</span> <span class="kw">varPower</span>(<span class="dt">form =</span> <span class="op">~</span>dbh)</span>
<span id="cb47-8"><a href="6-fit.html#cb47-8"></a>  ))</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##          Value  Std.Error DF  t-value      p-value
## b 2.740688e-05 1.4869e-06 41 18.43223 1.885592e-21</code></pre>
<p>with an estimated value of exponent <span class="math inline">\(c=\)</span> 1.9802634. This value is very similar to that evaluated for the weighted linear regression (<span class="math inline">\(c=2\)</span> in red line <a href="6-fit.html#exr:frlpD2H">6.5</a>). The fitted model is therefore: <span class="math inline">\(B=2.740688\times10^{-5}D^2H\)</span>, which is very similar to the model fitted by weighted linear regression (red line <a href="6-fit.html#exr:frlpD2H">6.5</a>).</p>
</div>
<div class="filrouge">
<ol start="14" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fDD2var" class="exercise"><strong>Red line 6.8  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Polynomial regression between <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span> with variance model</strong></span></p>
</div>
<p>Pre-empting section <a href="6-fit.html#nlm">6.2</a>, we will fit a multiple regression of biomass against <span class="math inline">\(D\)</span> and <span class="math inline">\(D^2\)</span> by specifying a power model on the residual variance:
<span class="math display">\[
B=a_0+a_1D+a_2D^2+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]</span>
We will see later (§ <a href="6-fit.html#nlm">6.2</a>) that this model is fitted by maximum likelihood. This regression is in spirit very similar to the previously-constructed polynomial regression of biomass against <span class="math inline">\(D\)</span> and <span class="math inline">\(D^2\)</span> (red line <a href="6-fit.html#exr:fDpara">6.6</a>), except that exponent <span class="math inline">\(c\)</span> employed to define the weighting of the observations is now a parameter that must be estimated in its own right rather than a coefficient that is fixed in advance. The linear regression with variance model is fitted as follows:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb49-1"><a href="6-fit.html#cb49-1"></a><span class="kw">library</span>(nlme)</span>
<span id="cb49-2"><a href="6-fit.html#cb49-2"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(Btot <span class="op">~</span><span class="st"> </span>dbh <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(dbh<span class="op">^</span><span class="dv">2</span>), <span class="dt">data =</span> dat))</span>
<span id="cb49-3"><a href="6-fit.html#cb49-3"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a0&quot;</span>, <span class="st">&quot;a1&quot;</span>, <span class="st">&quot;a2&quot;</span>)</span>
<span id="cb49-4"><a href="6-fit.html#cb49-4"></a><span class="kw">summary</span>(<span class="kw">nlme</span>(</span>
<span id="cb49-5"><a href="6-fit.html#cb49-5"></a>  <span class="dt">model   =</span> Btot <span class="op">~</span><span class="st"> </span>a0 <span class="op">+</span><span class="st"> </span>a1 <span class="op">*</span><span class="st"> </span>dbh <span class="op">+</span><span class="st"> </span>a2 <span class="op">*</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>, </span>
<span id="cb49-6"><a href="6-fit.html#cb49-6"></a>  <span class="dt">data    =</span> <span class="kw">cbind</span>(dat, <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>),</span>
<span id="cb49-7"><a href="6-fit.html#cb49-7"></a>  <span class="dt">fixed   =</span> a0 <span class="op">+</span><span class="st"> </span>a1 <span class="op">+</span><span class="st"> </span>a2 <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb49-8"><a href="6-fit.html#cb49-8"></a>  <span class="dt">start   =</span> start,</span>
<span id="cb49-9"><a href="6-fit.html#cb49-9"></a>  <span class="dt">groups  =</span> <span class="op">~</span>g,</span>
<span id="cb49-10"><a href="6-fit.html#cb49-10"></a>  <span class="dt">weights =</span> <span class="kw">varPower</span>(<span class="dt">form =</span> <span class="op">~</span>dbh)</span>
<span id="cb49-11"><a href="6-fit.html#cb49-11"></a>  ))</span></code></pre></div>
<p>and yields (we will return in section <a href="6-fit.html#nlm">6.2</a> to the meaning of the start object <code>start</code>):</p>
<pre class="Rout"><code>##           Value    Std.Error DF   t-value      p-value
## a0  0.009048499 5.139129e-03 39  1.760707 8.612839e-02
## a1 -0.006427411 1.872346e-03 39 -3.432812 1.428474e-03
## a2  0.001174388 9.406327e-05 39 12.485081 3.349019e-15</code></pre>
<p>with an estimated value of exponent <span class="math inline">\(c=2.127509\)</span>. Like for the weighted polynomial regression (red line <a href="6-fit.html#exr:fDpara">6.6</a>), the y-intercept proves not to be significantly different from zero. We can therefore refit the model without the intercept:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb51-1"><a href="6-fit.html#cb51-1"></a><span class="kw">summary</span>(<span class="kw">nlme</span>(</span>
<span id="cb51-2"><a href="6-fit.html#cb51-2"></a>  <span class="dt">model   =</span> Btot <span class="op">~</span><span class="st"> </span>a1 <span class="op">*</span><span class="st"> </span>dbh <span class="op">+</span><span class="st"> </span>a2 <span class="op">*</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>,</span>
<span id="cb51-3"><a href="6-fit.html#cb51-3"></a>  <span class="dt">data    =</span> <span class="kw">cbind</span>(dat, <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>),</span>
<span id="cb51-4"><a href="6-fit.html#cb51-4"></a>  <span class="dt">fixed   =</span> a1 <span class="op">+</span><span class="st"> </span>a2 <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, </span>
<span id="cb51-5"><a href="6-fit.html#cb51-5"></a>  <span class="dt">start   =</span> start[<span class="kw">c</span>(<span class="st">&quot;a1&quot;</span>, <span class="st">&quot;a2&quot;</span>)],</span>
<span id="cb51-6"><a href="6-fit.html#cb51-6"></a>  <span class="dt">groups  =</span> <span class="op">~</span>g, </span>
<span id="cb51-7"><a href="6-fit.html#cb51-7"></a>  <span class="dt">weights =</span> <span class="kw">varPower</span>(<span class="dt">form =</span> <span class="op">~</span>dbh)))</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##           Value    Std.Error DF   t-value      p-value
## a1 -0.003319456 6.891736e-04 40 -4.816574 2.120329e-05
## a2  0.001067068 7.597446e-05 40 14.045082 4.707228e-17</code></pre>
<p>with an estimated value of exponent <span class="math inline">\(c=2.139967\)</span>. This value is very similar to that evaluated for the weighted polynomial regression (<span class="math inline">\(c=2\)</span> in red line <a href="6-fit.html#exr:fDpara">6.6</a>). The fitted model is therefore: <span class="math inline">\(B=-3.319456\times10^{-3}D+1.067068\times10^{-3}D^2\)</span>, which is very similar to the model fitted by weighted polynomial regression (red line <a href="6-fit.html#exr:fDpara">6.6</a>).</p>
</div>
</div>
<div id="trans" class="section level3">
<h3><span class="header-section-number">6.1.5</span> Transforming variables</h3>
<p>Let us reconsider the example of the biomass model with a single entry (here dbh) of the power type:
<span class="math display" id="eq:p">\[\begin{equation}
B=aD^b\tag{6.13}
\end{equation}\]</span>
We have already seen that this is a non-linear model as <span class="math inline">\(B\)</span> is non linearly dependent upon coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. But this model can be rendered linear by log transformation. Relation <a href="6-fit.html#eq:p">(6.13)</a> is equivalent to: <span class="math inline">\(\ln(B)=\ln(a)+b\ln(D)\)</span>, which can be considered to be a linear regression of response variable <span class="math inline">\(Y=\ln(B)\)</span> against effect variable <span class="math inline">\(X=\ln(D)\)</span>. We can therefore estimate coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> (or rather <span class="math inline">\(\ln(a)\)</span> and <span class="math inline">\(b\)</span>) in power model <a href="6-fit.html#eq:p">(6.13)</a> by linear regression on log-transformed data. What about the residual error? If the linear regression on log-transformed data is appropriate, this means that <span class="math inline">\(\varepsilon=\ln(B)-\ln(a)-b\ln(D)\)</span> follows a centered normal distribution of constant standard deviation <span class="math inline">\(\sigma\)</span>. If we return to the starting data and use exponential transformation (which is the inverse transformation to log transformation), the residual error here is a factor:
<span class="math display">\[
B=aD^b\times\varepsilon&#39;
\]</span>
where <span class="math inline">\(\varepsilon&#39;=\exp(\varepsilon)\)</span>. Thus, we have moved from an additive error on log-transformed data to a multiplicative error on the starting data. Also, if <span class="math inline">\(\varepsilon\)</span> follows a centered normal distribution of standard deviation <span class="math inline">\(\sigma\)</span>, then, by definition, <span class="math inline">\(\varepsilon&#39;=\exp(\varepsilon)\)</span> follows a log-normal distribution of parameters 0 and <span class="math inline">\(\sigma\)</span>:
<span class="math display">\[
\varepsilon&#39;\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{LN}(0,\ \sigma)
\]</span>
Contrary to <span class="math inline">\(\varepsilon\)</span>, which has a mean of zero, the mean of <span class="math inline">\(\varepsilon&#39;\)</span> is not zero but:
<span class="math inline">\(\mathrm{E}(\varepsilon&#39;)=\exp(\sigma^2/2)\)</span>. The implications of this will be considered in chapter <a href="7-util.html#util">7</a>.</p>
<p>We can draw two lessons from this example:</p>
<ol style="list-style-type: decimal">
<li><p>when we are faced by a non-linear relation between a response variable and one (or several) effect variables, a transformation may render this relation linear;</p></li>
<li><p>this transformation of the variable affects not only the form of the relation between the effect variable(s) and the response variable, but also the residual error.</p></li>
</ol>
<p>Concerning the first point, this variables transformation means that we have two approaches for fitting a non-linear model. The first, when faced with a non-linear relation between a response variable and effect variables, consists in looking for a transformation that renders this relation linear, and thereafter using the approach employed for the linear model. The second consists in fitting the non-linear model directly, as we shall see in section <a href="6-fit.html#nlm">6.2</a>. Each approach has its advantages and its drawbacks. The linear model has the advantage of providing a relatively simple theoretical framework and, above all, the estimations of its coefficients have explicit expressions. The drawback is that the model linearization step introduces an additional difficulty, and the inverse transformation, if we are not careful, may produce prediction bias (we will return to this in chapter <a href="7-util.html#util">7</a>). Also, not all models can be rendered linear. For example, no variables transformation can render the following model linear: <span class="math inline">\(Y=a_0+a_1X+a_2\exp(a_3X)\)</span>.</p>
<p>Regarding the second point, we are now, therefore, obliged to distinguish the form of the relation between the response variable and the effect variables (we also speak of the mean model, i.e. the mean of the response variable <span class="math inline">\(Y\)</span>), and the form of the model for the residual error (we also speak of the variance model, i.e. the variance of <span class="math inline">\(Y\)</span>). This transformation of the variable affects both simultaneously. The art of transforming variables therefore lies in tackling the two simultaneously and thereby rendering the model linear with regard to its coefficients and stabilizing the variance of the residuals (i.e. rendering it constant).</p>
<div id="common-variable-transformations" class="section level4 unnumbered">
<h4>Common variable transformations</h4>
<p>Although theoretically there is no limit as to the variable transformations we can use, the transformations likely to concern volumes and biomasses are few in number. That most commonly employed to fit models is the log transformation. Given a power model:
<span class="math display">\[
Y=aX_1^{b_1}X_2^{b_2}\times\ldots\times X_p^{b_p}\times\varepsilon
\]</span>
the log transformation consists in replacing the variable <span class="math inline">\(Y\)</span> by its log: <span class="math inline">\(Y&#39;=\ln(Y)\)</span>, and each of the effect variables by their log: <span class="math inline">\(X_j&#39;=\ln(X_j)\)</span>. The resulting model corresponds to:
<span class="math display" id="eq:rm2">\[\begin{equation}
Y&#39;=a&#39;+b_1X_1&#39;+b_2X_2&#39;+\ldots+b_pX_p&#39;+\varepsilon&#39;\tag{6.14}
\end{equation}\]</span>
where <span class="math inline">\(\varepsilon&#39;=\ln(\varepsilon)\)</span>. The inverse transformation is the exponential for all the effect and response variables. In terms of residual error, the log transformation is appropriate if <span class="math inline">\(\varepsilon&#39;\)</span> follows a normal distribution, therefore if the error <span class="math inline">\(\varepsilon\)</span> is positive and multiplicative. It should be noted that log transformation poses a problem for variables that may take a zero value. In this case, the transformation <span class="math inline">\(X&#39;=\ln(X+1)\)</span> is used rather than <span class="math inline">\(X&#39;=\ln(X)\)</span> (or more generally, <span class="math inline">\(X&#39;=\ln(X+\mathrm{constant})\)</span> if <span class="math inline">\(X\)</span> can take a negative value, e.g. a dbh increment). By way of examples, the following biomass models:
<span class="math display">\[\begin{eqnarray*}
B &amp;=&amp; aD^b\\ %
B &amp;=&amp; a(D^2H)^b\\ %
B &amp;=&amp; a\rho^{b_1}D^{b_2}H^{b_3}
\end{eqnarray*}\]</span>
may be fitted by linear regression after log transformation of the data.</p>
<p>Given an exponential model:
<span class="math display" id="eq:expo">\[\begin{equation}
Y=a\exp(b_1X_1+b_2X_2+\ldots+b_pX_p)\times\varepsilon\tag{6.15}
\end{equation}\]</span>
the appropriate transformation consists in replacing variable <span class="math inline">\(Y\)</span> by its log: <span class="math inline">\(Y&#39;=\ln(Y)\)</span>, and not transforming the effect variables: <span class="math inline">\(X_j&#39;=X_j\)</span>. The resulting model is identical to <a href="6-fit.html#eq:rm2">(6.14)</a>. The inverse transformation is the exponential for the response variable and no change for the effect variables. In terms of residual error, this transformation is appropriate if <span class="math inline">\(\varepsilon&#39;\)</span> follows a normal distribution, therefore if the error <span class="math inline">\(\varepsilon\)</span> is positive and multiplicative. It should be noted that, without loss of generality, we can reparameterize the coefficients of the exponential model <a href="6-fit.html#eq:expo">(6.15)</a> by applying <span class="math inline">\(b&#39;_j=\exp(b_j)\)</span>. Strictly identical writing of exponential model <a href="6-fit.html#eq:expo">(6.15)</a> therefore yields:
<span class="math display">\[
Y=a{b&#39;_1}^{X_1}{b&#39;_2}^{X_2}\times\ldots\times{b&#39;_p}^{X_p}\times\varepsilon
\]</span>
For example, the following biomass model:
<span class="math display">\[
B=\exp\{a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3\}
\]</span>
may be fitted by linear regression after this type of variable transformation (with, in this example, <span class="math inline">\(X_j=[\ln(D)]^j\)</span>).</p>
<p>The Box-Cox transformation generalizes the log transformation. It is in fact a family of transformations indexed by a parameter <span class="math inline">\(\xi\)</span>. Given a variable <span class="math inline">\(X\)</span>, its Box-Cox transform <span class="math inline">\(X&#39;_{\xi}\)</span> corresponds to:
<span class="math display">\[
X&#39;_{\xi}=\left\{
\begin{array}{lcl}
(X^{\xi}-1)/\xi &amp;&amp; (\xi\neq0)\\ %
\ln(X)=\lim_{\xi\rightarrow0}(X^{\xi}-1)/\xi &amp;&amp; (\xi=0)
\end{array}
\right.
\]</span>
The Box-Cox transformation can be used to convert the question of choosing a variable transformation into a question of estimating parameter <span class="math inline">\(\xi\)</span> <span class="citation">(Hoeting et al. <a href="bibliography.html#ref-hoeting99" role="doc-biblioref">1999</a>)</span>.</p>
</div>
<div id="apart" class="section level4 unnumbered">
<h4>A special variable transformation</h4>
<p>The usual variable transformations change the form of the relation between the response variable and the effect variable. When the cluster of points <span class="math inline">\((X_i,\ Y_i)\)</span> formed by plotting the response variable against the effect variable is a straight line with heteroscedasticity, as shown in Figure <a href="6-fit.html#fig:linh">6.12</a>, then variable transformation needs to be employed to stabilize the variance of <span class="math inline">\(Y\)</span>, though without affecting the linear nature of the relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The particular case illustrated by <a href="6-fit.html#fig:linh">6.12</a> occurs fairly often when an allometric equation is fitted between two values that vary in a proportional manner <span class="citation">(see for example Ngomanda et al. <a href="bibliography.html#ref-ngomanda12" role="doc-biblioref">2012</a>)</span>. The linear nature of the relation between X and Y means that the model has the form:
<span class="math display" id="eq:rl2">\[\begin{equation}
Y=a+bX+\varepsilon\tag{6.16}
\end{equation}\]</span>
but the heteroscedasticity indicates that the variance of <span class="math inline">\(\varepsilon\)</span> is not constant, and this prevents any fitting of a linear regression. A variable transformation in this case consists in replacing <span class="math inline">\(Y\)</span> by <span class="math inline">\(Y&#39;=Y/X\)</span> and <span class="math inline">\(X\)</span> by <span class="math inline">\(X&#39;=1/X\)</span>. By dividing each member of <a href="6-fit.html#eq:rl2">(6.16)</a> by <span class="math inline">\(X\)</span>, the post-variable transformation model becomes:
<span class="math display" id="eq:rl3">\[\begin{equation}
Y&#39;=aX&#39;+b+\varepsilon&#39;\tag{6.17}
\end{equation}\]</span>
where <span class="math inline">\(\varepsilon&#39;=\varepsilon/X\)</span>. The transformed model still corresponds to a linear relation, except that the y-intercept <span class="math inline">\(a\)</span> of the relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> has become the slope of the relation between <span class="math inline">\(X&#39;\)</span> and <span class="math inline">\(Y&#39;\)</span>, and reciprocally, the slope <span class="math inline">\(b\)</span> of the relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> has become the y-intercept of the relation between <span class="math inline">\(X&#39;\)</span> and <span class="math inline">\(Y&#39;\)</span>. Model <a href="6-fit.html#eq:rl3">(6.17)</a> may be fitted by simple linear regression if the variance of <span class="math inline">\(\varepsilon&#39;\)</span> is constant. As <span class="math inline">\(\mathrm{Var}(\varepsilon&#39;)=\sigma^2\)</span> means <span class="math inline">\(\mathrm{Var}(\varepsilon)=\sigma^2X^2\)</span>, the variable transformation is appropriate if the standard deviation of <span class="math inline">\(\varepsilon\)</span> is proportional to <span class="math inline">\(X\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:linh"></span>
<img src="source/figures/transfo.png" alt="Linear relation between an effect variable (\(X\)) and a response variable (\(Y\)), with an increase in the variability of \(Y\) with an increase in \(X\) (heteroscedasticity)." width="40%" />
<p class="caption">
Figure 6.12: Linear relation between an effect variable (<span class="math inline">\(X\)</span>) and a response variable (<span class="math inline">\(Y\)</span>), with an increase in the variability of <span class="math inline">\(Y\)</span> with an increase in <span class="math inline">\(X\)</span> (heteroscedasticity).
</p>
</div>
<p>As model <a href="6-fit.html#eq:rl3">(6.17)</a> was fitted by simple linear regression, its sum of squares corresponds to:
<span class="math display">\[
\mathrm{SSE}(a,\ b)=\sum_{i=1}^n(Y&#39;_i-aX&#39;_i-b)^2
=\sum_{i=1}^n(Y_i/X_i-a/X_i-b)^2=\sum_{i=1}^nX_i^{-2}(Y_i-a-bX_i)^2
\]</span>
Here we recognize the expression for the sum of squares for a weighted regression using the weight <span class="math inline">\(w_i=X_i^{-2}\)</span>. Thus, the variable transformation <span class="math inline">\(Y&#39;=Y/X\)</span> and <span class="math inline">\(X&#39;=1/X\)</span> is strictly identical to a weighted regression of weight <span class="math inline">\(w=1/X^2\)</span>.</p>
<div class="filrouge">
<ol start="15" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fH" class="exercise"><strong>Red line 6.9  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Linear regression between <span class="math inline">\(B/D^2\)</span> and <span class="math inline">\(H\)</span></strong></span></p>
</div>
<p>We saw in red line <a href="6-fit.html#exr:frlpD2H">6.5</a> that a double-entry biomass model using dbh and height corresponds to: <span class="math inline">\(B=a+bD^2H+\varepsilon\)</span> where <span class="math inline">\(\mathrm{Var}(\varepsilon)\propto D^4\)</span>. By dividing each member of the equation by <span class="math inline">\(D^2\)</span>, we obtain:
<span class="math display">\[
B/D^2=a/D^2+bH+\varepsilon&#39;
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon&#39;)=\sigma^2
\]</span>
Thus, the regression of the response variable <span class="math inline">\(Y=B/D^2\)</span> against the two effect variables <span class="math inline">\(X_1=1/D^2\)</span> and <span class="math inline">\(X_2=H\)</span> in principle satisfies the hypotheses of multiple linear regression. This regression is fitted by the ordinary least squares method. Fitting of this multiple regression by the command:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb53-1"><a href="6-fit.html#cb53-1"></a><span class="kw">summary</span>(<span class="kw">lm</span>((Btot <span class="op">/</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>) <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>heig, <span class="dt">data =</span> dat))</span></code></pre></div>
<p>yields:</p>
<pre class="Rout"><code>##             Estimate Std. Error t value Pr(&gt;|t|)    
## I(1/dbh^2) 1.181e-03  2.288e-03   0.516    0.608    
## heig       2.742e-05  1.527e-06  17.957   &lt;2e-16 ***</code></pre>
<p>where it can be seen that the coefficient associated with <span class="math inline">\(X_1=1/D^2\)</span> is not significantly different from zero. If we now return to the starting data, this means simply that the y-intercept <span class="math inline">\(a\)</span> is not significantly different from zero, something we had already noted in red line <a href="6-fit.html#exr:frlpD2H">6.5</a>. Therefore, <span class="math inline">\(X_1\)</span> may be withdrawn and a simple linear regression may be fitted of <span class="math inline">\(Y=B/D^2\)</span> against <span class="math inline">\(X_2=H\)</span>:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb55-1"><a href="6-fit.html#cb55-1"></a><span class="kw">with</span>(dat, <span class="kw">plot</span>(</span>
<span id="cb55-2"><a href="6-fit.html#cb55-2"></a>  <span class="dt">x =</span> heig, </span>
<span id="cb55-3"><a href="6-fit.html#cb55-3"></a>  <span class="dt">y =</span> Btot <span class="op">/</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>, </span>
<span id="cb55-4"><a href="6-fit.html#cb55-4"></a>  <span class="dt">xlab =</span> <span class="st">&quot;Height (m)&quot;</span>, </span>
<span id="cb55-5"><a href="6-fit.html#cb55-5"></a>  <span class="dt">ylab =</span> <span class="st">&quot;Biomass/square of dbh (t/cm2)&quot;</span></span>
<span id="cb55-6"><a href="6-fit.html#cb55-6"></a>  ))</span>
<span id="cb55-7"><a href="6-fit.html#cb55-7"></a></span>
<span id="cb55-8"><a href="6-fit.html#cb55-8"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>((Btot <span class="op">/</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>) <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span>heig, <span class="dt">data =</span> dat)</span>
<span id="cb55-9"><a href="6-fit.html#cb55-9"></a><span class="kw">summary</span>(m)</span>
<span id="cb55-10"><a href="6-fit.html#cb55-10"></a></span>
<span id="cb55-11"><a href="6-fit.html#cb55-11"></a><span class="kw">plot</span>(m, <span class="dt">which =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</span></code></pre></div>
<p>The scatter plot of <span class="math inline">\(B/D^2\)</span> against <span class="math inline">\(H\)</span> is indeed a straight line with a variance of <span class="math inline">\(B/D^2\)</span> that is approximately constant ( Figure <a href="6-fit.html#fig:fBD2vH">6.13</a>). Fitting the simple linear regression yields:</p>
<pre class="Rout"><code>##       Estimate Std. Error t value Pr(&gt;|t|)    
## heig 2.747e-05  1.511e-06   18.19   &lt;2e-16 ***</code></pre>
<p>with <span class="math inline">\(R^2=\)</span> and a residual standard deviation of  tonnes cm<sup>-2</sup>. The model is written: <span class="math inline">\(B/D^2=2.747\times10^{-5}H\)</span>, or if we return to the starting variables: <span class="math inline">\(B=2747\times10^{-5}D^2H\)</span>. We now need to check that this model is strictly identical to the weighted regression of <span class="math inline">\(B\)</span> against <span class="math inline">\(D^2H\)</span> shown in red line <a href="6-fit.html#exr:frlpD2H">6.5</a> with weighting proportional to <span class="math inline">\(1/D^4\)</span>. The plot of the residuals against the fitted values and the quantile-quantile plot of the residuals are shown in Figure <a href="6-fit.html#fig:fHres">6.14</a>.</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fBD2vH"></span>
<img src="source/figures/fBD2vH.png" alt="Scatter plot of biomass divided by the square of the dbh (tonnes cm-2) against height (m) for the 42 trees measured in Ghana by Henry et al. (2010)." width="40%" />
<p class="caption">
Figure 6.13: Scatter plot of biomass divided by the square of the dbh (tonnes cm<sup>-2</sup>) against height (m) for the 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fHres"></span>
<img src="source/figures/fHres.png" alt="Residuals plotted against fitted values (left) and quantile–quantile plot (right) of the residuals of the simple linear regression of \(B/D^2\) against \(H\) fitted for the 42 trees measured by Henry et al. (2010) in Ghana." width="80%" />
<p class="caption">
Figure 6.14: Residuals plotted against fitted values (left) and quantile–quantile plot (right) of the residuals of the simple linear regression of <span class="math inline">\(B/D^2\)</span> against <span class="math inline">\(H\)</span> fitted for the 42 trees measured by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span> in Ghana.
</p>
</div>
<div class="filrouge">
<ol start="16" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:finvD" class="exercise"><strong>Red line 6.10  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Linear regression between <span class="math inline">\(B/D^2\)</span> and <span class="math inline">\(1/D\)</span></strong></span></p>
</div>
<p>We saw in red line @ref<span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Weighted polynomial regression between <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span></strong></span> that a polynomial biomass model against dbh corresponded to: <span class="math inline">\(B=a_0+a_1D+a_2D^2+\varepsilon\)</span> where <span class="math inline">\(\mathrm{Var}(\varepsilon)\propto D^4\)</span>. By dividing each member of the equation by <span class="math inline">\(D^2\)</span>, we obtain:
<span class="math display">\[
B/D^2=a_0/D^2+a_1/D+a_2+\varepsilon&#39;
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon&#39;)=\sigma^2
\]</span>
Thus, the regression of the response variable <span class="math inline">\(Y=B/D^2\)</span> against the two effect variables <span class="math inline">\(X_1=1/D^2\)</span> and <span class="math inline">\(X_2=1/D\)</span> in principle satisfies the hypotheses of multiple linear regression. This regression is fitted by the ordinary least squares method. Fitting this multiple regression by the command:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb57-1"><a href="6-fit.html#cb57-1"></a><span class="kw">summary</span>(<span class="kw">lm</span>((Btot <span class="op">/</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dbh), <span class="dt">data =</span> dat))</span></code></pre></div>
<p>yields:</p>
<pre class="Rout"><code>##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.215e-03  9.014e-05  13.478 2.93e-16 ***
## I(1/dbh^2)   1.127e-02  6.356e-03   1.772  0.08415 .  
## I(1/dbh)    -7.297e-03  2.140e-03  -3.409  0.00153 **</code></pre>
<p>where it can be seen that the coefficient associated with <span class="math inline">\(X_1=1/D^2\)</span> is not significantly different from zero. If we now return to the starting data, this means simply that the y-intercept <span class="math inline">\(a_0\)</span> is not significantly different from zero, something we had already noted in red line <a href="6-fit.html#exr:fDpara">6.6</a>. Therefore, <span class="math inline">\(X_1\)</span> may be withdrawn and a simple linear regression may be fitted of <span class="math inline">\(Y=B/D^2\)</span> against <span class="math inline">\(X_2=1/D\)</span>:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb59-1"><a href="6-fit.html#cb59-1"></a><span class="kw">with</span>(dat, <span class="kw">plot</span>(</span>
<span id="cb59-2"><a href="6-fit.html#cb59-2"></a>  <span class="dt">x =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dbh, </span>
<span id="cb59-3"><a href="6-fit.html#cb59-3"></a>  <span class="dt">y =</span> Btot <span class="op">/</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>, </span>
<span id="cb59-4"><a href="6-fit.html#cb59-4"></a>  <span class="dt">xlab =</span> <span class="st">&quot;1/Diameter (/cm)&quot;</span>, </span>
<span id="cb59-5"><a href="6-fit.html#cb59-5"></a>  <span class="dt">ylab =</span> <span class="st">&quot;Biomass/square of dbh (t/cm2)&quot;</span></span>
<span id="cb59-6"><a href="6-fit.html#cb59-6"></a>  ))</span>
<span id="cb59-7"><a href="6-fit.html#cb59-7"></a></span>
<span id="cb59-8"><a href="6-fit.html#cb59-8"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>((Btot <span class="op">/</span><span class="st"> </span>dbh<span class="op">^</span><span class="dv">2</span>) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dbh), <span class="dt">data =</span> dat)</span>
<span id="cb59-9"><a href="6-fit.html#cb59-9"></a><span class="kw">summary</span>(m)</span>
<span id="cb59-10"><a href="6-fit.html#cb59-10"></a></span>
<span id="cb59-11"><a href="6-fit.html#cb59-11"></a><span class="kw">plot</span>(m, <span class="dt">which=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</span></code></pre></div>
<p>The scatter plot of <span class="math inline">\(B/D^2\)</span> against <span class="math inline">\(1/D\)</span> is approximately a straight line with a variance of <span class="math inline">\(B/D^2\)</span> that is approximately constant (Figure <a href="6-fit.html#fig:fBD2vD">6.15</a>). Fitting the simple linear regression yields:</p>
<pre class="Rout"><code>##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.124e-03  7.599e-05  14.789  &lt; 2e-16 ***
## I(1/dbh)    -3.840e-03  9.047e-04  -4.245 0.000126 ***</code></pre>
<p>with <span class="math inline">\(R^2=\)</span> and a residual standard deviation of  tonnes cm<sup>-2</sup>. The model is written: <span class="math inline">\(B/D^2=1.124\times10^{-3}-3.84\times10^{-3}D^{-1}\)</span>, or if we return to the starting variables: <span class="math inline">\(B=-3.84\times10^{-3}D+1.124\times10^{-3}D^2\)</span>. We now need to check that this model is strictly identical to the polynomial regression of <span class="math inline">\(B\)</span> against <span class="math inline">\(D\)</span> shown in red line <a href="6-fit.html#exr:fDpara">6.6</a> with weighting proportional to <span class="math inline">\(1/D^4\)</span>. The plot of the residuals against the fitted values and the quantile-quantile plot of the residuals are shown in Figure <a href="6-fit.html#fig:finvDres">6.16</a>.</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fBD2vD"></span>
<img src="source/figures/fBD2vD.png" alt="Scatter plot of biomass divided by the square of the dbh (tonnes in cm-2) against the inverse of the dbh (cm-1) for 42 trees measured in Ghana by Henry et al. (2010)." width="40%" />
<p class="caption">
Figure 6.15: Scatter plot of biomass divided by the square of the dbh (tonnes in cm<sup>-2</sup>) against the inverse of the dbh (cm<sup>-1</sup>) for 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:finvDres"></span>
<img src="source/figures/finvDres.png" alt="Residuals plotted against fitted values (left) and quantile–quantile plot (right) of the residuals of the simple linear regression of \(B/D^2\) against \(1/D\) fitted for the 42 trees measured by Henry et al. (2010) in Ghana." width="80%" />
<p class="caption">
Figure 6.16: Residuals plotted against fitted values (left) and quantile–quantile plot (right) of the residuals of the simple linear regression of <span class="math inline">\(B/D^2\)</span> against <span class="math inline">\(1/D\)</span> fitted for the 42 trees measured by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span> in Ghana.
</p>
</div>

</div>
</div>
</div>
<div id="nlm" class="section level2">
<h2><span class="header-section-number">6.2</span> Fitting a non-linear model</h2>
<p>Let us now address the more general case of fitting a non-linear model. This model is written:
<span class="math display">\[
Y=f(X_1,\ \ldots,\ X_p;\theta)+\varepsilon
\]</span>
where <span class="math inline">\(Y\)</span> is the response variable, <span class="math inline">\(X_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span> are the effect variables, <span class="math inline">\(\theta\)</span> is the vector of all the model coefficients, <span class="math inline">\(\varepsilon\)</span> is the residual error, and <span class="math inline">\(f\)</span> is a function. If <span class="math inline">\(f\)</span> is linear in relation to the coefficients <span class="math inline">\(\theta\)</span>, this brings us back to the previously studied linear model. We will henceforth no longer make any a priori hypotheses concerning the linearity of <span class="math inline">\(f\)</span> in relation to coefficients <span class="math inline">\(\theta\)</span>. As previously, we assume that the residuals are independent and that they follow a centered normal distribution. By contrast, we do not make any a priori hypothesis concerning their variance. <span class="math inline">\(\mathrm{E}(\varepsilon)=0\)</span> means that <span class="math inline">\(\mathrm{E}(Y)=f(X_1,\ \ldots,\ X_p;\theta)\)</span>. This is why we can say that <span class="math inline">\(f\)</span> defines the mean model (i.e. for <span class="math inline">\(Y\)</span>). Let us write:
<span class="math display">\[
\mathrm{Var}(\varepsilon)=g(X_1,\ \ldots,\ X_p;\vartheta)
\]</span>
where <span class="math inline">\(g\)</span> is a function and <span class="math inline">\(\vartheta\)</span> a set of parameters. As <span class="math inline">\(\mathrm{Var}(Y)=\mathrm{Var}(\varepsilon)\)</span>, we can say that <span class="math inline">\(g\)</span> defines the variance model. Function <span class="math inline">\(g\)</span> may take various forms, but for biomass or volume data it is generally a power function of a variable that characterizes tree size (typically dbh). Without loss of generality, we can put forward that this effect variable is <span class="math inline">\(X_1\)</span>, and therefore:
<span class="math display">\[
g(X_1,\ \ldots,\ X_p;\vartheta)\equiv(kX_1^c)^2
\]</span>
where <span class="math inline">\(\vartheta\equiv(k,\ c)\)</span>, <span class="math inline">\(k&gt;0\)</span> and <span class="math inline">\(c\geq0\)</span>.</p>
<p>Interpreting the results of fitting a non-linear model is fundamentally the same as for the linear model. The difference between the linear model and the non-linear model, in addition to their properties, lies in the manner by which model coefficients are estimated. Two particular approaches are used: (<em>i</em>) exponent <span class="math inline">\(c\)</span> is fixed in advance; (<em>ii</em>) exponent <span class="math inline">\(c\)</span> is a parameter to be estimated in the same manner as the model’s other parameters.</p>
<div id="exponent-known" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Exponent known</h3>
<p>Let us first consider the case where the exponent <span class="math inline">\(c\)</span> of the variance model is known in advance. Here, the least squares method can again be used to fit the model. The weighted sum of squares corresponds to:
<span class="math display">\[
\mathrm{SSE}(\theta)=\sum_{i=1}^nw_i\ \varepsilon_i^2
=\sum_{i=1}^nw_i\ [Y_i-f(X_{i1},\ \ldots,\ X_{ip};\theta)]^2
\]</span>
where the weights are inversely proportional to the variance of the residuals:
<span class="math display">\[
w_i=\frac{1}{X_{i1}^{2c}}\propto\frac{1}{\mathrm{Var}(\varepsilon_i)}
\]</span>
As previously, the estimator of the model’s coefficients corresponds to the value of <span class="math inline">\(\theta\)</span> that minimizes the weighted sum of squares:
<span class="math display">\[
\hat{\theta}=\arg\min_{\theta}\mathrm{SSE}(\theta)
=\arg\min_{\theta}\bigg\{\sum_{i=1}^n\frac{1}{X_{i1}^{2c}}
[Y_i-f(X_{i1},\ \ldots,\ X_{ip};\theta)]^2\bigg\}
\]</span>
In the particular case where the residuals have a constant variance (i.e. <span class="math inline">\(c=0\)</span>), the weighted least squares method is simplified to the ordinary least squares method (all weights <span class="math inline">\(w_i\)</span> are 1), but the principle behind the calculations remains the same. The estimator <span class="math inline">\(\theta\)</span> is obtained by resolving
<span class="math display" id="eq:der">\[\begin{equation}
\frac{\partial\mathrm{SSE}}{\partial\theta}(\hat{\theta})=0
\tag{6.18}
\end{equation}\]</span>
with the constraint <span class="math inline">\((\partial^2\mathrm{SSE}/\partial\theta^2)&gt;0\)</span> to ensure that this is indeed a minimum, not a maximum. In the previous case of the linear model, resolving <a href="6-fit.html#eq:der">(6.18)</a> yielded an explicit expression for the estimator <span class="math inline">\(\hat{\theta}\)</span>. This is not the case for the general case of the non-linear model: there is no explicit expression for <span class="math inline">\(\hat{\theta}\)</span>. The sum of squares must therefore be minimized using a numerical algorithm. We will examine this point in depth in section <a href="6-fit.html#algo">6.2.3</a>.</p>
<div id="a-priori-value-for-the-exponent" class="section level4 unnumbered">
<h4><em>A priori</em> value for the exponent</h4>
<p>The <em>a priori</em> value of exponent <span class="math inline">\(c\)</span> is obtained in the non-linear case in the same manner as for the linear case : by trial and error, by dividing <span class="math inline">\(X_1\)</span> into classes and estimating the variance of <span class="math inline">\(Y\)</span> for each class, or by minimizing Furnival’s index .</p>
<div class="filrouge">
<ol start="17" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fnlsD" class="exercise"><strong>Red line 6.11  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Weighted non-linear regression between <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span></strong></span></p>
</div>
<p>The graphical exploration (red lines <a href="5-explo.html#exr:feBvD">5.1</a> and <a href="5-explo.html#exr:feln">5.4</a>) showed that the relation between biomass <span class="math inline">\(B\)</span> and dbh <span class="math inline">\(D\)</span> was of the power type, with the variance of the biomass increasing with dbh:
<span class="math display">\[
B=aD^b+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]</span>
We saw in red line <a href="6-fit.html#exr:frlpD2H">6.5</a> that the conditional standard deviation of the biomass derived from the dbh was proportional to the square of the dbh: <span class="math inline">\(c=2\)</span>. We can therefore fit a non-linear regression by the weighted least squares method using a weighting that is inversely proportional to <span class="math inline">\(D^4\)</span>:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb61-1"><a href="6-fit.html#cb61-1"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)), <span class="dt">data =</span>  dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]))</span>
<span id="cb61-2"><a href="6-fit.html#cb61-2"></a>start[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">exp</span>(start[<span class="dv">1</span>])</span>
<span id="cb61-3"><a href="6-fit.html#cb61-3"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>)</span>
<span id="cb61-4"><a href="6-fit.html#cb61-4"></a>m &lt;-<span class="st"> </span><span class="kw">nls</span>(</span>
<span id="cb61-5"><a href="6-fit.html#cb61-5"></a>  <span class="dt">formula =</span> Btot <span class="op">~</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>dbh<span class="op">^</span>b, </span>
<span id="cb61-6"><a href="6-fit.html#cb61-6"></a>  <span class="dt">data =</span> dat, </span>
<span id="cb61-7"><a href="6-fit.html#cb61-7"></a>  <span class="dt">start =</span> start, </span>
<span id="cb61-8"><a href="6-fit.html#cb61-8"></a>  <span class="dt">weights =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">4</span></span>
<span id="cb61-9"><a href="6-fit.html#cb61-9"></a>  )</span>
<span id="cb61-10"><a href="6-fit.html#cb61-10"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>The non-linear regression is fitted using the <code>nls</code> command which calls on the start values of the coefficients. These start values are contained in the <code>start</code> object and are calculated by re-transforming the coefficients of the linear regression on the log-transformed data. Fitting the non-linear regression by the weighted least squares method gives:</p>
<pre class="Rout"><code>##    Estimate Std. Error t value Pr(&gt;|t|)    
## a 2.492e-04  7.893e-05   3.157  0.00303 ** 
## b 2.346e+00  7.373e-02  31.824  &lt; 2e-16 ***</code></pre>
<p>with a residual standard deviation <span class="math inline">\(k=\)</span> ` tonnes cm<sup>-2</sup>. The model is therefore written: <span class="math inline">\(B=2.492\times10^{-4}D^{2.346}\)</span>. Let us now return to the linear regression fitted to log-transformed data (red line <a href="6-fit.html#exr:rllnBvD">6.1</a>) which was written: <span class="math inline">\(\ln(B)=-8.42722+2.36104\ln(D)\)</span>. If we return naively to the starting data by applying the exponential function (we will see in § <a href="7-util.html#invtra">7.2.4</a> why this is naive), the model becomes: <span class="math inline">\(B=\exp(-8.42722)\times D^{2.36104}=2.188\times10^{-4}D^{2.36104}\)</span>. The model fitted by non-linear regression and the model fitted by linear regression on log-transformed data are therefore very similar.</p>
</div>
<div class="filrouge">
<ol start="18" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fnlsD2H" class="exercise"><strong>Red line 6.12  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Weighted non-linear regression between <span class="math inline">\(B\)</span> and <span class="math inline">\(D^2H\)</span></strong></span></p>
</div>
<p>We have already fitted a power model <span class="math inline">\(B=a(D^2H)^b\)</span> by simple linear regression on log-transformed data (red line <a href="6-fit.html#exr:rllnBvD2H">6.2</a>). Let us now fit this model directly by non-linear regression:
<span class="math display">\[
B=a(D^2H)^b+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]</span>
In order to take account of the heteroscedasticity, and considering that the conditional standard deviation of the biomass derived from the diameter is proportional to <span class="math inline">\(D^2\)</span> (red line <a href="6-fit.html#exr:frlpD2H">6.5</a>), we can fit this non-linear model by the weighted least squares method using a weighting inversely proportional to <span class="math inline">\(D^4\)</span>:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb63-1"><a href="6-fit.html#cb63-1"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(</span>
<span id="cb63-2"><a href="6-fit.html#cb63-2"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig)),</span>
<span id="cb63-3"><a href="6-fit.html#cb63-3"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb63-4"><a href="6-fit.html#cb63-4"></a>  ))</span>
<span id="cb63-5"><a href="6-fit.html#cb63-5"></a>start[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">exp</span>(start[<span class="dv">1</span>])</span>
<span id="cb63-6"><a href="6-fit.html#cb63-6"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>)</span>
<span id="cb63-7"><a href="6-fit.html#cb63-7"></a>m &lt;-<span class="st"> </span><span class="kw">nls</span>(</span>
<span id="cb63-8"><a href="6-fit.html#cb63-8"></a>  <span class="dt">formula =</span> Btot <span class="op">~</span><span class="st"> </span>a<span class="op">*</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig)<span class="op">^</span>b, </span>
<span id="cb63-9"><a href="6-fit.html#cb63-9"></a>  <span class="dt">data =</span> dat, </span>
<span id="cb63-10"><a href="6-fit.html#cb63-10"></a>  <span class="dt">start =</span> start, </span>
<span id="cb63-11"><a href="6-fit.html#cb63-11"></a>  <span class="dt">weights =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">4</span></span>
<span id="cb63-12"><a href="6-fit.html#cb63-12"></a>  )</span>
<span id="cb63-13"><a href="6-fit.html#cb63-13"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>As previously (red line <a href="6-fit.html#exr:fnlsD">6.11</a>), the <code>nls</code> command calls on start values for the coefficients and these are obtained from the coefficients of the multiple regression on log-transformed data. The result of the fitting is as follows:</p>
<pre class="Rout"><code>##    Estimate Std. Error t value Pr(&gt;|t|)    
## a 7.885e-05  2.862e-05   2.755  0.00879 ** 
## b 9.154e-01  2.957e-02  30.953  &lt; 2e-16 ***</code></pre>
<p>with a residual standard deviation <span class="math inline">\(k=\)</span> tonnes cm<sup>-2</sup>. The model is therefore written: <span class="math inline">\(B=7.885\times10^{-5}(D^2H)^{0.9154}\)</span>. Let us now return to the linear regression fitted to log-transformed data (red line <a href="6-fit.html#exr:rllnBvD2H">6.2</a>), which was written: <span class="math inline">\(\ln(B)=-8.99427+0.87238\ln(D^2H)\)</span>. If we return naively to the starting data by applying the exponential function, this model becomes: <span class="math inline">\(B=\exp(-8.99427)\times D^{0.87238}=1.241\times10^{-4}D^{0.87238}\)</span>. The model fitted by non-linear regression and the model fitted by linear regression on log-transformed data are therefore very similar.</p>
</div>
<div class="filrouge">
<ol start="19" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fnlsDH" class="exercise"><strong>Red line 6.13  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Weighted non-linear regression between <span class="math inline">\(B\)</span>, <span class="math inline">\(D\)</span> and <span class="math inline">\(H\)</span></strong></span></p>
</div>
<p>We have already fitted a power model <span class="math inline">\(B=aD^{b_1}H^{b_2}\)</span> by multiple linear regression on log-transformed data (red line <a href="6-fit.html#exr:flnDlnH">6.4</a>). Let us now fit this model directly by non-linear regression:
<span class="math display">\[
B=aD^{b_1}H^{b_2}+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)\propto D^{2c}
\]</span>
In order to take account of the heteroscedasticity, and considering that the conditional standard deviation of the biomass derived from dbh is proportional to <span class="math inline">\(D^2\)</span> (red line <a href="6-fit.html#exr:frlpD2H">6.5</a>), we can fit this non-linear model by the weighted least squares method using a weighting inversely proportional to <span class="math inline">\(D^4\)</span>:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb65-1"><a href="6-fit.html#cb65-1"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(</span>
<span id="cb65-2"><a href="6-fit.html#cb65-2"></a>  <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(heig)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb65-3"><a href="6-fit.html#cb65-3"></a>  ))</span>
<span id="cb65-4"><a href="6-fit.html#cb65-4"></a>start[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">exp</span>(start[<span class="dv">1</span>])</span>
<span id="cb65-5"><a href="6-fit.html#cb65-5"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b1&quot;</span>, <span class="st">&quot;b2&quot;</span>)</span>
<span id="cb65-6"><a href="6-fit.html#cb65-6"></a>m &lt;-<span class="st"> </span><span class="kw">nls</span>(</span>
<span id="cb65-7"><a href="6-fit.html#cb65-7"></a>  <span class="dt">formula =</span> Btot <span class="op">~</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>dbh<span class="op">^</span>b1 <span class="op">*</span><span class="st"> </span>heig<span class="op">^</span>b2, </span>
<span id="cb65-8"><a href="6-fit.html#cb65-8"></a>  <span class="dt">data =</span> dat, </span>
<span id="cb65-9"><a href="6-fit.html#cb65-9"></a>  <span class="dt">start =</span> start, </span>
<span id="cb65-10"><a href="6-fit.html#cb65-10"></a>  <span class="dt">weights =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">4</span></span>
<span id="cb65-11"><a href="6-fit.html#cb65-11"></a>  )</span>
<span id="cb65-12"><a href="6-fit.html#cb65-12"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>As previously (red line <a href="6-fit.html#exr:fnlsD">6.11</a>), the <code>nls</code> command calls on start values for the coefficients and these are obtained from the coefficients of the multiple regression on log-transformed data. The result of the fitting is as follows:</p>
<pre class="Rout"><code>##     Estimate Std. Error t value Pr(&gt;|t|)    
## a  1.003e-04  5.496e-05   1.824   0.0758 .  
## b1 1.923e+00  1.956e-01   9.833 4.12e-12 ***
## b2 7.435e-01  3.298e-01   2.254   0.0299 *</code></pre>
<p>with a residual standard deviation <span class="math inline">\(k=\)</span> tonnes cm<sup>-2</sup>. The model is therefore written: <span class="math inline">\(B=1.003\times10^{-4}D^{1.923}H^{0.7435}\)</span>. The model is similar to that fitted by multiple regression on log-transformed data (red line <a href="6-fit.html#exr:flnDlnH">6.4</a>). But coefficient <span class="math inline">\(a\)</span> is estimated with less precision here than by the multiple regression on log-transformed data.</p>
</div>
</div>
</div>
<div id="estimating-the-exponent" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Estimating the exponent</h3>
<p>Let us now consider the case where the exponent <span class="math inline">\(c\)</span> needs to be estimated at the same time as the model’s other parameters. This case includes the linear regression with variance model that we mentioned in section <a href="6-fit.html#lme">6.1.4</a>. In this case the least squares method is no longer valid. We are therefore obliged to use another fitting method: the maximum likelihood method. The likelihood of an observation (<span class="math inline">\(X_{i1}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_{ip}\)</span>, <span class="math inline">\(Y_i\)</span>) is the probability density of observing (<span class="math inline">\(X_{i1}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_{ip}\)</span>, <span class="math inline">\(Y_i\)</span>) in the specified model. The probability density of a normal distribution of expectation <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> is:
<span class="math display">\[
\phi(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\bigg[-\frac{1}{2}\bigg(
\frac{x-\mu}{\sigma}\bigg)^2\bigg]
\]</span>
As <span class="math inline">\(Y_i\)</span> follows a normal distribution of expectation <span class="math inline">\(f(X_{i1},\ \ldots,\ X_{ip};\theta)\)</span> and standard deviation <span class="math inline">\(kX_{i1}^c\)</span>, the likelihood of the <span class="math inline">\(i\)</span>th observation is:
<span class="math display">\[
\frac{1}{kX_{i1}^c\sqrt{2\pi}}\exp\bigg[-\frac{1}{2}\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\bigg]
\]</span>
As the observations are independent, their joint likelihood is the product of the likelihoods of each observation. The likelihood of the sample of <span class="math inline">\(n\)</span> observations is therefore:
<span class="math display" id="eq:vrais">\[\begin{eqnarray}
\ell(\theta,\ k,\ c) &amp;=&amp; \prod_{i=1}^n
\frac{1}{kX_{i1}^c\sqrt{2\pi}}\exp\bigg[-\frac{1}{2}\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\bigg]
\tag{6.19}
\\ &amp;=&amp; \frac{1}{(k\sqrt{2\pi})^n}\frac{1}{(\prod_{i=1}^nX_{i1})^c}
\exp\bigg[-\frac{1}{2}\sum_{i=1}^n\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\bigg]
\nonumber
\end{eqnarray}\]</span>
This likelihood is considered to be a function of parameters <span class="math inline">\(\theta\)</span>, <span class="math inline">\(k\)</span> and <span class="math inline">\(c\)</span>.</p>
<p>The better the values of parameters <span class="math inline">\(\theta\)</span>, <span class="math inline">\(k\)</span> and <span class="math inline">\(c\)</span> the higher the probability of the observations being obtained in the model corresponding to these parameter values. In other words, the best values for parameters <span class="math inline">\(\theta\)</span>, <span class="math inline">\(k\)</span> and <span class="math inline">\(c\)</span> are those that maximize the likelihood of the observations. The corresponding estimator is by definition the maximum likelihood estimator, and is written:
<span class="math display">\[
(\hat{\theta},\ \hat{k},\ \hat{c})=\arg\max_{(\theta,\ k,\ c)}\;
\ell(\theta,\ k,\ c)=\arg\max_{(\theta,\ k,\ c)}\;
\ln[\ell(\theta,\ k,\ c)]
\]</span>
where the last equality stems from the fact that a function and its logarithm reach their maximum for the same values of their argument. The logarithm of the likelihood, which we call the log-likelihood and which we write <span class="math inline">\(\mathcal{L}\)</span>, is easier to calculate than the likelihood, and therefore, for our calculations, it is the log-likelihood we will be seeking to maximize. In the present case, the log-likelihood is written:
<span class="math display" id="eq:ll">\[\begin{eqnarray}
\mathcal{L}(\theta,\ k,\ c) &amp;=&amp;
\ln[\ell(\theta,\ k,\ c)]
\tag{6.20}
\\ &amp;=&amp; -n\ln(k\sqrt{2\pi})-c\sum_{i=1}^n\ln(X_{i1})-\frac{1}{2}\sum_{i=1}^n\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2\nonumber
\\ &amp;=&amp; -\frac{1}{2}\sum_{i=1}^n\bigg[\bigg(
\frac{Y_i-f(X_1,\ \ldots,\ X_p;\theta)}{kX_{i1}^c}\bigg)^2
+\ln(2\pi)+\ln(k^2X_i^{2c})\bigg]\nonumber
\end{eqnarray}\]</span>
To obtain the maximum likelihood estimators of the parameters, we would need to calculate the partial derivatives of the log-likelihood with respect to these parameters, and look for the values where they cancel each other out (while ensuring that the second derivatives are indeed negative). In the general case, there is no analytical solution to this problem. In the same manner as previously for the sum of squares, we will need to use a numerical algorithm to maximize the log-likelihood. We can show that the maximum likelihood method yields a coefficients estimator that is asymptotically (i.e. when the number <span class="math inline">\(n\)</span> of observations tends toward infinity) the best. We can also show that in the case of the linear model, the least squares estimator and the maximum likelihood estimator are the same.</p>
<div class="filrouge">
<ol start="20" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fnlmD" class="exercise"><strong>Red line 6.14  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Non-linear regression between <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span> with variance model</strong></span></p>
</div>
<p>Let us look again at the non-linear regression between biomass and dbh (see red line <a href="6-fit.html#exr:fnlsD">6.11</a>), but this time consider that exponent <span class="math inline">\(c\)</span> in the variance model is a parameter that needs to be estimated like the others. The model is written in the same fashion as before (red line <a href="6-fit.html#exr:fnlsD">6.11</a>):
<span class="math display">\[
B=aD^b+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]</span>
but is fitted by the maximum likelihood method:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb67-1"><a href="6-fit.html#cb67-1"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]))</span>
<span id="cb67-2"><a href="6-fit.html#cb67-2"></a>start[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">exp</span>(start[<span class="dv">1</span>])</span>
<span id="cb67-3"><a href="6-fit.html#cb67-3"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>)</span>
<span id="cb67-4"><a href="6-fit.html#cb67-4"></a><span class="kw">library</span>(nlme)</span>
<span id="cb67-5"><a href="6-fit.html#cb67-5"></a>m &lt;-<span class="st"> </span><span class="kw">nlme</span>(</span>
<span id="cb67-6"><a href="6-fit.html#cb67-6"></a>  <span class="dt">model =</span> Btot <span class="op">~</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>dbh<span class="op">^</span>b, </span>
<span id="cb67-7"><a href="6-fit.html#cb67-7"></a>  <span class="dt">data =</span> <span class="kw">cbind</span>(dat, <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>), </span>
<span id="cb67-8"><a href="6-fit.html#cb67-8"></a>  <span class="dt">fixed =</span> a <span class="op">+</span><span class="st"> </span>b <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb67-9"><a href="6-fit.html#cb67-9"></a>  <span class="dt">start =</span> start, </span>
<span id="cb67-10"><a href="6-fit.html#cb67-10"></a>  <span class="dt">groups =</span> <span class="op">~</span>g, </span>
<span id="cb67-11"><a href="6-fit.html#cb67-11"></a>  <span class="dt">weights =</span> <span class="kw">varPower</span>(<span class="dt">form =</span> <span class="op">~</span>dbh)</span>
<span id="cb67-12"><a href="6-fit.html#cb67-12"></a>  )</span>
<span id="cb67-13"><a href="6-fit.html#cb67-13"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>The model is fitted by the <code>nlme</code> command<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> which, like the <code>nls</code> command (red line <a href="6-fit.html#exr:fnlsD">6.11</a>), requires <code>start</code> values for the coefficients. These start values are calculated as in red line <a href="6-fit.html#exr:fnlsD">6.11</a>. The result of the fitting is as follows:</p>
<pre class="Rout"><code>##          Value    Std.Error DF   t-value      p-value
## a 0.0002444623 7.136164e-05 40  3.425683 1.431028e-03
## b 2.3510499658 6.947401e-02 40 33.840713 4.684925e-31</code></pre>
<p>with an estimated value of exponent <span class="math inline">\(c=2.090814\)</span>. This estimated value is very similar to that evaluated for the weighted non-linear regression (<span class="math inline">\(c=2\)</span>, see in red line <a href="6-fit.html#exr:frlpD2H">6.5</a>). The fitted model is therefore: <span class="math inline">\(B=2.445\times10^{-4}D^{2.35105}\)</span>, which is very similar to the model fitted by weighted non-linear regression (red line <a href="6-fit.html#exr:fnlsD">6.11</a>).</p>
</div>
<div class="filrouge">
<ol start="21" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fnlmD2H" class="exercise"><strong>Red line 6.15  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Non-linear regression between <span class="math inline">\(B\)</span> and <span class="math inline">\(D^2H\)</span> with variance model</strong></span></p>
</div>
<p>Let us look again at the non-linear regression between biomass and <span class="math inline">\(D^2H\)</span> (see red line <a href="6-fit.html#exr:fnlsD2H">6.12</a>), but this time consider that exponent <span class="math inline">\(c\)</span> in the variance model is a parameter that needs to be estimated like the others. The model is written in the same fashion as before (red line <a href="6-fit.html#exr:fnlsD2H">6.12</a>):
<span class="math display">\[
B=a(D^2H)^b+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]</span>
but is fitted by the maximum likelihood method:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb69-1"><a href="6-fit.html#cb69-1"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(</span>
<span id="cb69-2"><a href="6-fit.html#cb69-2"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig)), </span>
<span id="cb69-3"><a href="6-fit.html#cb69-3"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb69-4"><a href="6-fit.html#cb69-4"></a>  ))</span>
<span id="cb69-5"><a href="6-fit.html#cb69-5"></a>start[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">exp</span>(start[<span class="dv">1</span>])</span>
<span id="cb69-6"><a href="6-fit.html#cb69-6"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>)</span>
<span id="cb69-7"><a href="6-fit.html#cb69-7"></a></span>
<span id="cb69-8"><a href="6-fit.html#cb69-8"></a><span class="kw">library</span>(nlme)</span>
<span id="cb69-9"><a href="6-fit.html#cb69-9"></a>m &lt;-<span class="st"> </span><span class="kw">nlme</span>(</span>
<span id="cb69-10"><a href="6-fit.html#cb69-10"></a>  <span class="dt">model   =</span> Btot <span class="op">~</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig)<span class="op">^</span>b, </span>
<span id="cb69-11"><a href="6-fit.html#cb69-11"></a>  <span class="dt">data    =</span>  <span class="kw">cbind</span>(dat, <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>),</span>
<span id="cb69-12"><a href="6-fit.html#cb69-12"></a>  <span class="dt">fixed   =</span> a <span class="op">+</span><span class="st"> </span>b <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, </span>
<span id="cb69-13"><a href="6-fit.html#cb69-13"></a>  <span class="dt">start   =</span> start, </span>
<span id="cb69-14"><a href="6-fit.html#cb69-14"></a>  <span class="dt">groups  =</span> <span class="op">~</span>g, </span>
<span id="cb69-15"><a href="6-fit.html#cb69-15"></a>  <span class="dt">weights =</span> <span class="kw">varPower</span>(<span class="dt">form =</span> <span class="op">~</span>dbh)</span>
<span id="cb69-16"><a href="6-fit.html#cb69-16"></a>  )</span>
<span id="cb69-17"><a href="6-fit.html#cb69-17"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>The model is fitted by the <code>nlme</code> command, which, like the <code>nls</code> command (red line <a href="6-fit.html#exr:fnlsD">6.11</a>), requires start values for the coefficients. These <code>start</code> values are calculated as in red line <a href="6-fit.html#exr:fnlsD">6.11</a>. The result of the fitting is as follows:</p>
<pre class="Rout"><code>##          Value    Std.Error DF   t-value      p-value
## a 0.0000819357 2.852774e-05 40  2.872142 6.492376e-03
## b 0.9122144285 2.862782e-02 40 31.864613 4.779915e-30</code></pre>
<p>with an estimated value of exponent <span class="math inline">\(c=2.042586\)</span>. This estimated value is very similar to that evaluated for the weighted non-linear regression (<span class="math inline">\(c=2\)</span>, see red line <a href="6-fit.html#exr:frlpD2H">6.5</a>). The fitted model is therefore: <span class="math inline">\(B=8.19\times10^{-5}(D^2H)^{0.9122144}\)</span>, which is very similar to the model fitted by weighted non-linear regression (red line <a href="6-fit.html#exr:fnlsD2H">6.12</a>).</p>
</div>
<div class="filrouge">
<ol start="22" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fnlmDH" class="exercise"><strong>Red line 6.16  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Non-linear regression between <span class="math inline">\(B\)</span>, <span class="math inline">\(D\)</span> and <span class="math inline">\(H\)</span> with variance model</strong></span></p>
</div>
<p>Let us look again at the non-linear regression between biomass, dbh and height (see red line <a href="6-fit.html#exr:fnlsDH">6.13</a>):
<span class="math display">\[
B=aD^{b_1}H^{b_2}+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]</span>
but this time consider that exponent <span class="math inline">\(c\)</span> in the variance model is a parameter that needs to be estimated like the others. The fitting by maximum likelihood is as follows:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb71-1"><a href="6-fit.html#cb71-1"></a><span class="kw">library</span>(nlme)</span>
<span id="cb71-2"><a href="6-fit.html#cb71-2"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(</span>
<span id="cb71-3"><a href="6-fit.html#cb71-3"></a>  <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(heig)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb71-4"><a href="6-fit.html#cb71-4"></a>  ))</span>
<span id="cb71-5"><a href="6-fit.html#cb71-5"></a>start[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">exp</span>(start[<span class="dv">1</span>])</span>
<span id="cb71-6"><a href="6-fit.html#cb71-6"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b1&quot;</span>, <span class="st">&quot;b2&quot;</span>)</span>
<span id="cb71-7"><a href="6-fit.html#cb71-7"></a>m &lt;-<span class="st"> </span><span class="kw">nlme</span>(</span>
<span id="cb71-8"><a href="6-fit.html#cb71-8"></a>  <span class="dt">model   =</span> Btot <span class="op">~</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>dbh<span class="op">^</span>b1 <span class="op">*</span><span class="st"> </span>heig<span class="op">^</span>b2, </span>
<span id="cb71-9"><a href="6-fit.html#cb71-9"></a>  <span class="dt">data    =</span> <span class="kw">cbind</span>(dat, <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>), </span>
<span id="cb71-10"><a href="6-fit.html#cb71-10"></a>  <span class="dt">fixed   =</span> a <span class="op">+</span><span class="st"> </span>b1 <span class="op">+</span><span class="st"> </span>b2 <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb71-11"><a href="6-fit.html#cb71-11"></a>  <span class="dt">start   =</span> start, </span>
<span id="cb71-12"><a href="6-fit.html#cb71-12"></a>  <span class="dt">groups  =</span> <span class="op">~</span>g, </span>
<span id="cb71-13"><a href="6-fit.html#cb71-13"></a>  <span class="dt">weights =</span> <span class="kw">varPower</span>(<span class="dt">form =</span> <span class="op">~</span>dbh)</span>
<span id="cb71-14"><a href="6-fit.html#cb71-14"></a>  )</span>
<span id="cb71-15"><a href="6-fit.html#cb71-15"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>and, as previously, requires the provision of <code>start</code> values for the coefficients. The fitting yields:</p>
<pre class="Rout"><code>##          Value    Std.Error DF  t-value      p-value
## a  0.000110913 5.659208e-05 39 1.959868 5.718487e-02
## b1 1.943487599 1.947994e-01 39 9.976866 2.732305e-12
## b2 0.692625581 3.211766e-01 39 2.156526 3.726785e-02</code></pre>
<p>with an estimated value of exponent <span class="math inline">\(c=\)</span> 2.0555534. This estimated value is very similar to that evaluated for the weighted non-linear regression (<span class="math inline">\(c=2\)</span>, see red line <a href="6-fit.html#exr:frlpD2H">6.5</a>). The fitted model is therefore: <span class="math inline">\(B=1.109\times10^{-4}D^{1.9434876}H^{0.6926256}\)</span>, which is very similar to the model fitted by weighted non-linear regression (red line <a href="6-fit.html#exr:fnlsDH">6.13</a>).</p>
</div>
<div class="filrouge">
<ol start="23" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fexpDpol" class="exercise"><strong>Red line 6.17  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Non-linear regression between <span class="math inline">\(B\)</span> and a polynomial of <span class="math inline">\(\ln(D)\)</span></strong></span></p>
</div>
<p>Previously (red line <a href="6-fit.html#exr:flnDpol">6.3</a>), we used multiple regression to fit a model between <span class="math inline">\(\ln(B)\)</span> and a polynomial of <span class="math inline">\(\ln(D)\)</span>. If we look again at the start variable, the model is written:
<span class="math display">\[
B=\exp\{a_0+a_1\ln(D)+a_2[\ln(D)]^2+\ldots+a_p[\ln(D)]^p\}+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=(kD^c)^2
\]</span>
We will now directly fit this non-linear model by maximum likelihood (such that exponent <span class="math inline">\(c\)</span> is estimated at the same time as the model’s other parameters). For a third-order polynomial, the fitting is obtained by:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb73-1"><a href="6-fit.html#cb73-1"></a><span class="kw">library</span>(nlme)</span>
<span id="cb73-2"><a href="6-fit.html#cb73-2"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(</span>
<span id="cb73-3"><a href="6-fit.html#cb73-3"></a>  <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">3</span>), </span>
<span id="cb73-4"><a href="6-fit.html#cb73-4"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb73-5"><a href="6-fit.html#cb73-5"></a>  ))</span>
<span id="cb73-6"><a href="6-fit.html#cb73-6"></a>start[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">exp</span>(start[<span class="dv">1</span>])</span>
<span id="cb73-7"><a href="6-fit.html#cb73-7"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;a&quot;</span>, <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb73-8"><a href="6-fit.html#cb73-8"></a>m &lt;-<span class="st"> </span><span class="kw">nlme</span>(</span>
<span id="cb73-9"><a href="6-fit.html#cb73-9"></a>  <span class="dt">model =</span> Btot <span class="op">~</span><span class="st"> </span><span class="kw">exp</span>(a0 <span class="op">+</span><span class="st"> </span>a1 <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(dbh) <span class="op">+</span><span class="st"> </span>a2 <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>a3 <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">3</span>),</span>
<span id="cb73-10"><a href="6-fit.html#cb73-10"></a>  <span class="dt">data =</span> <span class="kw">cbind</span>(dat, <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>), </span>
<span id="cb73-11"><a href="6-fit.html#cb73-11"></a>  <span class="dt">fixed =</span> a0 <span class="op">+</span><span class="st"> </span>a1 <span class="op">+</span><span class="st"> </span>a2 <span class="op">+</span><span class="st"> </span>a3 <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb73-12"><a href="6-fit.html#cb73-12"></a>  <span class="dt">start =</span> start, </span>
<span id="cb73-13"><a href="6-fit.html#cb73-13"></a>  <span class="dt">groups =</span> <span class="op">~</span>g, </span>
<span id="cb73-14"><a href="6-fit.html#cb73-14"></a>  <span class="dt">weights =</span> <span class="kw">varPower</span>(<span class="dt">form =</span> <span class="op">~</span>dbh)</span>
<span id="cb73-15"><a href="6-fit.html#cb73-15"></a>  )</span>
<span id="cb73-16"><a href="6-fit.html#cb73-16"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>and results in:</p>
<pre class="Rout"><code>##          Value  Std.Error DF    t-value      p-value
## a0 -8.99041010 2.29609083 38 -3.9155289 0.0003625208
## a1  2.94478161 2.11009164 38  1.3955705 0.1709459839
## a2 -0.16016443 0.61793888 38 -0.2591914 0.7968865838
## a3  0.01359927 0.05818961 38  0.2337061 0.8164676343</code></pre>
<p>with an estimated value of exponent <span class="math inline">\(c=\)</span> 2.0999225. This result is very similar to that obtained by multiple regression on log-transformed data (red line <a href="6-fit.html#exr:flnDpol">6.3</a>).</p>
</div>
</div>
<div id="algo" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Numerical optimization</h3>
<p>If, in the case of a non-linear model, we are looking to minimize the sum of squares (when exponent <span class="math inline">\(c\)</span> is known) or maximize the log-likelihood (when exponent <span class="math inline">\(c\)</span> needs to be estimated) we need to use a numerical optimization algorithm. As maximizing the log-likelihood is equivalent to minimizing the opposite of the log-likelihood, we shall, in all that follows, consider only the problem of minimizing a function in a multidimensional space. A multitude of optimization algorithms have been developed <span class="citation">(Press et al. <a href="bibliography.html#ref-press07" role="doc-biblioref">2007</a>, chapitre 10)</span> but the aim here is not to present a review of them all. What simply needs to be recognized at this stage is that these algorithms are iterative and require parameter start values. Based on this start value and each iteration, the algorithm shifts in the parameter space while looking to minimize the target function (i.e. the sum of squares or minus the log-likelihood). The target function may be represented as a hypersurface in the parameter space (Figure <a href="6-fit.html#fig:sp">6.17</a>). Each position in this space corresponds to a value of the parameters. A lump in this surface corresponds to a local maximum of the target function, whereas a dip in the surface corresponds to a local minimum. The aim is to determine the overall minimum, i.e. the deepest dip. The position of this dip corresponds to the estimated value of the parameters. If the algorithm gives the position of a dip that is not the deepest, the estimation of the parameters is false.</p>
<div class="figure" style="text-align: center"><span id="fig:sp"></span>
<img src="source/figures/numopt1.png" alt="Representation of the target function (i.e. the quantity to be minimized) as a surface in parameter space. Each position in this space corresponds to a value of the parameters. The successive values \(\theta_1\), \(\theta_2\), , for the parameters are obtained from a start value \(\theta_0\) by descending the surface along the line with the steepest slope.(A) The surface has a single watershed. (B) The surface has several watersheds." width="60%" height="50%"  /><img src="source/figures/numopt2.png" alt="Representation of the target function (i.e. the quantity to be minimized) as a surface in parameter space. Each position in this space corresponds to a value of the parameters. The successive values \(\theta_1\), \(\theta_2\), , for the parameters are obtained from a start value \(\theta_0\) by descending the surface along the line with the steepest slope.(A) The surface has a single watershed. (B) The surface has several watersheds." width="60%" height="50%"  />
<p class="caption">
Figure 6.17: Representation of the target function (<em>i.e.</em> the quantity to be minimized) as a surface in parameter space. Each position in this space corresponds to a value of the parameters. The successive values <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, , for the parameters are obtained from a start value <span class="math inline">\(\theta_0\)</span> by descending the surface along the line with the steepest slope.(A) The surface has a single watershed. (B) The surface has several watersheds.
</p>
</div>
<div id="descending-algorithm" class="section level4 unnumbered">
<h4>Descending algorithm</h4>
<p>The simplest optimization algorithm consists in calculating successive positions, i.e. successive parameter values, by descending the surface defined by the target function along the line that has the steepest slope (Figure <a href="6-fit.html#fig:sp">6.17</a>A). This algorithm leads to one dip in the surface, but nothing says that this dip is the deepest for the surface may have several watersheds with several dips. Depending on the starting position, the algorithm will converge toward one dip or another (Figure <a href="6-fit.html#fig:sp">6.17</a>B). Also, even two starting positions very close together may be located on either side of a crest separating the two watersheds, and will therefore result in different dips, i.e. in different parameter estimations. The only case where this algorithm gives the correct parameter estimation regardless of starting value is when the surface has only a single dip, i.e. when the target function is convex. This in particular is the case for the linear model, but generally not for the non-linear model.</p>
</div>
<div id="improving-algorithms-in-the-case-of-local-minima" class="section level4 unnumbered">
<h4>Improving algorithms in the case of local minima</h4>
<p>More subtle algorithms have been developed than those descending along the steepest slope. For example, these may include the possibility to climb out of a dip into which the algorithm has temporarily converged in order to determine whether there might not be a deeper dip in the neighborhood. But no algorithm, even the most subtle, offers the certainty that it has converged to the deepest dip. Thus, any numerical optimization algorithm (<em>i</em>) may be trapped by a local minimum instead of converging to the overall minimum, and (<em>ii</em>) is sensitive to the indicated start position, which in part determines the final position toward which the algorithm will converge.</p>
<p>If we now return to the issue at hand, this means (<em>i</em>) fitting a non-linear model could yield erroneous parameter estimations, and (<em>ii</em>) selecting parameter start values for the algorithm is a sensitive issue. Herein lies the main drawback of fitting a non-linear model, and if it is to be avoided, the parameter start values must be carefully selected and, above all, several values must be tested.</p>
</div>
<div id="selecting-parameter-start-values" class="section level4 unnumbered">
<h4>Selecting parameter start values</h4>
<p>When mean model <span class="math inline">\(f\)</span> can be transformed into a linear relation between the response variable <span class="math inline">\(Y\)</span> and the effect variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span>, a start value for the coefficients may be obtained by fitting a linear regression on the transformed variables without considering the possible heteroscedasticity of the residuals. Let us for example consider a power type biomass table:
<span class="math display" id="eq:powea">\[\begin{equation}
B=aD^{b_1}H^{b_2}\rho^{b_3}+\varepsilon\tag{6.21}
\end{equation}\]</span>
where
<span class="math display">\[
\varepsilon\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\;kD^c)
\]</span>
The power model for the expectation of <span class="math inline">\(B\)</span> can be rendered linear by log-transforming the variables: <span class="math inline">\(\ln(B)=a&#39;+b_1\ln(D)+b_2\ln(H)+b_3\ln(\rho)\)</span>. But this transformation is incompatible with the additivity of the errors in the model <a href="6-fit.html#eq:powea">(6.21)</a>. In other words, the multiple regression of the response variable <span class="math inline">\(\ln(B)\)</span> against the effect variables <span class="math inline">\(\ln(D)\)</span>, <span class="math inline">\(\ln(H)\)</span> and <span class="math inline">\(\ln(\rho)\)</span>:
<span class="math display" id="eq:poweb">\[\begin{equation}
\ln(B)=a&#39;+b_1\ln(D)+b_2\ln(H)+b_3\ln(\rho)+\varepsilon&#39;\tag{6.22}
\end{equation}\]</span>
where <span class="math inline">\(\varepsilon&#39;\sim\mathcal{N}(0,\ \sigma)\)</span>, is not a model equivalent to <a href="6-fit.html#eq:powea">(6.21)</a>, even when the residuals <span class="math inline">\(\varepsilon&#39;\)</span> of this model have a constant variance. Even though models <a href="6-fit.html#eq:powea">(6.21)</a> and <a href="6-fit.html#eq:poweb">(6.22)</a> are not mathematically equivalent, the coefficients of <a href="6-fit.html#eq:poweb">(6.22)</a> — estimated by multiple regression — may serve as start values for the numerical algorithm that estimates the coefficients of <a href="6-fit.html#eq:powea">(6.21)</a>. If we write as <span class="math inline">\(x^{(0)}\)</span> the start value of parameter <span class="math inline">\(x\)</span> for the numerical optimization algorithm, we thus obtain:
<span class="math display">\[
a^{(0)}=\exp(\hat{a}&#39;),\quad b_i^{(0)}=\hat{b}_i,\quad
k^{(0)}=\hat{\sigma},\quad c^{(0)}=0
\]</span>
Sometimes, the mean model cannot be rendered linear. An example of this is the following biomass table used for trees in a plantation <span class="citation">(Saint-André et al. <a href="bibliography.html#ref-saintandre05" role="doc-biblioref">2005</a>)</span>:
<span class="math display">\[
B=a+[b_0+b_1T+b_2\exp(-b_3T)]D^2H+\varepsilon
\]</span>
where <span class="math inline">\(T\)</span> is the age of the plantation and <span class="math inline">\(\varepsilon\sim\mathcal{N}(0,\ kD^c)\)</span>, which has a mean model that cannot be rendered linear. In this case, parameter start values must be selected empirically. For this current example we could for instance take:
<span class="math display">\[
a^{(0)}=\hat{a},\quad b_0^{(0)}+b_2^{(0)}=\hat{b}_0,\quad
b_1^{(0)}=\hat{b}_1,\quad b_3^{(0)}=0,\quad
k^{(0)}=\hat{\sigma},\quad c^{(0)}=0
\]</span>
where <span class="math inline">\(\hat{a}\)</span>, <span class="math inline">\(\hat{b}_0\)</span>, <span class="math inline">\(\hat{b}_1\)</span> and <span class="math inline">\(\hat{\sigma}\)</span> are estimated values for the coefficients and residual standard deviations of the multiple regression of <span class="math inline">\(B\)</span> against <span class="math inline">\(D^2H\)</span> and <span class="math inline">\(D^2HT\)</span>.
Selecting parameter start values does not mean that several start values should not be tested. Therefore, when fitting a non-linear model with a numerical optimization algorithm, it is essential to test several parameter start values to ensure that the estimations are stable.</p>

</div>
</div>
</div>
<div id="select" class="section level2">
<h2><span class="header-section-number">6.3</span> Selecting variables and models</h2>
<p>When we look to construct a volume or biomass table, the graphical exploration of the data (chapter <a href="5-explo.html#explo">5</a>) generally yields several possible model forms. We could fit all these potentially interesting models, but ultimately, which of all these fitted models should be recommended to the user? Selecting variables and selecting models aims to determine which is the “best” possible expression of the model among all those fitted.</p>
<div id="selecting-variables" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Selecting variables</h3>
<p>Let us take the example of a biomass table we are looking to construct from a dataset that includes tree diameter (dbh) and height, and wood specific density. By working on log-transformed data, and given the variables they include, we may fit the following models:
<span class="math display">\[\begin{eqnarray*}
\ln(B) &amp;=&amp; a_0+a_1\ln(D)+\varepsilon\\ %
\ln(B) &amp;=&amp; a_0+a_2\ln(H)+\varepsilon\\ %
\ln(B) &amp;=&amp; a_0+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &amp;=&amp; a_0+a_1\ln(D)+a_2\ln(H)+\varepsilon\\ %
\ln(B) &amp;=&amp; a_0+a_1\ln(D)+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &amp;=&amp; a_0+a_2\ln(H)+a_3\ln(\rho)+\varepsilon\\ %
\ln(B) &amp;=&amp; a_0+a_1\ln(D)+a_2\ln(H)+a_3\ln(\rho)+\varepsilon
\end{eqnarray*}\]</span>
The <em>complete</em> model (the last in the above list) is that which includes all the effect variables available. All the other models may be considered to be subsets of the complete model, but where certain effect variables have been employed and other set aside. Selecting the variables aims to choose — from among the effect variables of the complete model — those that should be kept and those that may be set aside as they contribute little to the prediction of the response variable. In other words, in this example, selecting the variables would consist in choosing the best model from among the seven models envisaged for <span class="math inline">\(\ln(B)\)</span>.</p>
<p>Given that there are <span class="math inline">\(p\)</span> effect variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span>, there are <span class="math inline">\(2^p-1\)</span> models that include all or some of these effect variables. Selecting the variables consists in choosing the “best” combination of effect variables from all those available. This means firstly that we must have criterion that can be used to evaluate the quality of a model. We have already seen that <span class="math inline">\(R^2\)</span> is a poor criterion for evaluating the quality of one model in comparison with that of another as it increases automatically with the number of effect variables, and this regardless of whether these provide information useful for predicting the response variable or not. A better criterion for selecting effect variables is the residual variance estimator, which is linked to <span class="math inline">\(R^2\)</span> by the relation:
<span class="math display">\[
\hat{\sigma}^2=\frac{n}{n-p-1}(1-R^2)\ S_Y^2
\]</span>
where <span class="math inline">\(S_Y^2\)</span> is the empirical variance of the response variable.</p>
<p>Several methods may be used to select the best combination of effect variables. If <span class="math inline">\(p\)</span> is not too high, we can review all the <span class="math inline">\(2^p-1\)</span> possible models exhaustively. When <span class="math inline">\(p\)</span> is too high, a step by step method may be used to select the variables. Step-by-step methods proceed by the successive elimination or successive addition of effect variables. The descending method consists in eliminating the least significant of the <span class="math inline">\(p\)</span> variables. The regression is then recalculated and the process repeated until a stop criterion is satisfied (e.g. when all model coefficients are significantly different from zero). The ascending method proceeds in the opposite direction: we start with the best single-variable regression and add the variable that increases <span class="math inline">\(R^2\)</span> the most until the stop criterion is satisfied.</p>
<p>The so-called <em>stepwise</em> method is a further improvement upon the previous algorithm that consists in performing an additional Fisher’s significance test at each step such as not to introduce a non-significant variable and possibly eliminate variables that have already been introduced but are no longer informative given the last variable selected. The algorithm stops when no more variables can be added or withdrawn. The different step-by-step selection methods do not necessarily give the same result, but the “stepwise” method would appear to be the best. They do not, however, safeguard from the untimely removal of variables that are actually significant, which may well bias the result. And in connection with this, it should be recalled that if we know (for biological reasons) why a variable should be included in a model (e.g. wood specific density), it is not because a statistical test declares it non-significant that it should be rejected (because of the test’s type II error).</p>
<div class="filrouge">
<ol start="24" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:selvar" class="exercise"><strong>Red line 6.18  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Selecting variables</strong></span></p>
</div>
<p>Let us select the variables <span class="math inline">\(\ln(D)\)</span>, <span class="math inline">\([\ln(D)]^2\)</span>, <span class="math inline">\([\ln(D)]^3\)</span>, <span class="math inline">\(\ln(H)\)</span> to predict the log of the biomass. The complete model is therefore:
<span class="math display">\[
\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(H)
+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]</span>
Variables are selected in R using the <code>step</code> command applied to the fitted complete model:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb75-1"><a href="6-fit.html#cb75-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb75-2"><a href="6-fit.html#cb75-2"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh))<span class="op">+</span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">2</span>)<span class="op">+</span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">3</span>)<span class="op">+</span><span class="kw">I</span>(<span class="kw">log</span>(heig)),</span>
<span id="cb75-3"><a href="6-fit.html#cb75-3"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot<span class="op">&gt;</span><span class="dv">0</span>,]</span>
<span id="cb75-4"><a href="6-fit.html#cb75-4"></a>  )</span>
<span id="cb75-5"><a href="6-fit.html#cb75-5"></a><span class="kw">summary</span>(<span class="kw">step</span>(m))</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -6.50202    0.35999 -18.062  &lt; 2e-16 ***
## I(log(dbh)^2)  0.23756    0.01972  12.044 1.53e-14 ***
## I(log(heig))   1.01874    0.17950   5.675 1.59e-06 ***</code></pre>
<p>The variables selected are therefore <span class="math inline">\([\ln(D)]^2\)</span> and <span class="math inline">\(\ln(H)\)</span>. The model finally retained is therefore:
<span class="math inline">\(\ln(B)=-6.50202+0.23756[\ln(D)]^2+1.01874\ln(H)\)</span>, with a residual standard deviation of and <span class="math inline">\(R^2=\)</span> .</p>
</div>
</div>
<div id="selmod" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Selecting models</h3>
<p>Given two competitor models that predict the same response variable to within one transformation of a variable, which should we choose? Let us look at a few different cases before answering this question.</p>
<div id="nested-models" class="section level4 unnumbered">
<h4>Nested models</h4>
<p>The simplest case is where the two models to be compared are nested. A model is <em>nested</em> in another if the two predict the same response variable and if we can move from the second to the first by removing one or several effect variables. For example, the biomass table <span class="math inline">\(B=a_0+a_1D+\varepsilon\)</span> is nested in <span class="math inline">\(B=a_0+a_1D+a_2D^2H+\varepsilon\)</span> since we can move from the second to the first by deleting <span class="math inline">\(D^2H\)</span> from the effect variables. Likewise, the model <span class="math inline">\(B=a_0+a_2D^2H+\varepsilon\)</span> is nested in <span class="math inline">\(B=a_0+a_1D+a_2D^2H+\varepsilon\)</span> since we can move from the second to the first by deleting <span class="math inline">\(D\)</span> from the effect variables. By contrast, the model <span class="math inline">\(B=a_0+a_1D+\varepsilon\)</span> is not nested in <span class="math inline">\(B=a_0+a_2D^2H+\varepsilon\)</span>. Let <span class="math inline">\(p\)</span> be the number of effect variables in the complete model and <span class="math inline">\(p&#39;&lt;p\)</span> be the number of effect variables in the nested model. Without loss of generality, we can write the complete model as:
<span class="math display" id="eq:embnl">\[\begin{equation}
Y=f(X_1,\ \ldots,\ X_{p&#39;},\ X_{p&#39;+1},\ \ldots,\ X_p;
\theta_0,\ \theta_1)+\varepsilon\tag{6.23}
\end{equation}\]</span>
where (<span class="math inline">\(\theta_0\)</span>, <span class="math inline">\(\theta_1\)</span>) is the vector of the coefficients associated with the complete model and <span class="math inline">\(\theta_0\)</span> is the vector of the coefficients associated with the nested model, which may be obtained by setting
<span class="math inline">\(\theta_1=\mathbf{0}\)</span>. In the particular case of the linear model, the complete model is obtained as the sum of the nested model and additional terms:
<span class="math display" id="eq:emblm">\[\begin{equation}
\underbrace{\underbrace{Y=a_0+a_1X_1+\ldots+a_{p&#39;}X_{p&#39;}}_{\scriptsize \mbox{nested model}}+
a_{p&#39;+1}X_{p&#39;+1}+\ldots+a_pX_p}_{\scriptsize \mbox{complete model}}+\varepsilon\tag{6.24}
\end{equation}\]</span>
where <span class="math inline">\(\theta_0=(a_0,\ \ldots,\ a_{p&#39;})\)</span> and <span class="math inline">\(\theta_1=(a_{p&#39;+1},\ \ldots,\ a_p)\)</span>.</p>
<p>In the case of nested models, a statistical test can be used to test one of the models against the other. The null hypothesis of this test is that <span class="math inline">\(\theta_1=\mathbf{0}\)</span>, i.e. the additional terms are not significant, which can also be expressed as: the nested model is better than the complete model. If the p-value of this test proves to be below the significance level (typically 5 %), then the null hypothesis is rejected, i.e. the complete model is best. Conversely, if the p-value is above the significance threshold, the nested model is considered to be the best.</p>
<p>In the case of the linear model <a href="6-fit.html#eq:emblm">(6.24)</a>, the test statistic is a ratio of the mean squares, which under the null hypothesis follows Fisher’s distribution. This is the same type of test as that used to test the overall significance of a multiple regression, or that used in the “stepwise” method of selecting variables. In the general case of the non-linear model <a href="6-fit.html#eq:embnl">(6.23)</a>, the test statistic is a likelihood ratio, such that <span class="math inline">\(-2\log\)</span>(likelihood ratio) under the null hypothesis follows a <span class="math inline">\(\chi^2\)</span> distribution.</p>
<div class="filrouge">
<ol start="25" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fboite" class="exercise"><strong>Red line 6.19  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Testing nested models: <span class="math inline">\(\ln(D)\)</span></strong></span></p>
</div>
<p>In red line <a href="6-fit.html#exr:selvar">6.18</a> the variable <span class="math inline">\([\ln(D)]^2\)</span> was selected with <span class="math inline">\(\ln(H)\)</span> as effect variable of <span class="math inline">\(\ln(B)\)</span> but not <span class="math inline">\(\ln(D)\)</span>. Model <span class="math inline">\(\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_4\ln(H)\)</span>, which includes the additional term <span class="math inline">\(\ln(D)\)</span>, can be compared with the model <span class="math inline">\(\ln(B)=a_0+a_2[\ln(D)]^2+a_4\ln(H)\)</span> using the nested models test. In R, the <code>anova</code> command can be used to test a nested model, with the first argument being the nested model and the second being the complete model:</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb77-1"><a href="6-fit.html#cb77-1"></a>comp &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb77-2"><a href="6-fit.html#cb77-2"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(heig)), </span>
<span id="cb77-3"><a href="6-fit.html#cb77-3"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb77-4"><a href="6-fit.html#cb77-4"></a>  )</span>
<span id="cb77-5"><a href="6-fit.html#cb77-5"></a>nest &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb77-6"><a href="6-fit.html#cb77-6"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(heig)), </span>
<span id="cb77-7"><a href="6-fit.html#cb77-7"></a>  <span class="dt">data=</span>dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb77-8"><a href="6-fit.html#cb77-8"></a>  )</span>
<span id="cb77-9"><a href="6-fit.html#cb77-9"></a><span class="kw">anova</span>(nest, comp)</span></code></pre></div>
<p>The test gives the following result:</p>
<pre class="Rout"><code>##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1     38 6.0605                           
## 2     37 5.8964  1   0.16407 1.0295 0.3169</code></pre>
<p>The p-value is , therefore greater than 5 %. The nested model (without <span class="math inline">\(\ln(D)\)</span>) is therefore selected rather than the complete model.</p>
</div>
<div class="filrouge">
<ol start="26" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fboite2" class="exercise"><strong>Red line 6.20  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Testing nested models: <span class="math inline">\(\ln(H)\)</span></strong></span></p>
</div>
<p>The model <span class="math inline">\(\ln(B)=-8.42722+2.36104\ln(D)\)</span> was obtained in red line <a href="6-fit.html#exr:rllnBvD">6.1</a> whereas red line <a href="6-fit.html#exr:flnDlnH">6.4</a> gave the model <span class="math inline">\(\ln(B)=-8.9050+1.8654\ln(D)+0.7083\ln(H)\)</span>. As the first is nested in the second, we can test for which is the best. The command</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb79-1"><a href="6-fit.html#cb79-1"></a>comp &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb79-2"><a href="6-fit.html#cb79-2"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(heig)), </span>
<span id="cb79-3"><a href="6-fit.html#cb79-3"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb79-4"><a href="6-fit.html#cb79-4"></a>  )</span>
<span id="cb79-5"><a href="6-fit.html#cb79-5"></a>nest &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb79-6"><a href="6-fit.html#cb79-6"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)), </span>
<span id="cb79-7"><a href="6-fit.html#cb79-7"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb79-8"><a href="6-fit.html#cb79-8"></a>  )</span>
<span id="cb79-9"><a href="6-fit.html#cb79-9"></a><span class="kw">anova</span>(nest, comp)</span></code></pre></div>
<p>yields:</p>
<pre class="Rout"><code>##   Res.Df    RSS Df Sum of Sq     F   Pr(&gt;F)   
## 1     39 8.3236                               
## 2     38 6.4014  1    1.9222 11.41 0.001698 **</code></pre>
<p>As the p-value is less than 5 %, the complete model (including <span class="math inline">\(\ln(H)\)</span> as effect variable) is selected rather than the nested model.</p>
</div>
</div>
<div id="models-with-the-same-response-variable" class="section level4 unnumbered">
<h4>Models with the same response variable</h4>
<p>When we want to compare two models that have the same response variable but are not nested, we can no longer use a statistical test. For example, we cannot use the above-mentioned test to compare <span class="math inline">\(B=a_0+a_1D+\varepsilon\)</span> and <span class="math inline">\(B=a_0+a_2D^2H+\varepsilon\)</span>. In this case, we must use an information criterion <span class="citation">(Bozdogan <a href="bibliography.html#ref-bozdogan87" role="doc-biblioref">1987</a>; Burnham and Anderson <a href="bibliography.html#ref-burnham02" role="doc-biblioref">2002</a>, <a href="bibliography.html#ref-burnham04" role="doc-biblioref">2004</a>)</span>. There are several, suited to different contexts. The most widespread are the “Bayesian information criterion”, or BIC, and above all the <span class="citation">Akaike (<a href="bibliography.html#ref-akaike74" role="doc-biblioref">1974</a>)</span> information criterion (or AIC). The AIC is expressed as:
<span class="math display">\[
\mathrm{AIC}=-2\ln\ell(\hat{\theta})+2q
\]</span>
where <span class="math inline">\(\ell(\hat{\theta})\)</span> if the model’s likelihood, i.e. the likelihood of the sample for the values estimated from model parameters, see equation <a href="6-fit.html#eq:vrais">(6.19)</a>, and <span class="math inline">\(q\)</span> is the number of free parameters estimated. In particular, in the case of a multiple regression against <span class="math inline">\(p\)</span> effect variables, <span class="math inline">\(q=p+1\)</span> (i.e. the <span class="math inline">\(p\)</span> coefficients associated with the <span class="math inline">\(p\)</span> effect variables plus the y-intercept). The coefficient <span class="math inline">\(-2\)</span> in front of the log-likelihood in the AIC expression is identical to that in the test statistic on the likelihood ratio in the case of nested models. Given two models with the same number of parameters, the best model is that with the highest likelihood, therefore that with the smallest AIC. At equal likelihoods, the best model is that with the fewest parameters (in accordance with the principle of Occam’s razor), and therefore is once more that with the smallest AIC. When all is said and done, the best model is that with the smallest value of AIC.</p>
<p>The BIC is similar in expression to the AIC, but with a term that penalizes more strongly the number of parameters:
<span class="math display">\[
\mathrm{BIC}=-2\ln\ell(\hat{\theta})+q\ln(n)
\]</span>
where <span class="math inline">\(n\)</span> is the number of observations. Here again, the best model is that with the smallest value of BIC. When fitting volume or biomass tables, AIC is used rather than BIC as a model selection criterion.</p>
<div class="filrouge">
<ol start="27" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:selB" class="exercise"><strong>Red line 6.21  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Selecting models with <span class="math inline">\(B\)</span> as response variable</strong></span></p>
</div>
<p>The following models with B as response variable were fitted:</p>
<ul>
<li>red line <a href="6-fit.html#exr:fDpara">6.6</a> or <a href="6-fit.html#exr:finvD">6.10</a>: <span class="math inline">\(B=-3.840\times10^{-3}D+1.124\times10^{-3}D^2\)</span></li>
<li>red line <a href="6-fit.html#exr:fDD2var">6.8</a>: <span class="math inline">\(B=-3.319456\times10^{-3}D+1.067068\times10^{-3}D^2\)</span></li>
<li>red line <a href="6-fit.html#exr:fnlsD">6.11</a>: <span class="math inline">\(B=2.492\times10^{-4}D^{2.346}\)</span></li>
<li>red line <a href="6-fit.html#exr:fnlmD">6.14</a>: <span class="math inline">\(B=2.445\times10^{-4}D^{2.35105}\)</span></li>
<li>red line <a href="6-fit.html#exr:frlpD2H">6.5</a> or <a href="6-fit.html#exr:fH">6.9</a>: <span class="math inline">\(B=2.747\times10^{-5}D^2H\)</span></li>
<li>red line <a href="6-fit.html#exr:fD2Hvar">6.7</a>: <span class="math inline">\(B=2.740688\times10^{-5}D^2H\)</span></li>
<li>red line <a href="6-fit.html#exr:fnlsD2H">6.12</a>: <span class="math inline">\(B=7.885\times10^{-5}(D^2H)^{0.9154}\)</span></li>
<li>red line <a href="6-fit.html#exr:fnlmD2H">6.15</a>: <span class="math inline">\(B=8.19\times10^{-5}(D^2H)^{0.9122144}\)</span></li>
<li>red line <a href="6-fit.html#exr:fnlsDH">6.13</a>: <span class="math inline">\(B=1.003\times10^{-4}D^{1.923}H^{0.7435}\)</span></li>
<li>red line <a href="6-fit.html#exr:fnlmDH">6.16</a>: <span class="math inline">\(B=1.109\times10^{-4}D^{1.9434876}H^{0.6926256}\)</span></li>
</ul>
<p>The models in red lines <a href="6-fit.html#exr:fDpara">6.6</a>, <a href="6-fit.html#exr:fDD2var">6.8</a>, <a href="6-fit.html#exr:frlpD2H">6.5</a> and <a href="6-fit.html#exr:fD2Hvar">6.7</a> are fitted by linear regression while the others are fitted by non-linear regression. These models have five distinct forms, with two fitting methods for each: using a weighted regression by the weighted least squares method (red lines <a href="6-fit.html#exr:fDpara">6.6</a>, <a href="6-fit.html#exr:fnlsD">6.11</a>, <a href="6-fit.html#exr:frlpD2H">6.5</a>, <a href="6-fit.html#exr:fnlsD2H">6.12</a> and <a href="6-fit.html#exr:fnlsDH">6.13</a>) or using a regression with variance model by the maximum likelihood method (red lines <a href="6-fit.html#exr:fDD2var">6.8</a>, <a href="6-fit.html#exr:fnlmD">6.14</a>, <a href="6-fit.html#exr:fD2Hvar">6.7</a>, <a href="6-fit.html#exr:fnlmD2H">6.15</a> and <a href="6-fit.html#exr:fnlmDH">6.16</a>). The predictions made by these different models are shown in Figure <a href="6-fit.html#fig:fpredB">6.18</a>. Let <code>m</code> be one of the fitted models with dbh as the only entry. A plot of the predictions made by this model may be obtained as follows:</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb81-1"><a href="6-fit.html#cb81-1"></a><span class="co">## model (red line 14)</span></span>
<span id="cb81-2"><a href="6-fit.html#cb81-2"></a>m &lt;-<span class="st">  </span><span class="kw">lm</span>(</span>
<span id="cb81-3"><a href="6-fit.html#cb81-3"></a>  <span class="dt">formula =</span> Btot <span class="op">~</span><span class="st"> </span>dbh <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(dbh<span class="op">^</span><span class="dv">2</span>), </span>
<span id="cb81-4"><a href="6-fit.html#cb81-4"></a>  <span class="dt">data =</span> dat, </span>
<span id="cb81-5"><a href="6-fit.html#cb81-5"></a>  <span class="dt">weights =</span> <span class="dv">1</span><span class="op">/</span>dat<span class="op">$</span>dbh<span class="op">^</span><span class="dv">4</span></span>
<span id="cb81-6"><a href="6-fit.html#cb81-6"></a>  )</span>
<span id="cb81-7"><a href="6-fit.html#cb81-7"></a><span class="kw">with</span>(dat, <span class="kw">plot</span>(<span class="dt">x =</span> dbh, <span class="dt">y =</span> Btot, <span class="dt">xlab =</span> <span class="st">&quot;Dbh (cm)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Biomass (t)&quot;</span>))</span>
<span id="cb81-8"><a href="6-fit.html#cb81-8"></a>D &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">1</span>], <span class="kw">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">2</span>], <span class="dt">length=</span><span class="dv">200</span>)</span>
<span id="cb81-9"><a href="6-fit.html#cb81-9"></a><span class="kw">lines</span>(D, <span class="kw">predict</span>(m, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> D)), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p>For a model <code>m</code> that has dbh and height as entries, its predictions may be obtained as follows:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb82-1"><a href="6-fit.html#cb82-1"></a><span class="co">## model (red line 22)</span></span>
<span id="cb82-2"><a href="6-fit.html#cb82-2"></a><span class="kw">library</span>(nlme)</span>
<span id="cb82-3"><a href="6-fit.html#cb82-3"></a>start &lt;-<span class="st"> </span><span class="kw">coef</span>(<span class="kw">lm</span>(</span>
<span id="cb82-4"><a href="6-fit.html#cb82-4"></a>  <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(heig)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb82-5"><a href="6-fit.html#cb82-5"></a>  ))</span>
<span id="cb82-6"><a href="6-fit.html#cb82-6"></a>start[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">exp</span>(start[<span class="dv">1</span>])</span>
<span id="cb82-7"><a href="6-fit.html#cb82-7"></a><span class="kw">names</span>(start) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b1&quot;</span>, <span class="st">&quot;b2&quot;</span>)</span>
<span id="cb82-8"><a href="6-fit.html#cb82-8"></a>m &lt;-<span class="st"> </span><span class="kw">nlme</span>(</span>
<span id="cb82-9"><a href="6-fit.html#cb82-9"></a>  <span class="dt">model   =</span> Btot <span class="op">~</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>dbh<span class="op">^</span>b1 <span class="op">*</span><span class="st"> </span>heig<span class="op">^</span>b2,</span>
<span id="cb82-10"><a href="6-fit.html#cb82-10"></a>  <span class="dt">data    =</span> <span class="kw">cbind</span>(dat, <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>),</span>
<span id="cb82-11"><a href="6-fit.html#cb82-11"></a>  <span class="dt">fixed   =</span> a <span class="op">+</span><span class="st"> </span>b1 <span class="op">+</span><span class="st"> </span>b2 <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb82-12"><a href="6-fit.html#cb82-12"></a>  <span class="dt">start   =</span> start,</span>
<span id="cb82-13"><a href="6-fit.html#cb82-13"></a>  <span class="dt">groups  =</span> <span class="op">~</span>g,</span>
<span id="cb82-14"><a href="6-fit.html#cb82-14"></a>  <span class="dt">weights =</span> <span class="kw">varPower</span>(<span class="dt">form =</span> <span class="op">~</span>dbh)</span>
<span id="cb82-15"><a href="6-fit.html#cb82-15"></a>  )</span>
<span id="cb82-16"><a href="6-fit.html#cb82-16"></a></span>
<span id="cb82-17"><a href="6-fit.html#cb82-17"></a><span class="co">## Predictions</span></span>
<span id="cb82-18"><a href="6-fit.html#cb82-18"></a>D &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">180</span>,<span class="dt">length=</span><span class="dv">20</span>)</span>
<span id="cb82-19"><a href="6-fit.html#cb82-19"></a>H &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">61</span>,<span class="dt">length=</span><span class="dv">20</span>)</span>
<span id="cb82-20"><a href="6-fit.html#cb82-20"></a>newdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">expand.grid</span>(<span class="dt">dbh=</span>D, <span class="dt">heig=</span>H), <span class="dt">g =</span> <span class="st">&quot;a&quot;</span>)</span>
<span id="cb82-21"><a href="6-fit.html#cb82-21"></a>B &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">predict</span>(m, newdata), <span class="kw">length</span>(D))</span></code></pre></div>
<p>and a plot of the biomass response surface against diameter and height may be obtained by:</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb83-1"><a href="6-fit.html#cb83-1"></a>M &lt;-<span class="st"> </span><span class="kw">persp</span>(</span>
<span id="cb83-2"><a href="6-fit.html#cb83-2"></a>  <span class="dt">x =</span> D, <span class="dt">y =</span> H, <span class="dt">z =</span> B, </span>
<span id="cb83-3"><a href="6-fit.html#cb83-3"></a>  <span class="dt">xlab =</span> <span class="st">&quot;Dbh (cm)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Height (m)&quot;</span>, <span class="dt">zlab=</span><span class="st">&quot;Biomass (t)&quot;</span>, </span>
<span id="cb83-4"><a href="6-fit.html#cb83-4"></a>  <span class="dt">ticktype =</span> <span class="st">&quot;detailed&quot;</span>, <span class="dt">theta =</span> <span class="dv">-30</span>, <span class="dt">phi =</span> <span class="dv">20</span></span>
<span id="cb83-5"><a href="6-fit.html#cb83-5"></a>  )</span>
<span id="cb83-6"><a href="6-fit.html#cb83-6"></a><span class="kw">points</span>(<span class="kw">trans3d</span>(<span class="dt">x =</span> dat<span class="op">$</span>dbh, <span class="dt">y =</span> dat<span class="op">$</span>heig, <span class="dt">z =</span> dat<span class="op">$</span>Btot, <span class="dt">pmat =</span> M))</span></code></pre></div>
<p>Given a fitted model <code>m</code>, its AIC may be calculated by the command:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb84-1"><a href="6-fit.html#cb84-1"></a><span class="kw">AIC</span>(m)</span></code></pre></div>
<p>AIC values for the 10 models listed above are given in Table <a href="6-fit.html#tab:fAICD">6.1</a>. This table illustrates a problem that arises with several statistical software packages, including R: when we maximize the log-likelihood <a href="6-fit.html#eq:ll">(6.20)</a>, the constants (such as <span class="math inline">\(-n\ln(2\pi)/2\)</span>) no longer play any role. The constant we use to calculate the log-likelihood, and by consequence AIC, is therefore a matter of convention, and different constants are used depending on the calculation. Thus, in Table <a href="6-fit.html#tab:fAICD">6.1</a>, we can see that the values of AIC in models fitted by the <code>nls</code> command are substantially higher than those in the other models: it is not that these models are substantially worse than the others, it is simply that the <code>nls</code> command uses a constant different from the others to calculate the log-likelihood. Therefore, with R, it should be remembered that the values of AIC should be compared only for models that have been fitted using the same command. In our present case, if we compare the two models that were fitted with the <code>lm</code> command, the best (i.e. that with the smallest AIC) is that which has <span class="math inline">\(D^2H\)</span> as effect variable (red line <a href="6-fit.html#exr:frlpD2H">6.5</a>). If we compare the five models fitted with the <code>nlme</code> command, the best is again that with <span class="math inline">\(D^2H\)</span> as effect variable (red line <a href="6-fit.html#exr:fD2Hvar">6.7</a>). And if we compare the three models fitted with the <code>nls</code> command, the best is yet again that with <span class="math inline">\(D^2H\)</span> as effect variable (red line <a href="6-fit.html#exr:fnlsD2H">6.12</a>). It may therefore be concluded that whatever fitting method is used, the biomass table that uses <span class="math inline">\(D^2H\)</span> as effect variable is the best.</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fpredB"></span>
<img src="source/figures/faicD1.png" alt="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by Henry et al. (2010). The data are shown as points. (A) Tables with dbh as only entry, corresponding to red lines 6.6 (red), 6.8 (green), 6.11 (blue) and 6.14 (magenta). (B) Tables with \(D^2H\) as the only effect variable, corresponding to red lines 6.5 (red), 6.7 (green), 6.12 (blue) and 6.15 (magenta). (C) Table corresponds to red line 6.13. (D) Table corresponds to red line 6.16." width="50%" height="50%"  /><img src="source/figures/faicD2.png" alt="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by Henry et al. (2010). The data are shown as points. (A) Tables with dbh as only entry, corresponding to red lines 6.6 (red), 6.8 (green), 6.11 (blue) and 6.14 (magenta). (B) Tables with \(D^2H\) as the only effect variable, corresponding to red lines 6.5 (red), 6.7 (green), 6.12 (blue) and 6.15 (magenta). (C) Table corresponds to red line 6.13. (D) Table corresponds to red line 6.16." width="50%" height="50%"  /><img src="source/figures/faicD3.png" alt="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by Henry et al. (2010). The data are shown as points. (A) Tables with dbh as only entry, corresponding to red lines 6.6 (red), 6.8 (green), 6.11 (blue) and 6.14 (magenta). (B) Tables with \(D^2H\) as the only effect variable, corresponding to red lines 6.5 (red), 6.7 (green), 6.12 (blue) and 6.15 (magenta). (C) Table corresponds to red line 6.13. (D) Table corresponds to red line 6.16." width="50%" height="50%"  /><img src="source/figures/faicD4.png" alt="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by Henry et al. (2010). The data are shown as points. (A) Tables with dbh as only entry, corresponding to red lines 6.6 (red), 6.8 (green), 6.11 (blue) and 6.14 (magenta). (B) Tables with \(D^2H\) as the only effect variable, corresponding to red lines 6.5 (red), 6.7 (green), 6.12 (blue) and 6.15 (magenta). (C) Table corresponds to red line 6.13. (D) Table corresponds to red line 6.16." width="50%" height="50%"  />
<p class="caption">
Figure 6.18: Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>. The data are shown as points. (A) Tables with dbh as only entry, corresponding to red lines <a href="6-fit.html#exr:fDpara">6.6</a> (red), <a href="6-fit.html#exr:fDD2var">6.8</a> (green), <a href="6-fit.html#exr:fnlsD">6.11</a> (blue) and <a href="6-fit.html#exr:fnlmD">6.14</a> (magenta). (B) Tables with <span class="math inline">\(D^2H\)</span> as the only effect variable, corresponding to red lines <a href="6-fit.html#exr:frlpD2H">6.5</a> (red), <a href="6-fit.html#exr:fD2Hvar">6.7</a> (green), <a href="6-fit.html#exr:fnlsD2H">6.12</a> (blue) and <a href="6-fit.html#exr:fnlmD2H">6.15</a> (magenta). (C) Table corresponds to red line <a href="6-fit.html#exr:fnlsDH">6.13</a>. (D) Table corresponds to red line <a href="6-fit.html#exr:fnlmDH">6.16</a>.
</p>
</div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;">
<caption>
<span id="tab:fAICD">Table 6.1: </span>AIC values for 10 biomass tables fitted to data from 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>. These 10 tables directly predict biomass.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Red line
</th>
<th style="text-align:left;">
Entry
</th>
<th style="text-align:left;">
Fitting<span class="math inline">\(^*\)</span> method
</th>
<th style="text-align:left;">
R command
</th>
<th style="text-align:right;">
AIC
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:fDpara">6.6</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D\)</span>
</td>
<td style="text-align:left;">
WLS
</td>
<td style="text-align:left;">
<code>lm</code>
</td>
<td style="text-align:right;">
76.71133
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:fDD2var">6.8</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D\)</span>
</td>
<td style="text-align:left;">
ML
</td>
<td style="text-align:left;">
<code>nlme</code>
</td>
<td style="text-align:right;">
83.09157
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:fnlsD">6.11</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D\)</span>
</td>
<td style="text-align:left;">
WLS
</td>
<td style="text-align:left;">
<code>nls</code>
</td>
<td style="text-align:right;">
24809.75727
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:fnlmD">6.14</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D\)</span>
</td>
<td style="text-align:left;">
ML
</td>
<td style="text-align:left;">
<code>nlme</code>
</td>
<td style="text-align:right;">
75.00927
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:frlpD2H">6.5</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D^2H\)</span>
</td>
<td style="text-align:left;">
WLS
</td>
<td style="text-align:left;">
<code>lm</code>
</td>
<td style="text-align:right;">
65.15002
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:fD2Hvar">6.7</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D^2H\)</span>
</td>
<td style="text-align:left;">
ML
</td>
<td style="text-align:left;">
<code>nlme</code>
</td>
<td style="text-align:right;">
69.09644
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:fnlsD2H">6.12</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D^2H\)</span>
</td>
<td style="text-align:left;">
WLS
</td>
<td style="text-align:left;">
<code>nls</code>
</td>
<td style="text-align:right;">
24797.53706
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:fnlmD2H">6.15</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D^2H\)</span>
</td>
<td style="text-align:left;">
ML
</td>
<td style="text-align:left;">
<code>nlme</code>
</td>
<td style="text-align:right;">
69.24482
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:fnlsDH">6.13</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D\)</span>, <span class="math inline">\(H\)</span>
</td>
<td style="text-align:left;">
WLS
</td>
<td style="text-align:left;">
<code>nls</code>
</td>
<td style="text-align:right;">
24802.91248
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:fnlmDH">6.16</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D\)</span>, <span class="math inline">\(H\)</span>
</td>
<td style="text-align:left;">
ML
</td>
<td style="text-align:left;">
<code>nlme</code>
</td>
<td style="text-align:right;">
76.80204
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; " colspan="100%">
<sup>*</sup> WLS = weighted least squares, ML = maximum likelihood
</td>
</tr>
</tfoot>
</table>
<div class="filrouge">
<ol start="28" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:sellnB" class="exercise"><strong>Red line 6.22  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Selecting models with <span class="math inline">\(\ln(B)\)</span> as response variable</strong></span></p>
</div>
<p>The following models with <span class="math inline">\(\ln(B)\)</span> as response variable were fitted:</p>
<ul>
<li>red line <a href="6-fit.html#exr:rllnBvD">6.1</a> or <a href="6-fit.html#exr:flnDpol">6.3</a>: <span class="math inline">\(\ln(B)=-8.42722+2.36104\ln(D)\)</span></li>
<li>red line <a href="6-fit.html#exr:rllnBvD2H">6.2</a>: <span class="math inline">\(\ln(B)=-8.99427+0.87238\ln(D^2H)\)</span></li>
<li>red line <a href="6-fit.html#exr:flnDlnH">6.4</a>: <span class="math inline">\(\ln(B)=-8.9050+1.8654\ln(D)+0.7083\ln(H)\)</span></li>
<li>red line <a href="6-fit.html#exr:selvar">6.18</a>: <span class="math inline">\(\ln(B)=-6.50202+0.23756[\ln(D)]^2+1.01874\ln(H)\)</span></li>
</ul>
<p>All these models were fitted using linear regression by the ordinary least squares method. A plot of the predictions on a log scale for model <code>m</code> with dbh as only entry may be obtained by the following command:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb85-1"><a href="6-fit.html#cb85-1"></a><span class="co">## Model (red line 7)</span></span>
<span id="cb85-2"><a href="6-fit.html#cb85-2"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,])</span>
<span id="cb85-3"><a href="6-fit.html#cb85-3"></a></span>
<span id="cb85-4"><a href="6-fit.html#cb85-4"></a><span class="kw">with</span>(dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,], <span class="kw">plot</span>(</span>
<span id="cb85-5"><a href="6-fit.html#cb85-5"></a>  <span class="dt">x =</span> dbh, <span class="dt">y =</span> Btot, <span class="dt">xlab =</span> <span class="st">&quot;Dbh (cm)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Biomass (t)&quot;</span>, <span class="dt">log =</span> <span class="st">&quot;xy&quot;</span></span>
<span id="cb85-6"><a href="6-fit.html#cb85-6"></a>  ))</span>
<span id="cb85-7"><a href="6-fit.html#cb85-7"></a>D &lt;-<span class="st"> </span><span class="dv">10</span><span class="op">^</span><span class="kw">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]</span>
<span id="cb85-8"><a href="6-fit.html#cb85-8"></a><span class="kw">lines</span>(D, <span class="kw">exp</span>(<span class="kw">predict</span>(m, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> D))))</span></code></pre></div>
<p>For a model that uses both dbh and height as entries, a plot on a log scale may be obtained by the command:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb86-1"><a href="6-fit.html#cb86-1"></a><span class="co">## Model (red line 10)</span></span>
<span id="cb86-2"><a href="6-fit.html#cb86-2"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(heig)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]) </span>
<span id="cb86-3"><a href="6-fit.html#cb86-3"></a></span>
<span id="cb86-4"><a href="6-fit.html#cb86-4"></a>D &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">seq</span>(<span class="kw">log</span>(<span class="dv">1</span>), <span class="kw">log</span>(<span class="dv">180</span>), <span class="dt">length =</span> <span class="dv">20</span>))</span>
<span id="cb86-5"><a href="6-fit.html#cb86-5"></a>H &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">seq</span>(<span class="kw">log</span>(<span class="dv">1</span>), <span class="kw">log</span>(<span class="dv">61</span>), <span class="dt">length =</span> <span class="dv">20</span>))</span>
<span id="cb86-6"><a href="6-fit.html#cb86-6"></a>B &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">predict</span>(m, <span class="dt">newdata =</span> <span class="kw">expand.grid</span>(<span class="dt">dbh =</span> D, <span class="dt">heig =</span> H)), <span class="kw">length</span>(D))</span>
<span id="cb86-7"><a href="6-fit.html#cb86-7"></a>M &lt;-<span class="st"> </span><span class="kw">persp</span>(</span>
<span id="cb86-8"><a href="6-fit.html#cb86-8"></a>  <span class="dt">x =</span> <span class="kw">log</span>(D), <span class="dt">y =</span> <span class="kw">log</span>(H), <span class="dt">z =</span> B, </span>
<span id="cb86-9"><a href="6-fit.html#cb86-9"></a>  <span class="dt">xlab=</span><span class="st">&quot;log(Diameter) (cm)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;log(height) (m)&quot;</span>, <span class="dt">zlab=</span><span class="st">&quot;log(Biomass) (t)&quot;</span>, </span>
<span id="cb86-10"><a href="6-fit.html#cb86-10"></a>  <span class="dt">ticktype =</span> <span class="st">&quot;detailed&quot;</span>, <span class="dt">theta =</span> <span class="dv">-30</span>, <span class="dt">phi =</span> <span class="dv">20</span></span>
<span id="cb86-11"><a href="6-fit.html#cb86-11"></a>  )</span>
<span id="cb86-12"><a href="6-fit.html#cb86-12"></a><span class="kw">points</span>(<span class="kw">trans3d</span>(<span class="kw">log</span>(dat<span class="op">$</span>dbh), <span class="kw">log</span>(dat<span class="op">$</span>heig), <span class="kw">log</span>(dat<span class="op">$</span>Btot), M))</span></code></pre></div>
<p>Predictions of <span class="math inline">\(\ln(B)\)</span> by the four models are given in Figure <a href="6-fit.html#fig:fpredlnB">6.19</a>. Given a fitted model <code>m</code>, its AIC may be calculated by the command:</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb87-1"><a href="6-fit.html#cb87-1"></a><span class="kw">AIC</span>(m)</span></code></pre></div>
<pre class="Rout"><code>## [1] 45.96998</code></pre>
<p>Table <a href="6-fit.html#tab:fAIClnD">6.2</a> gives AIC values for the four models. As all four models were fitted by the same <code>lm</code> command, the AIC values are directly comparable. The best model, i.e. that with the smallest AIC, is the fourth (red line <a href="6-fit.html#exr:selvar">6.18</a>). It should also be noted that an AIC-based classification of these models is entirely consistent with the results of the nested models tests performed previously (red lines <a href="6-fit.html#exr:fboite">6.19</a> and <a href="6-fit.html#exr:fboite2">6.20</a>).</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fpredlnB"></span>
<img src="source/figures/faiclnD1.png" alt="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by Henry et al. (2010). The data are shown as points. (A) Model in red line 6.1. (B) Model in red line 6.2. (C) Model in red line 6.4. (D) Model in red line 6.18." width="50%" height="50%"  /><img src="source/figures/faiclnD2.png" alt="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by Henry et al. (2010). The data are shown as points. (A) Model in red line 6.1. (B) Model in red line 6.2. (C) Model in red line 6.4. (D) Model in red line 6.18." width="50%" height="50%"  /><img src="source/figures/faiclnD3.png" alt="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by Henry et al. (2010). The data are shown as points. (A) Model in red line 6.1. (B) Model in red line 6.2. (C) Model in red line 6.4. (D) Model in red line 6.18." width="50%" height="50%"  /><img src="source/figures/faiclnD4.png" alt="Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by Henry et al. (2010). The data are shown as points. (A) Model in red line 6.1. (B) Model in red line 6.2. (C) Model in red line 6.4. (D) Model in red line 6.18." width="50%" height="50%"  />
<p class="caption">
Figure 6.19: Biomass predictions by different tables fitted to data from 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>. The data are shown as points. (A) Model in red line <a href="6-fit.html#exr:rllnBvD">6.1</a>. (B) Model in red line <a href="6-fit.html#exr:rllnBvD2H">6.2</a>. (C) Model in red line <a href="6-fit.html#exr:flnDlnH">6.4</a>. (D) Model in red line <a href="6-fit.html#exr:selvar">6.18</a>.
</p>
</div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:fAIClnD">Table 6.2: </span>AIC values for four biomass tables fitted to data from 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>. These four tables predict the log of the biomass and are all fitted using linear regression by the ordinary least squares (OLS) method.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Red line
</th>
<th style="text-align:left;">
Entry
</th>
<th style="text-align:left;">
Fitting method
</th>
<th style="text-align:left;">
R command
</th>
<th style="text-align:right;">
AIC
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:rllnBvD">6.1</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D\)</span>
</td>
<td style="text-align:left;">
OLS
</td>
<td style="text-align:left;">
<code>lm</code>
</td>
<td style="text-align:right;">
56.97923
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:rllnBvD2H">6.2</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D^2H\)</span>
</td>
<td style="text-align:left;">
OLS
</td>
<td style="text-align:left;">
<code>lm</code>
</td>
<td style="text-align:right;">
46.87780
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:flnDlnH">6.4</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D\)</span>, <span class="math inline">\(H\)</span>
</td>
<td style="text-align:left;">
OLS
</td>
<td style="text-align:left;">
<code>lm</code>
</td>
<td style="text-align:right;">
48.21367
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="6-fit.html#exr:selvar">6.18</a>
</td>
<td style="text-align:left;">
<span class="math inline">\(D\)</span>, <span class="math inline">\(H\)</span>
</td>
<td style="text-align:left;">
OLS
</td>
<td style="text-align:left;">
<code>lm</code>
</td>
<td style="text-align:right;">
45.96998
</td>
</tr>
</tbody>
</table>
</div>
<div id="furni" class="section level4 unnumbered">
<h4>Models with different response variables</h4>
<p>The more general case is when we want to compare two models that do not have the same response variable because one is a transform of the other. For example, the models <span class="math inline">\(B=aD^b+\varepsilon\)</span> and <span class="math inline">\(\ln(B)=a+b\ln(D)+\varepsilon\)</span> both predict biomass, but the response variable is <span class="math inline">\(B\)</span> in one case and <span class="math inline">\(\ln(B)\)</span> in the other. In this case, we cannot use the AIC or BIC to compare the models. By contrast, <span class="citation">Furnival (<a href="bibliography.html#ref-furnival61" role="doc-biblioref">1961</a>)</span> index can in this case be used to compare the models. The model with the smallest Furnival’s index is considered to be the best <span class="citation">(Parresol <a href="bibliography.html#ref-parresol99" role="doc-biblioref">1999</a>)</span>.</p>
<p>Furnival’s index is defined only for a model whose residual error <span class="math inline">\(\varepsilon\)</span> has a variance that is assumed to be constant: <span class="math inline">\(\mbox{Var}(\varepsilon)=\sigma^2\)</span>. By contrast, no constraint is imposed on the form of the transformation of the variable linking the modeled response variable <span class="math inline">\(Y\)</span> to the variable of interest (volume or biomass). Let us consider the case of a biomass model (that can immediately be transposed into a volume model) and let <span class="math inline">\(\psi\)</span> be this variable transformation: <span class="math inline">\(Y=\psi(B)\)</span>. Furnival’s index is defined by:
<span class="math display">\[
F=\frac{\hat{\sigma}}{\sqrt[n]{\prod_{i=1}^n\psi&#39;(B_i)}}
=\exp\Big(-\frac{1}{n}\sum_{i=1}^n\ln[\psi&#39;(B_i)]\Big)\ \hat{\sigma}
\]</span>
where <span class="math inline">\(\hat{\sigma}\)</span> if the estimation of the residual standard deviation of the fitted model and <span class="math inline">\(B_i\)</span> is the biomass of the <span class="math inline">\(i\)</span>th tree measured. When there is no transformation of variables, <span class="math inline">\(\psi\)</span> is the identity function and Furnival’s index <span class="math inline">\(F\)</span> is then equal to the residual standard deviation <span class="math inline">\(\hat{\sigma}\)</span>. The most common variable transformation is the log transformation:
<span class="math inline">\(\psi(B)=\ln(B)\)</span> and <span class="math inline">\(\psi&#39;(B)=1/B\)</span>, in which case Furnival’s index is:
<span class="math display">\[
F_{\ln}=\hat{\sigma}\sqrt[n]{\textstyle\prod_{i=1}^nB_i}
=\exp\Big(\frac{1}{n}\sum_{i=1}^n\ln(B_i)\Big)\ \hat{\sigma}
\]</span>
For linear regressions where the residual variance is assumed to be proportional to a power of an effect variable <span class="math inline">\(X_1\)</span>, a trick can nevertheless be used to define Furnival’s index. The linear regression:
<span class="math display" id="eq:F1">\[\begin{equation}
Y=a_0+a_1X_1+a_2X_2+\ldots+a_pX_p+\varepsilon\tag{6.25}
\end{equation}\]</span>
where <span class="math inline">\(\mbox{Var}(\varepsilon)=(kX_1^c)^2\)</span>; is strictly identical to the linear regression:
<span class="math display" id="eq:F2">\[\begin{equation}
Y&#39;=a_0X_1^{-c}+a_1X_1^{1-c}+a_2X_2X_1^{-c}+\ldots+
a_pX_pX_1^{-c}+\varepsilon&#39;\tag{6.26}
\end{equation}\]</span>
where <span class="math inline">\(Y&#39;=YX_1^{-c}\)</span>, <span class="math inline">\(\varepsilon&#39;=\varepsilon X_1^{-c}\)</span> and <span class="math inline">\(\mbox{Var}(\varepsilon&#39;)=k^2\)</span>. As model <a href="6-fit.html#eq:F2">(6.26)</a> has constant residual variance, its Furnival’s index is defined. By extension, we can define the Furnival index of model <a href="6-fit.html#eq:F1">(6.25)</a> as being the Furnival index of model <a href="6-fit.html#eq:F2">(6.26)</a>. If <span class="math inline">\(Y=\psi(B)\)</span>, then <span class="math inline">\(Y&#39;=X_1^{-c}\psi(B)\)</span>, such that Furnival’s index is:
<span class="math display">\[
F=\frac{\hat{k}}{\sqrt[n]{\prod_{i=1}^nX_{i1}^{-c}\psi&#39;(B_i)}}
=\exp\Big(\frac{1}{n}\sum_{i=1}^n\{c\ln(X_{i1})-\ln[\psi&#39;(B_i)]\}
\Big)\ \hat{k}
\]</span>
Therefore, Furnival’s index can be used to select the value of exponent <span class="math inline">\(c\)</span> in a weighted regression.</p>
</div>
</div>
<div id="choosing-a-fitting-method" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Choosing a fitting method</h3>
<p>Let us return to the method used to fit a volume or biomass model. Many solutions may be available to fit a model. Let us for instance consider the biomass model:
<span class="math display">\[
B=a\rho^{b_1}D^{b_2}H^{b_3}+\varepsilon
\]</span>
where
<span class="math display">\[
\varepsilon\sim\mathcal{N}(0,\ kD^c)
\]</span>
This model could be adjusted as a non-linear model (<em>i</em>) by the weighted least squares method (<span class="math inline">\(c\)</span> fixed in advance) or (<em>ii</em>) by the maximum likelihood method (<span class="math inline">\(c\)</span> not fixed in advance). If we apply a log transformation to the data, we could (<em>iii</em>) fit the multiple regression:
<span class="math display">\[
\ln(B)=a&#39;+b_1\ln(\rho)+b_2\ln(D)+b_3\ln(H)+\varepsilon
\]</span>
where
<span class="math display">\[
\varepsilon\sim\mathcal{N}(0,\ \sigma)
\]</span>
Thus, for a given model that predicts biomass as a power of effect variables, we have three possible fitting methods. Obviously, methods (<em>i</em>)–(<em>ii</em>) and (<em>iii</em>) are based on different hypotheses concerning the structure of the residual errors: additive error with regard to <span class="math inline">\(B\)</span> in cases (<em>i</em>) and (<em>ii</em>), multiplicative error with regard to <span class="math inline">\(B\)</span> in case (<em>iii</em>). But both these error types reflect data heteroscedasticity, and therefore fitting methods (<em>i</em>), (<em>ii</em>) and (<em>iii</em>) all have a chance of being valid.</p>
<p>As another example, let us consider the biomass table:
<span class="math display">\[
B=\exp\{a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(\rho)\}+\varepsilon
\]</span>
where
<span class="math display">\[
\varepsilon\sim\mathcal{N}(0,\ kD^c)
\]</span>
Here again, we can (<em>i</em>) fit a non-linear model by the least squares method (specifying <span class="math inline">\(c\)</span> in advance), (<em>ii</em>) fit a non-linear model by the maximum likelihood method (estimating <span class="math inline">\(c\)</span>), or (<em>ii</em>) fit a multiple regression on log-transformed data:
<span class="math display">\[
\ln(B)=a_0+a_1\ln(D)+a_2[\ln(D)]^2+a_3[\ln(D)]^3+a_4\ln(\rho)+\varepsilon
\]</span>
where
<span class="math display">\[
\varepsilon\sim\mathcal{N}(0,\ \sigma)
\]</span>
Here again, the structure of the errors is not the same in the three cases, but all can correctly reflect the heteroscedasticity of the biomass. In most cases, the different fitting methods give very similar results in terms of predictions. Should any doubt persist regarding the most appropriate fitting method, then model selection methods may be employed to decide. But in practice the choice of a particular fitting method results rather from the importance given to the respective advantages and drawbacks of each method. Multiple regression has the drawback of imposing constraints on the form of the residuals, and has less flexibility for the mean model. By contrast, it has the advantage of offering an explicit expression for the estimators of the model’s coefficients: there is therefore no risk of having erroneous estimators for the coefficients. The non-linear model has the advantage of not imposing any constraints on the mean model or the variance model. Its drawback is that it does not have an explicit expression for parameter estimators: there is therefore the risk of having erroneous parameter estimators.</p>
<div class="filrouge">
<ol start="29" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fmeth" class="exercise"><strong>Red line 6.23  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Power model fitting methods</strong></span></p>
</div>
<p>We have already taken a look at three methods for fitting the power model <span class="math inline">\(B=aD^b\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>using a simple linear regression on log-transformed data (red line <a href="6-fit.html#exr:rllnBvD">6.1</a>): <span class="math inline">\(\ln(B)=-8.42722+2.36104\ln(D)\)</span>, if we “naively” apply the exponential inverse transformation;</p></li>
<li><p>using a weighted non-linear regression (red line <a href="6-fit.html#exr:fnlsD">6.11</a>): <span class="math inline">\(B=2.492\times10^{-4}D^{2.346}\)</span>;</p></li>
<li><p>using a non-linear regression with variance model (red line <a href="6-fit.html#exr:fnlmD">6.14</a>): <span class="math inline">\(B=2.445\times10^{-4}D^{2.35105}\)</span>.</p></li>
</ol>
<p>The predictions given by these three fittings of the same model are illustrated in Figure <a href="6-fit.html#fig:fselm">6.20</a>. The differences can be seen to be minimal and well below prediction precision, as we will see later ( <a href="7-util.html#BVpred">7.2</a>).</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:fselm"></span>
<img src="source/figures/fselm.png" alt="Biomass predictions by the same power table fitted in three different ways to data from 42 trees measured in Ghana by Henry et al. (2010). The data are shown as points. The red line is the linear regression on log-transformed data (red line 6.1). The green line (practically superimposed by the blue line) is the fitting by weighted non-linear regression (red line 6.11). The blue color shows the fitting by non-linear regression with variance model (red line 6.14). (A) No data transformation. (B) On a log scale." width="60%" />
<p class="caption">
Figure 6.20: Biomass predictions by the same power table fitted in three different ways to data from 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>. The data are shown as points. The red line is the linear regression on log-transformed data (red line <a href="6-fit.html#exr:rllnBvD">6.1</a>). The green line (practically superimposed by the blue line) is the fitting by weighted non-linear regression (red line <a href="6-fit.html#exr:fnlsD">6.11</a>). The blue color shows the fitting by non-linear regression with variance model (red line <a href="6-fit.html#exr:fnlmD">6.14</a>). (A) No data transformation. (B) On a log scale.
</p>
</div>

</div>
</div>
<div id="stratification-and-aggregation-factors" class="section level2">
<h2><span class="header-section-number">6.4</span> Stratification and aggregation factors</h2>
<p>Up until now we have considered that the dataset used to fit the volume or biomass model is homogeneous. In reality, the dataset may be drawn from measurements made under different conditions, or may have been generated by merging several separate datasets. Covariables are generally used to describe this dataset heterogeneity. For example, a covariable may indicate the type of forest in which the measurements were made (deciduous, semi-deciduous, evergreen, etc.), the type of soil, or year planted (if a plantation), etc. A crucial covariable for multispecific data is tree species. Initially, all these covariables that can explain the heterogeneity of a dataset were considered as qualitative variables (or factors). The various modalities of these factors define strata, and a well constituted dataset is one that is sampled in relation to previously identified strata (see § <a href="2-samp.html#stratif">2.2.3</a>). How can these qualitative covariables be taken into account in a volume or biomass model? Is it valid to analyze the dataset in an overall manner? Or should we analyze the data subsets corresponding to each stratum separately? These are the questions we will be addressing here (§ <a href="6-fit.html#stdat">6.4.1</a>).</p>
<p>Also, biomass measurements are made separately for each compartment in the tree (see chapter <a href="3-ter.html#ter">3</a>). Therefore, in addition to an estimation of its total biomass, we also have, for each tree in the sample, an estimation of the biomass of its foliage, the biomass of its truck, of its large branches, of its small branches, etc. How can we take account of these different compartments when establishing biomass models? We will also be addressing this question (§ <a href="6-fit.html#cmpt">6.4.2</a>).</p>
<div id="stdat" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Stratifying data</h3>
<p>Let us now consider that there are qualitative covariables that stratify the dataset into <span class="math inline">\(S\)</span> strata. As each stratum corresponds to a cross between the modalities of the qualitative covariables (in the context of experimental plans we speak of <em>treatment</em> rather than stratum) we will not consider each covariable separately. For example, if one covariable indicates the type of forest with three modalities (let us say: deciduous forest, semi-deciduous forest and evergreen forest) and another covariable indicates the type of soil with three modalities (let us say: sandy soil, clay soil, loamy soil), the cross between these two covariables gives <span class="math inline">\(S=3\times 3=9\)</span> strata (deciduous forest on sandy soil, deciduous forest on clay soil, etc.). We are not looking to analyze the forest type effect separately, or the soil type effect separately. Also, if certain combinations of the covariable modalities are not represented in the dataset, this reduces the number of strata. For example, if there are no evergreen forests on loamy soil, the number of strata <span class="math inline">\(S=8\)</span> instead of 9.</p>
<p>Therefore, when stratifying a dataset, one strategy consists in fitting a model separately for each stratum. In the case of a multiple regression, this would be written:
<span class="math display">\[
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon_s
\]</span>
where
<span class="math display">\[
\varepsilon_s\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma_s)
\]</span>
where (<span class="math inline">\(Y_s\)</span>, <span class="math inline">\(X_{1s}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_{ps}\)</span>) designates an observation relative to stratum <span class="math inline">\(s\)</span>, for <span class="math inline">\(s=1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(S\)</span>. There are now <span class="math inline">\(S\times(p+1)\)</span> coefficients to be estimated. An alternative strategy consists in analyzing the dataset as a whole, by fitting a model such as:
<span class="math display" id="eq:ancov">\[\begin{equation}
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon
\tag{6.27}
\end{equation}\]</span>
where
<span class="math display">\[
\varepsilon\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma)
\]</span>
This model differs only in the structure of the error. This type of model is called an analysis of <em>covariance</em>. It assumes that all the residuals have the same variance, not only within each stratum, but also from one stratum to another. An analysis of covariance can test whether there is a stratum effect on the response variable, alone or in interaction with each of the effect variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span>. Testing the main effect of the stratification is equivalent to testing the null hypothesis <span class="math inline">\(a_{01}=a_{02}=\ldots=a_{0S}\)</span>. The test statistic is a mean squares ratio which, under the null hypothesis, follows Fisher’s distribution. Testing the effect of the interaction between the stratification and the <span class="math inline">\(j\)</span>th effect variable is equivalent to testing the null hypothesis <span class="math inline">\(a_{j1}=a_{j2}=\ldots=a_{jS}\)</span>. As previously, the test statistic is a mean squares ratio which, under the null hypothesis, follows Fisher’s distribution.</p>
<p>The advantage of testing these effects is that each time one proves not to be significant, the <span class="math inline">\(S\)</span> coefficients <span class="math inline">\(a_{j1}\)</span>, <span class="math inline">\(a_{j2}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(a_{jS}\)</span> to be estimated can be replaced by a single common coefficient <span class="math inline">\(a_j\)</span>. Let us imagine, for example, that in analysis of covariance <a href="6-fit.html#eq:ancov">(6.27)</a>, the main effect of the stratum is not significant, and neither is the interaction between the stratum and the first <span class="math inline">\(p&#39;\)</span> effect variables (where <span class="math inline">\(p&#39;&lt;p\)</span>). In this case the model to be fitted is:
<span class="math display">\[
Y_s=a_0+a_1X_{1s}+\ldots+a_{p&#39;}X_{p&#39;s}+a_{p&#39;+1,s}X_{p&#39;+1,s}+\ldots+a_{ps}X_{ps}+\varepsilon
\]</span>
where <span class="math inline">\(\varepsilon\sim\mathcal{N}(0,\ \sigma)\)</span>. This model now includes “only” <span class="math inline">\(p&#39;+1+(p-p&#39;)S\)</span> coefficients to be estimated, instead of <span class="math inline">\((p+1)S\)</span> coefficients if we fit the model separately for each stratum. As all the observations serve to estimate common coefficients <span class="math inline">\(a_0\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(a_{p&#39;}\)</span>, these are estimated more precisely than if we fit the model separately for each stratum.</p>
<p>This principle of an analysis of covariance can be immediately extended to the case of a non-linear model. Here again, we can test whether or not the coefficients are significantly different between the strata, and if necessary estimate a common coefficient for all the strata.</p>
<div class="filrouge">
<ol start="30" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fmspe" class="exercise"><strong>Red line 6.24  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Specific biomass model</strong></span></p>
</div>
<p>In red line <a href="6-fit.html#exr:rllnBvD2H">6.2</a>, we used simple linear regression to fit a power model with <span class="math inline">\(D^2H\)</span> as effect variable to log-transformed data: <span class="math inline">\(\ln(B)=a+b\ln(D^2H)\)</span>. We can now include species-related information in this model to test whether the coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> differ from one species to another. The model corresponds to an analysis of covariance:
<span class="math display">\[
\ln(B_s)=a_s+b_s\ln(D_s^2H_s)+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]</span>
where index <span class="math inline">\(s\)</span> designates the species. This model is fitted by the command:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb89-1"><a href="6-fit.html#cb89-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb89-2"><a href="6-fit.html#cb89-2"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span>species <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig)), </span>
<span id="cb89-3"><a href="6-fit.html#cb89-3"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb89-4"><a href="6-fit.html#cb89-4"></a>  )</span></code></pre></div>
<p>To test whether the coefficients <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> differ from one species to another, we can use the command:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb90-1"><a href="6-fit.html#cb90-1"></a><span class="kw">anova</span>(m)</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##                              Df  Sum Sq Mean Sq   F value    Pr(&gt;F)    
## species                      15 117.667   7.844   98.4396 1.647e-13 ***
## I(log(dbh^2 * heig))          1 112.689 112.689 1414.1228 &lt; 2.2e-16 ***
## species:I(log(dbh^2 * heig))  7   0.942   0.135    1.6879    0.1785    
## Residuals                    17   1.355   0.080</code></pre>
<p>The first line in the table tests for a species effect, i.e. whether the y-intercept <span class="math inline">\(a_s\)</span> differs from one species to another. The null hypothesis of this test is that there is no difference between species: <span class="math inline">\(a_1=a_2=\ldots=a_S\)</span>, where <span class="math inline">\(S=16\)</span> is the number of species. The test statistic is given in the “F value” column. As the p-value of the test is less than 5 %, it may be concluded that the y-intercept of the model is significantly different from one species to another. The second line in the table tests for an effect of the <span class="math inline">\(D^2H\)</span>, variable, i.e. whether the mean slope associated with this variable is significantly different from zero. The third line in the table tests whether the slope-species interaction is significant, i.e. whether slope <span class="math inline">\(b_s\)</span> differs from one species to another. The null hypothesis is that there is no difference between species: <span class="math inline">\(b_1=b_2=\ldots=b_S\)</span>. The p-value here is 0.1785, therefore greater than 5 %: there is therefore no significant slope difference between the species.</p>
<p>This leads us to fit the following model:
<span class="math display" id="eq:mspe">\[\begin{equation}
\ln(B_s)=a_s+b\ln(D_s^2H_s)+\varepsilon\tag{6.28}
\end{equation}\]</span>
which considers that slope <span class="math inline">\(b\)</span> is the same for all species. The relevant command is:</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb92-1"><a href="6-fit.html#cb92-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb92-2"><a href="6-fit.html#cb92-2"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span>species <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig)),</span>
<span id="cb92-3"><a href="6-fit.html#cb92-3"></a>  <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb92-4"><a href="6-fit.html#cb92-4"></a>  )</span>
<span id="cb92-5"><a href="6-fit.html#cb92-5"></a><span class="kw">anova</span>(m)</span></code></pre></div>
<p>and yields:</p>
<pre class="Rout"><code>##                      Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## species              15 117.667   7.844   81.99 &lt; 2.2e-16 ***
## I(log(dbh^2 * heig))  1 112.689 112.689 1177.81 &lt; 2.2e-16 ***
## Residuals            24   2.296   0.096</code></pre>
<p>Model coefficients may be obtained by the command:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb94-1"><a href="6-fit.html#cb94-1"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##                                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                       -9.0036     0.4514  -19.94   &lt;2e-16 ***
## speciesAubrevillea kerstingii     -0.5463     0.4378   -1.25    0.224    
## speciesCecropia peltata           -0.7769     0.3626   -2.14    0.043 *  
## speciesCeiba pentandra            -0.7084     0.3805   -1.86    0.075 .  
## speciesCola nitida                -0.4643     0.4448   -1.04    0.307    
## speciesDaniellia thurifera         0.0469     0.4641    0.10    0.920    
## speciesDialium aubrevilliei       -0.1563     0.4376   -0.36    0.724    
## speciesDrypetes chevalieri         0.0495     0.4539    0.11    0.914    
## speciesGarcinia epunctata          1.0965     0.4732    2.32    0.029 *  
## speciesGuarea cedrata             -0.4525     0.3846   -1.18    0.251    
## speciesHeritiera utilis           -0.2686     0.3266   -0.82    0.419    
## speciesNauclea diderrichii        -0.5546     0.3576   -1.55    0.134    
## speciesNesogordonia papaverifera  -0.4782     0.4434   -1.08    0.292    
## speciesPiptadeniastrum africanum  -0.1796     0.3572   -0.50    0.620    
## speciesStrombosia glaucescens      0.0633     0.3960    0.16    0.874    
## speciesTieghemella heckelii       -0.0910     0.3391   -0.27    0.791    
## I(log(dbh^2 * heig))               0.8998     0.0262   34.32   &lt;2e-16 ***</code></pre>
<p>The last line in this table gives the value of the slope: <span class="math inline">\(b=0.89985\)</span>. The other lines give y-intercept values for the 16 species. By convention, R proceeds in the following manner when specifying these values: the first line in the table gives the y-intercept for the first species in alphabetical order. As the first species in alphabetical order is <em>Afzelia bella</em>, the y-intercept for <em>Afzelia bella</em> is therefore <span class="math inline">\(a_1=-9.00359\)</span>. The subsequent lines give the <em>difference</em> <span class="math inline">\(a_s-a_1\)</span> between the y-intercepts of the species indicated and the y-intercept of <em>Afzelia bella</em>. Thus, the y-intercept of <em>Aubrevillea kerstingii</em> is: <span class="math inline">\(a_2=a_1-0.54634=-9.00359-0.54634=-9.54993\)</span>. Finally, the expression for the specific table corresponds to:
<span class="math display">\[
\ln(B)=0.89985\ln(D^2H)-\left\{
\begin{array}{lcl}
9.00359 &amp;&amp; \textrm{for } \textit{Afzelia bella}   \\ %
9.54993 &amp;&amp; \textrm{for } \textit{Aubrevillea kerstingii}   \\ %
9.78047 &amp;&amp; \textrm{for } \textit{Cecropia peltata}         \\ %
9.71200 &amp;&amp; \textrm{for } \textit{Ceiba pentandra}          \\ %
9.46786 &amp;&amp; \textrm{for } \textit{Cola nitida}              \\ %
8.95674 &amp;&amp; \textrm{for } \textit{Daniellia thurifera}      \\ %
9.15985 &amp;&amp; \textrm{for } \textit{Dialium aubrevilliei}     \\ %
8.95406 &amp;&amp; \textrm{for } \textit{Drypetes chevalieri}      \\ %
7.90713 &amp;&amp; \textrm{for } \textit{Garcinia epunctata}       \\ %
9.45614 &amp;&amp; \textrm{for } \textit{Guarea cedrata}           \\ %
9.27223 &amp;&amp; \textrm{for } \textit{Heritiera utilis}         \\ %
9.55823 &amp;&amp; \textrm{for } \textit{Nauclea diderrichii}      \\ %
9.48176 &amp;&amp; \textrm{for } \textit{Nesogordonia papaverifera}\\ %
9.18315 &amp;&amp; \textrm{for } \textit{Piptadeniastrum africanum}\\ %
8.94026 &amp;&amp; \textrm{for } \textit{Strombosia glaucescens}   \\ %
9.09462 &amp;&amp; \textrm{for } \textit{Tieghemella heckelii}     \\ %
\end{array}
\right.
\]</span></p>
<p>For a given diameter and height, the species with the highest biomass is <em>Garcinia epunctata</em> while that with the lowest biomass is <em>Cecropia peltata</em>. The model’s residual standard deviation is <span class="math inline">\(\hat{\sigma}=0.3093\)</span> and <span class="math inline">\(R^2=0.9901\)</span>.</p>
</div>
<div id="case-of-a-numerical-covariable" class="section level4 unnumbered">
<h4>Case of a numerical covariable</h4>
<p>Up until now we have considered that the covariables defining the stratification are qualitative factors. But in certain cases these covariables can also be interpreted as numerical variables. For instance, let us consider a biomass model for plantations <span class="citation">(Saint-André et al. <a href="bibliography.html#ref-saintandre05" role="doc-biblioref">2005</a>)</span>. The year the plantation was set up (or, which comes to the same thing, the age of the trees), may be used as a stratification covariable. This year or age may be seen indifferently as a qualitative variable (cohort of trees of the same age) or as a numerical value. More generally, any numerical variable may be seen as a qualitative variable if it is divided into classes. In the case of age, we could also consider plantations aged 0 to 5 years as a stratum, plantations aged 5 to 10 years as another stratum, plantations aged 10 to 20 years as a third stratum, etc.. The advantage of dividing covariable <span class="math inline">\(Z\)</span> into classes and considering it as a qualitative variable is that this allows us to model the relation between <span class="math inline">\(Z\)</span> and response variable <span class="math inline">\(Y\)</span> without any <em>a priori</em> constraint on the form of this relation. By contrast, if we consider <span class="math inline">\(Z\)</span> to be a numerical variable, we are obliged <em>a priori</em> to set the form of the relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> (linear relation, polynomial relation, exponential relation, power relation, etc.). The disadvantage of dividing <span class="math inline">\(Z\)</span> into classes and considering this covariable as being qualitative is that the division introduces an element of randomness. Also, the covariance model that uses classes of <span class="math inline">\(Z\)</span> (qualitative covariables) will generally have more parameters that need to be estimated than the model that considers <span class="math inline">\(Z\)</span> to be a numerical covariable.</p>
<p>It is customary in modeling to play on this dual interpretation of numerical variables. When covariable <span class="math inline">\(Z\)</span> is numerical (e.g. tree age), it is advisable to proceed in two steps (as explained in § <a href="5-explo.html#plus">5.1.1</a>):</p>
<ol style="list-style-type: decimal">
<li><p>consider <span class="math inline">\(Z\)</span> as a qualitative variable (if necessary after division into classes) and fit a covariance model which will provide a picture of the form of the relation between Z and the model’s coefficients;</p></li>
<li><p>model this relation using an appropriate expression then return to the fitting of a linear or non-linear model, but considering <span class="math inline">\(Z\)</span> to be a numerical variable.</p></li>
</ol>
<p>If we return to the example of the age of the trees in a plantation: let us assume that age <span class="math inline">\(Z\)</span> has been divided into <span class="math inline">\(S\)</span> age classes. The first step consists typically of an analysis of covariance (assuming that the model has been rendered linear):
<span class="math display">\[
Y_s=a_{0s}+a_{1s}X_{1s}+a_{2s}X_{2s}+\ldots+a_{ps}X_{ps}+\varepsilon
\]</span>
where <span class="math inline">\(s=1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(S\)</span>. Let <span class="math inline">\(Z_s\)</span> be the median age of age class <span class="math inline">\(s\)</span>. We then draw scatter plots of <span class="math inline">\(a_{0s}\)</span> against <span class="math inline">\(Z_s\)</span>, <span class="math inline">\(a_{1s}\)</span> against <span class="math inline">\(Z_s\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(a_{ps}\)</span> against <span class="math inline">\(Z_s\)</span> and for each scatter plot we look for the form of the relation that fits the plot. Let us imagine that <span class="math inline">\(a_{0s}\)</span> varies in a linear manner with <span class="math inline">\(Z_s\)</span>, that <span class="math inline">\(a_{1s}\)</span> varies in a exponential manner with <span class="math inline">\(Z_s\)</span>, that <span class="math inline">\(a_{2s}\)</span> varies in a power manner with <span class="math inline">\(Z_s\)</span>, and that the coefficients <span class="math inline">\(a_{3s}\)</span> to <span class="math inline">\(a_{ps}\)</span> do not vary in relation to <span class="math inline">\(Z_s\)</span> (which besides can be formally tested). In this particular case we will then, as the second step, fit the following non-linear model:
<span class="math display">\[
Y=\underbrace{b_0+b_1Z}_{a_{0s}}+\underbrace{b_2\exp(-b_3Z)}_{a_{1s}}X_1
+\underbrace{b_4Z^{b_5}}_{a_{2s}}X_2+a_3X_3+\ldots+a_pX_p+\varepsilon
\]</span>
where age <span class="math inline">\(Z\)</span> is now considered as a numerical variable. Such a model involving a numerical effect covariable is called a <em>parameterized</em> model (here by age).</p>
<p><em>Ordinal</em> covariables warrant a special remark. An ordinal variable is a qualitative variable that establishes an order. For example, the month of the year is a qualitative variable that establishes a chronological order. The type of soil along a soil fertility gradient is also an ordinal variable. Ordinal variables are generally treated as fully fledged qualitative variables, but in this case the order information they contain is lost. An alternative consists in numbering the ordered modalities of the ordinal variable using integers, then considering the ordinal variable as a numerical variable. For example, in the case of the months of the year, we can set January = 1, February = 2, etc. This approach is meaningful only if the differences between the integers correctly reflect the differences between the modalities of the ordinal variable. For instance, if we set 1 = January 2011 up to 12 = December 2011, we will set 1 = January 2012 if the response is cyclically seasonal, whereas we will set 13 = January 2012 if the response presents as a continuous trend. In the case of three soil types along a fertility gradient, we will set 1 = poorest soil, 2 = soil of intermediate fertility, and 3 = richest soil if we consider the fertility difference between two soils induces a response that is proportional to this difference, but we will set 1 = poorest soil, 4 = soil of intermediate fertility, and 9 = richest soil if we consider that the response is proportional to the square of the fertility difference.</p>
</div>
<div id="special-case-of-the-species" class="section level4 unnumbered">
<h4>Special case of the species</h4>
<p>In the case of multispecific datasets, the species is a stratification covariable that warrants special attention. If the dataset includes only a few species (less than about 10), and there are sufficient observations per species (see § <a href="2-samp.html#size">2.2.1</a>), then species may be considered to be a stratification covariable like any other. The model in this case would be divided into <span class="math inline">\(S\)</span> specific models, or the models could be grouped together based on the allometric resemblance of the species.</p>
<p>If the dataset contains many species, or if only a few observations are available for certain species, it is difficult to treat the species as a stratification covariable. A solution in this case consists in using species <em>functional traits</em>. Functional traits are defined here, a little abusively, as the numerical variables that characterize the species <span class="citation">(Dı́az and Cabido <a href="bibliography.html#ref-diaz97" role="doc-biblioref">1997</a>; Rösch, Van Rooyen, and Theron <a href="bibliography.html#ref-rosch97" role="doc-biblioref">1997</a>; Lavorel and Garnier <a href="bibliography.html#ref-lavorel02" role="doc-biblioref">2002</a>; see Violle et al. <a href="bibliography.html#ref-violle07" role="doc-biblioref">2007</a> for a more rigorous definition)</span>. The most widely used trait in biomass models is wood density. If we decide to use functional traits to represent species, these traits intervene as effect variables in the model in the same way as the effect variables that characterize the tree, for example its dbh or height. The single-entry (dbh) monospecific biomass model of the power type, which in its linear form may be written:
<span class="math display">\[
\ln(B)=a_0+a_1\ln(D)+\varepsilon
\]</span>
will thus, in the multispecific case, become a double-entry biomass model:
<span class="math display">\[
\ln(B)=a_0+a_1\ln(D)+a_2\ln(\rho)+\varepsilon
\]</span>
if we decide to use wood density <span class="math inline">\(\rho\)</span> to represent the specific effect.</p>
<div class="filrouge">
<ol start="31" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fdens" class="exercise"><strong>Red line 6.25  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Specific wood density-dependent biomass model</strong></span></p>
</div>
<p>In red line <a href="6-fit.html#exr:fmspe">6.24</a>, species-related information was taken into account in the model <span class="math inline">\(\ln(B)=a+b\ln(D^2H)\)</span> through a qualitative covariable. We can now capture this information through specific wood density <span class="math inline">\(\rho\)</span>. The fitted model is therefore:
<span class="math display" id="eq:dens">\[\begin{equation}
\ln(B)=a_0+a_1\ln(D^2H)+a_2\ln(\rho)+\varepsilon\tag{6.29}
\end{equation}\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]</span>
As wood density in the dataset was measured for each individual, we must start by calculating mean wood density for each species:</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb96-1"><a href="6-fit.html#cb96-1"></a>dm &lt;-<span class="st"> </span><span class="kw">tapply</span>(dat<span class="op">$</span>dens, dat<span class="op">$</span>species, mean)</span>
<span id="cb96-2"><a href="6-fit.html#cb96-2"></a>dat2 &lt;-<span class="st"> </span><span class="kw">cbind</span>(dat, <span class="dt">dmoy =</span> dm[<span class="kw">as.character</span>(dat<span class="op">$</span>species)])</span></code></pre></div>
<p>The <code>dat2</code> dataset now contains an additional variable <code>dmoy</code> that gives specific wood density. The model is fitted by the command:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb97-1"><a href="6-fit.html#cb97-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb97-2"><a href="6-fit.html#cb97-2"></a>  <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dmoy)), </span>
<span id="cb97-3"><a href="6-fit.html#cb97-3"></a>  <span class="dt">data =</span> dat2[dat2<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]</span>
<span id="cb97-4"><a href="6-fit.html#cb97-4"></a>  )</span>
<span id="cb97-5"><a href="6-fit.html#cb97-5"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##                      Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          -8.38900    0.26452 -31.714  &lt; 2e-16 ***
## I(log(dbh^2 * heig))  0.85715    0.02031  42.205  &lt; 2e-16 ***
## I(log(dmoy))          0.72864    0.17720   4.112 0.000202 ***</code></pre>
<p>with a residual standard deviation of and <span class="math inline">\(R^2=\)</span> . The model is therefore written: <span class="math inline">\(\ln(B)=-8.38900+0.85715\ln(D^2H)+0.72864\ln(\rho)\)</span>. Is it better to take account of the species through wood density, as we have just done, or by constructing specific models as in red line <a href="6-fit.html#exr:fmspe">6.24</a>? To answer this question we need to compare model <a href="6-fit.html#eq:mspe">(6.28)</a> to the model <a href="6-fit.html#eq:dens">(6.29)</a> using the AIC:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb99-1"><a href="6-fit.html#cb99-1"></a><span class="kw">AIC</span>(m)</span></code></pre></div>
<p>which yields <span class="math inline">\(\mathrm{AIC}=34.17859\)</span> for specific model <a href="6-fit.html#eq:mspe">(6.28)</a> and <span class="math inline">\(\mathrm{AIC}=33.78733\)</span> for the model <a href="6-fit.html#eq:dens">(6.29)</a> that uses wood density. The latter therefore is slightly better, but the difference in AIC is nevertheless minor.</p>
</div>
<p>To better take account of wood density variations within a tree, it is possible to analyze inter- and intra-specific variations rather than use a mean density based on the hypothesis that wood density is the same from the pith to the bark and from the top to the bottom of trees (see chapter <a href="1-biol.html#biol">1</a>). Wood density can be modeled by taking account of factors such as species, functional group, tree dimensions, and radial or vertical position in the tree. An initial comparison may be made using Friedman’s analysis of variance test, followed by Tuckey’s highly significant difference (HSD) test. These tests will indicate the variables that most influence wood density. This wood density may then be modeled based on these variables <span class="citation">(Henry et al. <a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span>.</p>
<div class="filrouge">
<ol start="32" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:fidens" class="exercise"><strong>Red line 6.26  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Individual wood density-dependent biomass model</strong></span></p>
</div>
<p>In red line <a href="6-fit.html#exr:fdens">6.25</a>, wood density <span class="math inline">\(\rho\)</span> was defined for the species by calculating the mean of individual densities for trees in the same species. Let us now fit a biomass model based on individual wood density measurements in order to take account of inter-individual variations in wood density in a given species. The fitted model is:
<span class="math display">\[
\ln(B)=a_0+a_1\ln(D)+a_2\ln(\rho)+\varepsilon
\]</span>
where
<span class="math display">\[
\mathrm{Var}(\varepsilon)=\sigma^2
\]</span>
where <span class="math inline">\(\rho\)</span> unlike in red line <a href="6-fit.html#exr:fdens">6.25</a>, is here the <em>individual</em> measurement of wood density. The model is fitted by the command:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb100-1"><a href="6-fit.html#cb100-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dens)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,])</span>
<span id="cb100-2"><a href="6-fit.html#cb100-2"></a><span class="kw">summary</span>(m)</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -7.76644    0.20618 -37.668  &lt; 2e-16 ***
## I(log(dbh))   2.35272    0.04812  48.889  &lt; 2e-16 ***
## I(log(dens))  1.00717    0.14053   7.167 1.46e-08 ***</code></pre>
<p>with a residual standard deviation of and <span class="math inline">\(R^2=\)</span> . The model is written: <span class="math inline">\(\ln(B)=-7.76644+2.35272\ln(D)+1.00717\ln(\rho)\)</span>. According to this model, biomass is dependent upon individual wood density by the term <span class="math inline">\(\rho^{1.00717}\)</span>, i.e. practically <span class="math inline">\(\rho\)</span>. By way of a comparison, model <a href="6-fit.html#eq:dens">(6.29)</a> was dependent upon specific wood density by the term <span class="math inline">\(\rho^{0.72864}\)</span>. From a biological standpoint, the exponent 1.00717 is more satisfactory than the exponent 0.72864 since it means that the biomass is the product of a volume (that depends solely on tree dimensions) and a density. The difference between the two exponents may be attributed to inter-individual variations in wood density within the species. But the model based on individual wood density has little practical usefulness as it would require a wood density measurement in every tree whose biomass we are seeking to predict.</p>
</div>
</div>
</div>
<div id="cmpt" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Tree compartments</h3>
<p>Tree biomass is determined separately for each compartment in the tree (stump, trunk, large branches, small branches, leaves, etc.). Total above-ground biomass is the sum of these compartments. The approach already presented for fitting a model could be followed for each compartment separately. In this case we would construct a model for leaf biomass, a model for the biomass of the large branches, etc.. This approach integrates dataset stratification. Thus, we could initially fit a model for each compartment and each stratum; as a second step, and depending on the differences found between strata, we could aggregate the strata and/or parameterize the model in order to construct a model by compartment for all the strata. But the approach would not end there. We could also continue the data integration in order to move toward a smaller number of more integrating models.</p>
<div id="compartment-additivity" class="section level4 unnumbered">
<h4>Compartment additivity</h4>
<p>As total above-ground biomass is the sum of the biomasses of the different compartments, it could be imagined that the best model for predicting above-ground biomass is the sum of the models predicting the biomass of each compartment. In fact, because of the correlations between the biomasses of the different compartment, this is not the case <span class="citation">(Cunia and Briggs <a href="bibliography.html#ref-cunia84" role="doc-biblioref">1984</a>, <a href="bibliography.html#ref-cunia85b" role="doc-biblioref">1985</a><a href="bibliography.html#ref-cunia85b" role="doc-biblioref">a</a>; Parresol <a href="bibliography.html#ref-parresol99" role="doc-biblioref">1999</a>)</span>. Also, certain model families are not stable by addition. This in particular is the case for power models: the sum of two power functions is not a power function. If we have fitted a power model for each compartment:
<span class="math display">\[\begin{eqnarray*}
B^{\mathrm{stump}} &amp;=&amp; a_1D^{b_1}
\\ B^{\mathrm{trunk}} &amp;=&amp; a_2D^{b_2}
\\ B^{\mathrm{large\ branches}} &amp;=&amp; a_3D^{b_3}
\\ B^{\mathrm{small\ branches}} &amp;=&amp; a_4D^{b_4}
\\ B^{\mathrm{leaves}} &amp;=&amp; a_5D^{b_5}
\end{eqnarray*}\]</span>
The sum <span class="math inline">\(B^{\textrm{above-ground}}=B^{\mathrm{stump}} +B^{\mathrm{trunk}}+B^{\mathrm{large\ branches}} +B^{\mathrm{small\ branches}}+B^{\mathrm{leaves}}\)</span> = <span class="math inline">\(\sum_{m=1}^5a_m\)</span> <span class="math inline">\(D^{b_m}\)</span> is not a power function of dbh. By contrast, polynomial models are stable by addition.</p>
</div>
<div id="fitting-a-multivariate-model" class="section level4 unnumbered">
<h4>Fitting a multivariate model</h4>
<p>In order to take account of the correlations between the biomasses of the different compartments, we can fit the models relative to the different compartments in a simultaneous manner rather than separately. This last step in model integration requires us to redefine the response variable. As we are looking to predict simultaneously the biomasses of different compartments, we are no longer dealing with a response variable but a <em>response vector</em> <span class="math inline">\(\mathbf{Y}\)</span>. The length of this vector is equal to the number <span class="math inline">\(M\)</span> of compartments. For example, if the response variable is the biomass,
<span class="math display">\[
\mathbf{Y}=\left[
\begin{array}{l}
B^{\textrm{above-ground}}\\ %
B^{\mathrm{stump}}\\ %
B^{\mathrm{trunk}}\\ %
B^{\mathrm{large\ branches}}\\ %
B^{\mathrm{small\ branches}}\\ %
B^{\mathrm{leaves}}
\end{array}
\right]
\]</span>
If the response variable is the log of the biomass,
<span class="math display">\[
\mathbf{Y}=\left[
\begin{array}{l}
\ln(B^{\textrm{above-ground}})\\ %
\ln(B^{\mathrm{stump}})\\ %
\ln(B^{\mathrm{trunk}})\\ %
\ln(B^{\mathrm{large\ branches}})\\ %
\ln(B^{\mathrm{small\ branches}})\\ %
\ln(B^{\mathrm{leaves}})
\end{array}
\right]
\]</span>
Let <span class="math inline">\(Y_m\)</span> be the response variable of the <span class="math inline">\(m\)</span>th compartment (where <span class="math inline">\(m=1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(M\)</span>). Without loss of generality, we can consider that all the compartments have the same set <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span> of effect variables (if a variable is not included in the prediction of a compartment, the corresponding coefficient can simply be set at zero). A model that predicts a response vector rather than a response variable is a multivariate model. An observation used to fit a multivariate model consists of a vector (<span class="math inline">\(Y_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(Y_M\)</span>, <span class="math inline">\(X_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span>) of length <span class="math inline">\(M+p\)</span>. The residual of a multivariate model is a vector <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> of length <span class="math inline">\(M\)</span>, equal to the difference between the observed response vector and the predicted response vector.</p>
<p>The expression of an <span class="math inline">\(M\)</span>-variate model differs from the <span class="math inline">\(M\)</span> univariate models corresponding to the different compartments only by the structure of the residual error; the structure of the mean model does not change. Let us consider the general case of a non-linear model. If the <span class="math inline">\(M\)</span> univariate models are:
<span class="math display" id="eq:uvar">\[\begin{equation}
Y_m=f_m(X_1,\ \ldots,\ X_p;\theta_m)+\varepsilon_m\tag{6.30}
\end{equation}\]</span>
for <span class="math inline">\(m=1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(M\)</span>, then the multivariate model is written:
<span class="math display">\[
\mathbf{Y}=\mathbf{F}(X_1,\ \ldots,\ X_p;\boldsymbol{\theta})+
\boldsymbol{\varepsilon}
\]</span>
where <span class="math inline">\(\mathbf{Y}={}^{\mathrm{t}}{[Y_1,\ \ldots,\ Y_M]}\)</span>, <span class="math inline">\(\boldsymbol{\theta}={}^{\mathrm{t}}{[\theta_1,\ \ldots,\ \theta_M]}\)</span>, and
<span class="math display" id="eq:mvar">\[\begin{equation}
\mathbf{F}(X_1,\ \ldots,\ X_p;\boldsymbol{\theta})=\left[
\begin{array}{c}
f_1(X_1,\ \ldots,\ X_p;\theta_1)
\\\vdots\\
f_m(X_1,\ \ldots,\ X_p;\theta_m)
\\\vdots\\
f_M(X_1,\ \ldots,\ X_p;\theta_M)
\end{array}
\right]\tag{6.31}
\end{equation}\]</span>
The residual vector <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> now follows a centered multinormal distribution, with a variance-covariance matrix of:
<span class="math display">\[
\mathrm{Var}(\boldsymbol{\varepsilon})\equiv\boldsymbol{\Sigma}
=\left[
\begin{array}{cccc}
\sigma_1^2 &amp; \zeta_{12} &amp; \cdots        &amp; \zeta_{1M}\\ %
\zeta_{21} &amp; \sigma_2^2 &amp; \ddots        &amp; \vdots\\ %
\vdots     &amp; \ddots     &amp; \ddots        &amp; \zeta_{M-1,M}\\ %
\zeta_{M1} &amp; \cdots     &amp; \zeta_{M,M-1} &amp; \sigma_M^2
\end{array}
\right]
\]</span>
Matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is symmetrical with <span class="math inline">\(M\)</span> lines and <span class="math inline">\(M\)</span> columns, such that <span class="math inline">\(\sigma_m^2=\mathrm{Var}(\varepsilon_m)\)</span> is the residual variance of the biomass of the <span class="math inline">\(m\)</span>th compartment and <span class="math inline">\(\zeta_{ml}=\zeta_{lm}\)</span> is the residual covariance between the biomass of the <span class="math inline">\(m\)</span>th compartment and that of the <span class="math inline">\(l\)</span>th compartment. Like in the univariate case, two residuals corresponding to two different observations are assumed to be independent: <span class="math inline">\(\boldsymbol{\varepsilon}_i\)</span> is independent of <span class="math inline">\(\boldsymbol{\varepsilon}_j\)</span> pour <span class="math inline">\(i\neq j\)</span>. The difference arises from the fact that the different compartments are no longer assumed to be independent one from another.</p>
<p>A multivariate model such as <a href="6-fit.html#eq:mvar">(6.31)</a> is fitted based on the same principles as univariate models <a href="6-fit.html#eq:uvar">(6.30)</a>. If the variance-covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is diagonal (i.e. <span class="math inline">\(\zeta_{ml}=0\)</span>, <span class="math inline">\(\forall m\)</span>, <span class="math inline">\(l\)</span>), then the fitting of the multivariate model <a href="6-fit.html#eq:mvar">(6.31)</a> is equivalent to the separate fitting of the <span class="math inline">\(M\)</span> univariate models <a href="6-fit.html#eq:uvar">(6.30)</a>. In the case of a linear model, estimated values for coefficients <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\theta_M\)</span> resulting from fitting the <span class="math inline">\(M\)</span>-variate linear model are the same as the values obtained by the separate fitting of the <span class="math inline">\(M\)</span> univariate linear models (on condition that the same effect variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span> are used in all cases) <span class="citation">(Muller and Stewart <a href="bibliography.html#ref-muller06" role="doc-biblioref">2006</a>, chap. 3)</span>. But the significance tests associated with the coefficients do not give the same results in the two cases. If the different compartments are sufficiently correlated one with the other, the simultaneous fitting of all the compartments by the multivariate model <a href="6-fit.html#eq:mvar">(6.31)</a> will yield a more precise estimation of model coefficients, and therefore more precise biomass predictions.</p>
</div>
<div id="harmonizing-a-model" class="section level4 unnumbered">
<h4>Harmonizing a model</h4>
<p>In certain cases, particularly energy wood, we are looking to predict the dry biomass of the trunk at different cross-cut diameters. For instance, we want to predict simultaneously the total biomass <span class="math inline">\(B\)</span> of the trunk, the biomass <span class="math inline">\(B_{7}\)</span> of the trunk to the small-end diameter of 7~cm, and the biomass <span class="math inline">\(B_{10}\)</span> of the trunk to the small-end diameter of 10~cm. We could consider the entire trunk, the trunk to a cross-cut of 7~cm, and the trunk to a cross-cut of 10~cm as three different compartments and apply the same fitting principles as presented in the previous section. In fact, the problem is more complex because, unlike the trunk and leaf compartments which are separate, the compartments defined by different cross-cut diameters are nested one within the others: <span class="math inline">\(B=B_{7}+\)</span> biomass of the section to a small-end diameter of 7~cm, and <span class="math inline">\(B_{7}=B_{10}+\)</span> biomass of the section from the 10~cm to the 7~cm small-end cross-cuts. Thus, the multivariate model that predicts vector (<span class="math inline">\(B\)</span>, <span class="math inline">\(B_7\)</span>, <span class="math inline">\(B_{10}\)</span>) must ensure that <span class="math inline">\(B&gt;B_{7}&gt;B_{10}\)</span> across the entire valid range of the model. The process consisting of constraining the multivariate model in such a manner that it predicts the biomasses of the different compartments while checking the logic of their nesting is called model <em>harmonization</em> <span class="citation">(Parresol <a href="bibliography.html#ref-parresol99" role="doc-biblioref">1999</a>)</span>. <span class="citation">Jacobs and Cunia (<a href="bibliography.html#ref-jacobs80" role="doc-biblioref">1980</a>)</span> and <span class="citation">Cunia and Briggs (<a href="bibliography.html#ref-cunia85" role="doc-biblioref">1985</a><a href="bibliography.html#ref-cunia85" role="doc-biblioref">b</a>)</span> have put forward solutions to this problem in the form of equations relating the coefficients in the models used for the different compartments. In this case we must fit an <span class="math inline">\(M\)</span>-variate model (if there are <span class="math inline">\(M\)</span> cross-cut diameters) while ensuring that the coefficients <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\theta_M\)</span> corresponding to the <span class="math inline">\(M\)</span> cross-cut diameters satisfy a number of equations linking them together. When the coefficients in the multivariate model are estimated by maximum likelihood, their numerical estimation is in fact a problem of constrained optimization.</p>
<p>If predicting the volume or biomass of a stem, an alternative to the volume or biomass model is to use stem profile integration <span class="citation">(Parresol and Thomas <a href="bibliography.html#ref-parresol89" role="doc-biblioref">1989</a>; Parresol <a href="bibliography.html#ref-parresol99" role="doc-biblioref">1999</a>)</span>. Let <span class="math inline">\(P(h)\)</span> be a stem profile, i.e. a plot of trunk transversal section area against height <span class="math inline">\(h\)</span> from the ground (<span class="math inline">\(h\)</span> also represents the length when a stem is followed from its large to its small end) <span class="citation">(Maguire and Batista <a href="bibliography.html#ref-maguire96" role="doc-biblioref">1996</a>; Dean and Roxburgh <a href="bibliography.html#ref-dean06" role="doc-biblioref">2006</a>; Metcalf, Clark, and Clark <a href="bibliography.html#ref-metcalf09" role="doc-biblioref">2009</a>)</span>. If the section through the stem is roughly circular, the diameter of the tree at height <span class="math inline">\(h\)</span> may be calculated as: <span class="math inline">\(D(h)=\sqrt{4P(h)/\pi}\)</span>. The biomass of the trunk to a cross-cut diameter <span class="math inline">\(D\)</span> is calculated by integrating the stem profile from the ground (<span class="math inline">\(h=0\)</span>) to height <span class="math inline">\(P^{-1}(\frac{\pi}{4}D^2)\)</span> corresponds to this diameter:
<span class="math display">\[
B_D=\int_0^{P^{-1}(\frac{\pi}{4}D^2)}\rho(h)\;P(h)\ \mathrm{d}h
\]</span>
where <span class="math inline">\(\rho(h)\)</span> is wood density at height <span class="math inline">\(h\)</span>. Stem volume to cross-cut diameter <span class="math inline">\(D\)</span> is calculated in the same manner, except that <span class="math inline">\(\rho\)</span> is replaced by 1. The stem profile approach has the advantage that model harmonization here is automatic. But it is an approach that is conceptually different from volume and biomass models, with specific fitting problems <span class="citation">(Fang and Bailey <a href="bibliography.html#ref-fang99" role="doc-biblioref">1999</a>; Parresol <a href="bibliography.html#ref-parresol99" role="doc-biblioref">1999</a>)</span>, and is outside the scope of this guide. It should be noted that when dealing with very large trees, for which it is almost impossible to measure biomass directly, the stem profile approach is a relevant alternative <span class="citation">(Van Pelt <a href="bibliography.html#ref-vanpelt01" role="doc-biblioref">2001</a>; Dean, Roxburgh, and Mackey <a href="bibliography.html#ref-dean03" role="doc-biblioref">2003</a>; Dean <a href="bibliography.html#ref-dean03b" role="doc-biblioref">2003</a>; Dean and Roxburgh <a href="bibliography.html#ref-dean06" role="doc-biblioref">2006</a>; Sillett et al. <a href="bibliography.html#ref-sillett10" role="doc-biblioref">2010</a>)</span>.</p>

</div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>The <code>nlme</code> command in fact serves to fit mixed effect non-linear models. The <code>nlreg</code> command is used to fit non-linear models with variance model, but we have obtained abnormal results with this command (version 3.1-96) which explains why we prefer to use the <code>nlme</code> command here, even though there is no mixed effect in the models considered here.<a href="6-fit.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="5-explo.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="7-util.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": true,
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
