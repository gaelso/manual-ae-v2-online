<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Uses and prediction | Manual for building tree volume and biomass allometric equations</title>
  <meta name="description" content="7 Uses and prediction | Manual for building tree volume and biomass allometric equations" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Uses and prediction | Manual for building tree volume and biomass allometric equations" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Uses and prediction | Manual for building tree volume and biomass allometric equations" />
  
  
  

<meta name="author" content="Nicolas Picard, Laurent Saint-André, Matieu Henry" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="6-fit.html"/>
<link rel="next" href="conclusions-and-recommendations.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html"><b>Manual for building tree volume and biomass allometric equations </b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover page</a></li>
<li class="chapter" data-level="" data-path="note-on-the-version-2.html"><a href="note-on-the-version-2.html"><i class="fa fa-check"></i>Note on the version 2</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="preamble.html"><a href="preamble.html"><i class="fa fa-check"></i>Preamble</a></li>
<li class="chapter" data-level="1" data-path="1-biol.html"><a href="1-biol.html"><i class="fa fa-check"></i><b>1</b> The foundations of biomass estimations</a><ul>
<li class="chapter" data-level="1.1" data-path="1-biol.html"><a href="1-biol.html#biology-eichhorns-rule-site-index-etc."><i class="fa fa-check"></i><b>1.1</b> “Biology”: Eichhorn’s rule, site index, etc.</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-biol.html"><a href="1-biol.html#even-aged-monospecific-stands"><i class="fa fa-check"></i><b>1.1.1</b> Even-aged, monospecific stands</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-biol.html"><a href="1-biol.html#uneven-aged-andor-multispecific-stands"><i class="fa fa-check"></i><b>1.1.2</b> Uneven-aged and/or multispecific stands</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-biol.html"><a href="1-biol.html#selecting-a-method"><i class="fa fa-check"></i><b>1.2</b> Selecting a method</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-biol.html"><a href="1-biol.html#estimating-the-biomass-of-a-biome"><i class="fa fa-check"></i><b>1.2.1</b> Estimating the biomass of a biome</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-biol.html"><a href="1-biol.html#estimating-the-biomass-of-a-forest-or-a-set-of-forests"><i class="fa fa-check"></i><b>1.2.2</b> Estimating the biomass of a forest or a set of forests</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-biol.html"><a href="1-biol.html#measuring-the-biomass-of-a-tree"><i class="fa fa-check"></i><b>1.2.3</b> Measuring the biomass of a tree</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-samp.html"><a href="2-samp.html"><i class="fa fa-check"></i><b>2</b> Sampling and stratification</a><ul>
<li class="chapter" data-level="2.1" data-path="2-samp.html"><a href="2-samp.html#simple"><i class="fa fa-check"></i><b>2.1</b> Sampling for a simple linear regression</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-samp.html"><a href="2-samp.html#predicting-the-volume-of-a-particular-tree"><i class="fa fa-check"></i><b>2.1.1</b> Predicting the volume of a particular tree</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-samp.html"><a href="2-samp.html#peup"><i class="fa fa-check"></i><b>2.1.2</b> Predicting the volume of a stand</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-samp.html"><a href="2-samp.html#sampling-to-construct-volume-tables"><i class="fa fa-check"></i><b>2.2</b> Sampling to construct volume tables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-samp.html"><a href="2-samp.html#size"><i class="fa fa-check"></i><b>2.2.1</b> Number of trees</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-samp.html"><a href="2-samp.html#vent"><i class="fa fa-check"></i><b>2.2.2</b> Sorting trees</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-samp.html"><a href="2-samp.html#stratif"><i class="fa fa-check"></i><b>2.2.3</b> Stratification</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-samp.html"><a href="2-samp.html#sel"><i class="fa fa-check"></i><b>2.2.4</b> Selecting trees</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-samp.html"><a href="2-samp.html#sampeup"><i class="fa fa-check"></i><b>2.3</b> Sampling for stand estimations</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-samp.html"><a href="2-samp.html#sampling-unit"><i class="fa fa-check"></i><b>2.3.1</b> Sampling unit</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-samp.html"><a href="2-samp.html#relation-between-coefficient-of-variation-and-plot-size"><i class="fa fa-check"></i><b>2.3.2</b> Relation between coefficient of variation and plot size</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-samp.html"><a href="2-samp.html#selecting-plot-size"><i class="fa fa-check"></i><b>2.3.3</b> Selecting plot size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ter.html"><a href="3-ter.html"><i class="fa fa-check"></i><b>3</b> In the field</a><ul>
<li class="chapter" data-level="3.1" data-path="3-ter.html"><a href="3-ter.html#pese"><i class="fa fa-check"></i><b>3.1</b> Weighing all compartments directly in the field</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-ter.html"><a href="3-ter.html#in-the-field"><i class="fa fa-check"></i><b>3.1.1</b> In the field</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-ter.html"><a href="3-ter.html#lab1"><i class="fa fa-check"></i><b>3.1.2</b> In the laboratory</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-ter.html"><a href="3-ter.html#calculations"><i class="fa fa-check"></i><b>3.1.3</b> Calculations</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-ter.html"><a href="3-ter.html#direct-weighing-for-certain-compartments-and-volume-and-density-measurements-for-others"><i class="fa fa-check"></i><b>3.2</b> Direct weighing for certain compartments and volume and density measurements for others</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-ter.html"><a href="3-ter.html#in-the-field-case-of-semi-destructive-measurements"><i class="fa fa-check"></i><b>3.2.1</b> In the field: case of semi-destructive measurements</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-ter.html"><a href="3-ter.html#lab2"><i class="fa fa-check"></i><b>3.2.2</b> In the laboratory</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-ter.html"><a href="3-ter.html#calculations-1"><i class="fa fa-check"></i><b>3.2.3</b> Calculations</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-ter.html"><a href="3-ter.html#partial-weighings-in-the-field"><i class="fa fa-check"></i><b>3.3</b> Partial weighings in the field</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-ter.html"><a href="3-ter.html#trees-less-than-20-cm-in-diameter"><i class="fa fa-check"></i><b>3.3.1</b> Trees less than 20 cm in diameter</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-ter.html"><a href="3-ter.html#trees-more-than-20-cm-in-diameter"><i class="fa fa-check"></i><b>3.3.2</b> Trees more than 20 cm in diameter</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-ter.html"><a href="3-ter.html#measuring-roots"><i class="fa fa-check"></i><b>3.4</b> Measuring roots</a></li>
<li class="chapter" data-level="3.5" data-path="3-ter.html"><a href="3-ter.html#matos"><i class="fa fa-check"></i><b>3.5</b> Recommended equipment</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-ter.html"><a href="3-ter.html#heavy-machinery-and-vehicles"><i class="fa fa-check"></i><b>3.5.1</b> Heavy machinery and vehicles</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-ter.html"><a href="3-ter.html#general-equipment"><i class="fa fa-check"></i><b>3.5.2</b> General equipment</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-ter.html"><a href="3-ter.html#computer-entering-field-data"><i class="fa fa-check"></i><b>3.5.3</b> Computer-entering field data</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-ter.html"><a href="3-ter.html#laboratory-equipment"><i class="fa fa-check"></i><b>3.5.4</b> Laboratory equipment</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-ter.html"><a href="3-ter.html#equip"><i class="fa fa-check"></i><b>3.6</b> Recommended field team composition</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-don.html"><a href="4-don.html"><i class="fa fa-check"></i><b>4</b> Entering and formatting data</a><ul>
<li class="chapter" data-level="4.1" data-path="4-don.html"><a href="4-don.html#data-entry"><i class="fa fa-check"></i><b>4.1</b> Data entry</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-don.html"><a href="4-don.html#data-entry-errors"><i class="fa fa-check"></i><b>4.1.1</b> Data entry errors</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-don.html"><a href="4-don.html#meta-information"><i class="fa fa-check"></i><b>4.1.2</b> Meta-information</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-don.html"><a href="4-don.html#nested-levels"><i class="fa fa-check"></i><b>4.1.3</b> Nested levels</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-don.html"><a href="4-don.html#data-cleansing"><i class="fa fa-check"></i><b>4.2</b> Data cleansing</a></li>
<li class="chapter" data-level="4.3" data-path="4-don.html"><a href="4-don.html#data-formatting"><i class="fa fa-check"></i><b>4.3</b> Data formatting</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-explo.html"><a href="5-explo.html"><i class="fa fa-check"></i><b>5</b> Graphical exploration of the data</a><ul>
<li class="chapter" data-level="5.1" data-path="5-explo.html"><a href="5-explo.html#exploring-the-mean-relation"><i class="fa fa-check"></i><b>5.1</b> Exploring the mean relation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-explo.html"><a href="5-explo.html#plus"><i class="fa fa-check"></i><b>5.1.1</b> When there is more than one effect variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-explo.html"><a href="5-explo.html#determining-whether-or-not-a-relation-is-adequate"><i class="fa fa-check"></i><b>5.1.2</b> Determining whether or not a relation is adequate</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-explo.html"><a href="5-explo.html#catalog-of-primitives"><i class="fa fa-check"></i><b>5.1.3</b> Catalog of primitives</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-explo.html"><a href="5-explo.html#exploring-variance"><i class="fa fa-check"></i><b>5.2</b> Exploring variance</a></li>
<li class="chapter" data-level="5.3" data-path="5-explo.html"><a href="5-explo.html#exploring-doesnt-mean-selecting"><i class="fa fa-check"></i><b>5.3</b> Exploring doesn’t mean selecting</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-fit.html"><a href="6-fit.html"><i class="fa fa-check"></i><b>6</b> Model fitting</a><ul>
<li class="chapter" data-level="6.1" data-path="6-fit.html"><a href="6-fit.html#fitting-a-linear-model"><i class="fa fa-check"></i><b>6.1</b> Fitting a linear model</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-fit.html"><a href="6-fit.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.1.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-fit.html"><a href="6-fit.html#multiple-regression"><i class="fa fa-check"></i><b>6.1.2</b> Multiple regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-fit.html"><a href="6-fit.html#pond"><i class="fa fa-check"></i><b>6.1.3</b> Weighted regression</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-fit.html"><a href="6-fit.html#lme"><i class="fa fa-check"></i><b>6.1.4</b> Linear regression with variance model</a></li>
<li class="chapter" data-level="6.1.5" data-path="6-fit.html"><a href="6-fit.html#trans"><i class="fa fa-check"></i><b>6.1.5</b> Transforming variables</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-fit.html"><a href="6-fit.html#nlm"><i class="fa fa-check"></i><b>6.2</b> Fitting a non-linear model</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-fit.html"><a href="6-fit.html#exponent-known"><i class="fa fa-check"></i><b>6.2.1</b> Exponent known</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-fit.html"><a href="6-fit.html#estimating-the-exponent"><i class="fa fa-check"></i><b>6.2.2</b> Estimating the exponent</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-fit.html"><a href="6-fit.html#algo"><i class="fa fa-check"></i><b>6.2.3</b> Numerical optimization</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-fit.html"><a href="6-fit.html#select"><i class="fa fa-check"></i><b>6.3</b> Selecting variables and models</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-fit.html"><a href="6-fit.html#selecting-variables"><i class="fa fa-check"></i><b>6.3.1</b> Selecting variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-fit.html"><a href="6-fit.html#selmod"><i class="fa fa-check"></i><b>6.3.2</b> Selecting models</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-fit.html"><a href="6-fit.html#choosing-a-fitting-method"><i class="fa fa-check"></i><b>6.3.3</b> Choosing a fitting method</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-fit.html"><a href="6-fit.html#stratification-and-aggregation-factors"><i class="fa fa-check"></i><b>6.4</b> Stratification and aggregation factors</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-fit.html"><a href="6-fit.html#stdat"><i class="fa fa-check"></i><b>6.4.1</b> Stratifying data</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-fit.html"><a href="6-fit.html#cmpt"><i class="fa fa-check"></i><b>6.4.2</b> Tree compartments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-util.html"><a href="7-util.html"><i class="fa fa-check"></i><b>7</b> Uses and prediction</a><ul>
<li class="chapter" data-level="7.1" data-path="7-util.html"><a href="7-util.html#val"><i class="fa fa-check"></i><b>7.1</b> Validating a model</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-util.html"><a href="7-util.html#Ival"><i class="fa fa-check"></i><b>7.1.1</b> Validation criteria</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-util.html"><a href="7-util.html#cross-validation"><i class="fa fa-check"></i><b>7.1.2</b> Cross validation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-util.html"><a href="7-util.html#BVpred"><i class="fa fa-check"></i><b>7.2</b> Predicting the volume or biomass of a tree</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-util.html"><a href="7-util.html#plm"><i class="fa fa-check"></i><b>7.2.1</b> Prediction: linear model</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-util.html"><a href="7-util.html#pnls"><i class="fa fa-check"></i><b>7.2.2</b> Prediction: case of a non-linear model</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-util.html"><a href="7-util.html#papp"><i class="fa fa-check"></i><b>7.2.3</b> Approximated confidence intervals</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-util.html"><a href="7-util.html#invtra"><i class="fa fa-check"></i><b>7.2.4</b> Inverse variables transformation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-util.html"><a href="7-util.html#predicting-the-volume-or-biomass-of-a-stand"><i class="fa fa-check"></i><b>7.3</b> Predicting the volume or biomass of a stand</a></li>
<li class="chapter" data-level="7.4" data-path="7-util.html"><a href="7-util.html#BEF"><i class="fa fa-check"></i><b>7.4</b> Expanding and converting volume and biomass models</a></li>
<li class="chapter" data-level="7.5" data-path="7-util.html"><a href="7-util.html#arbi"><i class="fa fa-check"></i><b>7.5</b> Arbitrating between different models</a><ul>
<li class="chapter" data-level="7.5.1" data-path="7-util.html"><a href="7-util.html#comparing-using-validation-criteria"><i class="fa fa-check"></i><b>7.5.1</b> Comparing using validation criteria</a></li>
<li class="chapter" data-level="7.5.2" data-path="7-util.html"><a href="7-util.html#choosing-a-model"><i class="fa fa-check"></i><b>7.5.2</b> Choosing a model</a></li>
<li class="chapter" data-level="7.5.3" data-path="7-util.html"><a href="7-util.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>7.5.3</b> Bayesian model averaging</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusions-and-recommendations.html"><a href="conclusions-and-recommendations.html"><i class="fa fa-check"></i>Conclusions and recommendations</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="" data-path="list-of-red-lines.html"><a href="list-of-red-lines.html"><i class="fa fa-check"></i>List of red lines</a></li>
<li class="chapter" data-level="" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="lexicon-of-mathematical-symbols.html"><a href="lexicon-of-mathematical-symbols.html"><i class="fa fa-check"></i>Lexicon of mathematical symbols</a><ul>
<li class="chapter" data-level="" data-path="lexicon-of-mathematical-symbols.html"><a href="lexicon-of-mathematical-symbols.html#latin-symbols"><i class="fa fa-check"></i>Latin symbols</a></li>
<li class="chapter" data-level="" data-path="lexicon-of-mathematical-symbols.html"><a href="lexicon-of-mathematical-symbols.html#greek-symbols"><i class="fa fa-check"></i>Greek symbols</a></li>
<li class="chapter" data-level="" data-path="lexicon-of-mathematical-symbols.html"><a href="lexicon-of-mathematical-symbols.html#non-alphabetical-symbols"><i class="fa fa-check"></i>Non-alphabetical symbols</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="http://www.globallometree.org/" target="blank"><b>More on GlobAllomeTree</b></a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank"><i>Published with bookdown</i></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Manual for building tree volume and biomass allometric equations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="util" class="section level1">
<h1><span class="header-section-number">7</span> Uses and prediction</h1>
<div class="watermark">
DRAFT
</div>
<p>Once a volume or biomass model has been fitted, several uses may be made of its predictions. Most commonly, it is used to predict the volume or biomass of trees whose volume or biomass has not been measured. This is <em>prediction</em> in the strictest sense (§ <a href="7-util.html#BVpred">7.2</a>–<a href="7-util.html#BEF">7.4</a>). Sometimes, tree volume or biomass will also have been measured in addition to table entry variables. In this case, when a dataset is available <em>independently</em> of that used to fit the model, and which contains both the model’s response variable and its effect variables, this can be used to
<em>validate</em> the model (§ <a href="7-util.html#val">7.1</a>). When validation criteria are applied to the same dataset that served to calibrate the model, we speak of model <em>verification</em> We will not dwell here on model verification as this is already implicit in the analysis of the fitted model’s residuals. Finally, if we are in possession of models that were developed prior to a new fitted model, we may also wish to compare or combine the models (§ <a href="7-util.html#arbi">7.5</a>).</p>
<div id="model-valid-range" class="section level4 unnumbered">
<h4>Model valid range</h4>
<p>Before using a model, we must check that the characteristics of the tree whose volume or biomass we wish to predict fall within the <em>valid range</em> of the model <span class="citation">(Rykiel <a href="bibliography.html#ref-rykiel96" role="doc-biblioref">1996</a>)</span>. If a volume or biomass model has been fitted for trees of dbh between <span class="math inline">\(D_{\min}\)</span> and <span class="math inline">\(D_{\max}\)</span>, in principle it cannot be used to predict the volume or biomass of a tree whose dbh is less than <span class="math inline">\(D_{\min}\)</span> or more than <span class="math inline">\(D_{\max}\)</span>. The same applies for all model entries. But not all models are subject to the same errors when extrapolating outside their valid range. Power models can generally be extrapolated outside their valid range and still yield reliable results as they are based on a fractal allometric model that is invariant on all scales <span class="citation">(Zianis and Mencuccini <a href="bibliography.html#ref-zianis04" role="doc-biblioref">2004</a>)</span>. By contrast, polynomial models often behave abnormally outside their valid range (e.g. predicting negative values), particularly when using high-order polynomials.</p>
</div>
<div id="val" class="section level2">
<h2><span class="header-section-number">7.1</span> Validating a model</h2>
<p>Validating a model consists in comparing its predictions with observations independent of those used for its fitting <span class="citation">(Rykiel <a href="bibliography.html#ref-rykiel96" role="doc-biblioref">1996</a>)</span>. Let (<span class="math inline">\(Y&#39;_i\)</span>, <span class="math inline">\(X&#39;_{i1}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X&#39;_{ip}\)</span>) with <span class="math inline">\(i=1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(n&#39;\)</span> be a dataset of observations independent of that used to fit a model <span class="math inline">\(f\)</span>, where <span class="math inline">\(X&#39;_{i1}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X&#39;_{ip}\)</span> are the effect variables and <span class="math inline">\(Y&#39;_i\)</span> is the response variable, i.e. the volume, biomass or a transform of one of these quantities. Let
<span class="math display">\[
\hat{Y}&#39;_i=f(X&#39;_{i1},\ \ldots,\ X&#39;_{ip};\hat{\theta})
\]</span>
be the predicted value of the response variable for the <span class="math inline">\(i\)</span>th observation, where <span class="math inline">\(\hat{\theta}\)</span> are the estimated values of the model’s parameters. The validation consists in comparing the predicted values <span class="math inline">\(\hat{Y}&#39;_i\)</span> with the observed values <span class="math inline">\(Y&#39;_i\)</span>.</p>
<div id="Ival" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Validation criteria</h3>
<p>Several criteria, which are the counterparts of the criteria used to evaluate the quality of model fitting, may be used to compare predictions with observations <span class="citation">(Schlaegel <a href="bibliography.html#ref-schlaegel82" role="doc-biblioref">1982</a>; Parresol <a href="bibliography.html#ref-parresol99" role="doc-biblioref">1999</a>; Tedeschi <a href="bibliography.html#ref-tedeschi06" role="doc-biblioref">2006</a>)</span>, in particular:</p>
<ul>
<li><p>bias: <span class="math inline">\(\sum_{i=1}^{n&#39;}|Y&#39;_i-\hat{Y}&#39;_i|\)</span></p></li>
<li><p>sum of squares of the residuals: <span class="math inline">\(\mathrm{SSE}=\sum_{i=1}^{n&#39;}(Y&#39;_i-\hat{Y}&#39;_i)^2\)</span></p></li>
<li><p>residual variance: <span class="math inline">\(s^2=\mathrm{SSE}/(n&#39;-p)\)</span></p></li>
<li><p>fitted residual error: <span class="math inline">\(\mathrm{SSE}/(n&#39;-2p)\)</span></p></li>
<li><p><span class="math inline">\(R^2\)</span> of the regression: <span class="math inline">\(R^2=1-s^2/\mathrm{Var}(Y&#39;)\)</span></p></li>
<li><p>Akaike information criterion: <span class="math inline">\(\mathrm{AIC}=n&#39;\ln(s^2)+n&#39;\ln(1-p/n&#39;)+2p\)</span></p></li>
</ul>
<p>where <span class="math inline">\(\mathrm{Var}(Y&#39;)\)</span> is the empirical variance of <span class="math inline">\(Y&#39;\)</span> and <span class="math inline">\(p\)</span> is the number of freely estimated parameters in the model. The first two criteria correspond to two distinct norms of the difference between the vector (<span class="math inline">\(Y&#39;_1,\ \ldots,\ Y&#39;_{n&#39;}\)</span>) of the observations and the vector (<span class="math inline">\(\hat{Y}&#39;_1,\ \ldots,\ \hat{Y}&#39;_{n&#39;}\)</span>) of the predictions: norm <span class="math inline">\(L^1\)</span> for the bias and norm <span class="math inline">\(L^2\)</span> for the sum of squares. Any other norm is also valid. The last three criteria involve the number of parameters used in the model, and are therefore more appropriate for comparing different models.</p>
</div>
<div id="cross-validation" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Cross validation</h3>
<p>If no independent dataset is available, it is tempting to split the calibration dataset into two subsets: one to fit the model, the other to validate the model. Given that volume or biomass datasets are costly, and often fairly small, we do not recommend this practice when constructing volume or biomass tables. By contrast, in this case, we do recommend the use of <em>cross validation</em> <span class="citation">(Efron and Tibshirani <a href="bibliography.html#ref-efron93" role="doc-biblioref">1993</a>, chapitre 17)</span>.</p>
<p>A “<span class="math inline">\(K\)</span> fold” cross validation consists in dividing the dataset into <span class="math inline">\(K\)</span> subsets of approximately equal size and using each subset, one after the other, as validation datasets, with
the model being fitted on the <span class="math inline">\(K-1\)</span> subsets remaining. The “<span class="math inline">\(K\)</span> fold” cross validation pseudo-algorithm is as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Split the dataset <span class="math inline">\(\mathcal{S}_n\equiv\{(Y_i,\ X_{i1},\ \ldots,\ X_{ip}): i=1,\ \ldots,\ n\}\)</span> into <span class="math inline">\(K\)</span> data subsets <span class="math inline">\(\mathcal{S}_n^{(1)},\ \ldots,\ \mathcal{S}_n^{(K)}\)</span> of approximately equal size (i.e. with about <span class="math inline">\(n/K\)</span> observations in each data subset, totaling <span class="math inline">\(n\)</span>).</p></li>
<li><p>For <span class="math inline">\(k\)</span> from 1 to <span class="math inline">\(K\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li>fit the model on the dataset deprived of its <span class="math inline">\(k\)</span>th subset, i.e. on <span class="math inline">\(\mathcal{S}_n\backslash\mathcal{S}_n^{(k)}=\mathcal{S}_n^{(1)} \cup\ldots\cup\mathcal{S}_n^{(k-1)}\cup\mathcal{S}_n^{(k+1)}\cup\ldots\cup\mathcal{S}_n^{(K)}\)</span>;</li>
<li>calculate a validation criterion (see  <a href="7-util.html#Ival">7.1.1</a>) for this fitted model by using the remaining dataset <span class="math inline">\(\mathcal{S}_n^{(k)}\)</span> as validation dataset; let <span class="math inline">\(C_k\)</span> be the value of this criterion calculated for <span class="math inline">\(\mathcal{S}_n^{(k)}\)</span>.</li>
</ol></li>
<li><p>Calculate the mean <span class="math inline">\((\sum_{k=1}^KC_k)/K\)</span> of the <span class="math inline">\(K\)</span> validation criteria calculated.</p></li>
</ol>
<p>The fact that there is no overlap between the data used for model fitting and the data used to calculate the validation criterion means that the approach is valid. Cross validation needs more calculations than a simple validation, but has the advantage of making the most of all the observations available for model fitting.</p>
<p>A special case of a “<span class="math inline">\(K\)</span> fold” cross validation is when <span class="math inline">\(K\)</span> is equal to the number <span class="math inline">\(n\)</span> of observations available in the dataset. This method is also called “leave-one-out” cross validation and is conceptually similar to the Jack-knife technique <span class="citation">(Efron and Tibshirani <a href="bibliography.html#ref-efron93" role="doc-biblioref">1993</a>)</span>. The principle consists in fitting the model on <span class="math inline">\(n-1\)</span> observations and calculating the residual error for the observation that was left out. It is used when analyzing the residuals to quantify the influence of the observations <span class="citation">(and in particular is the basis of the calculation of Cook’s distance, see Saporta <a href="bibliography.html#ref-saporta90" role="doc-biblioref">1990</a>)</span>.</p>

</div>
</div>
<div id="BVpred" class="section level2">
<h2><span class="header-section-number">7.2</span> Predicting the volume or biomass of a tree</h2>
<p>Making a prediction based on model <span class="math inline">\(f\)</span> consists in using the known values of the effect variables <span class="math inline">\(X_1,\ \ldots,\ X_p\)</span>, to calculate the predicted value <span class="math inline">\(\hat{Y}\)</span>. A prediction does not end with the calculation of
<span class="math display">\[
\hat{Y}=f(X_1,\ \ldots,\ X_p;\hat{\theta})
\]</span>
The estimator <span class="math inline">\(\hat{\theta}\)</span> of model parameters is a random vector whose distribution stems from the distribution of the observations used to fit the model. Any model prediction <span class="math inline">\(\hat{Y}\)</span> is therefore itself a random variable whose distribution stems from the distribution of the observations used to fit the model. If we are to express this intrinsic variability of the prediction, we must associate it with an uncertainty indicator such as the standard deviation of the prediction or its 95 % confidence interval.</p>
<p>Several confidence intervals are available depending on whether we are predicting the volume or biomass of a tree selected at random in the stand, or the volume or biomass of the mean tree in the stand. We will now describe in detail the analytical expressions of these confidence intervals for a linear model (§ <a href="7-util.html#plm">7.2.1</a>), then for a non-linear model (§ <a href="7-util.html#pnls">7.2.2</a>). We will then present expressions for these confidence intervals that are similar but simpler to compute (§ <a href="7-util.html#papp">7.2.3</a>, before focusing on the case of transformed variables (§ <a href="7-util.html#invtra">7.2.4</a>).</p>
<div id="plm" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Prediction: linear model</h3>
<div id="prediction-by-simple-linear-regression" class="section level4 unnumbered">
<h4>Prediction by simple linear regression</h4>
<p>Let <span class="math inline">\(\hat{a}\)</span> be the estimated y-intercept of a linear regression, and <span class="math inline">\(\hat{b}\)</span> be its estimated slope. The prediction of <span class="math inline">\(\hat{Y}\)</span> the response variable may be written in two different ways:
<span class="math display" id="eq:Yind" id="eq:Ypeup">\[\begin{eqnarray}
\hat{Y}&amp;=&amp;\hat{a}+\hat{b}X\tag{7.1}\\ %
\hat{Y}&amp;=&amp;\hat{a}+\hat{b}X+\varepsilon\tag{7.2}
\end{eqnarray}\]</span>
In both cases the expectation of <span class="math inline">\(\hat{Y}\)</span> is the same as <span class="math inline">\(\mathrm{E}(\varepsilon)=0\)</span>. By contrast, the variance of <span class="math inline">\(\hat{Y}\)</span>
is not the same: it is higher in the second than in the first. These two equations may be interpreted as follows. Let us assume that effect variable <span class="math inline">\(X\)</span> is dbh and that the response variable <span class="math inline">\(Y\)</span> is the biomass. The number of trees in the whole forest with a given dbh <span class="math inline">\(X\)</span> (to within measurements errors) is immeasurably huge. If we were able to measure the biomass of all these trees with the same dbh, we would obtain variable values oscillating around a certain mean value. If we seek to predict this mean biomass (i.e. the mean of all the trees with dbh <span class="math inline">\(X\)</span>), then equation <a href="7-util.html#eq:Ypeup">(7.1)</a> of the prediction is valid. By contrast, if we seek to predict the biomass of a tree selected at random from all the trees with dbh <span class="math inline">\(X\)</span>, then equation <a href="7-util.html#eq:Yind">(7.2)</a> of the prediction is valid. The variability of the prediction is greater for <a href="7-util.html#eq:Yind">(7.2)</a> than for <a href="7-util.html#eq:Ypeup">(7.1)</a> because here the variability of the mean biomass prediction is supplemented by the differences in biomass between the trees.</p>
<p>This means that two methods can be used to calculate a confidence interval for a prediction. There is a confidence interval for the prediction of the mean of <span class="math inline">\(Y\)</span>, and a confidence interval for the prediction of an individual selected at random from the population in which the mean of <span class="math inline">\(Y\)</span> was calculated. The second confidence interval is wider than the first.</p>
<p>In the case of a simple linear regression, it can be shown <span class="citation">(Saporta <a href="bibliography.html#ref-saporta90" role="doc-biblioref">1990</a>, 373–74)</span> that the confidence interval at a significance level of <span class="math inline">\(\alpha\)</span> for predicting the mean <a href="7-util.html#eq:Ypeup">(7.1)</a> is:
<span class="math display" id="eq:icpeup">\[\begin{equation}
\hat{a}+\hat{b}X\pm
t_{n-2}\ \hat{\sigma}\sqrt{\frac{1}{n}+\frac{(X-\bar{X})^2}{nS_X^2}}
\tag{7.3}
\end{equation}\]</span>
while the confidence interval at a significance level of <span class="math inline">\(\alpha\)</span> for predicting a tree selected at random <a href="7-util.html#eq:Yind">(7.2)</a> is:
<span class="math display" id="eq:icind">\[\begin{equation}
\hat{a}+\hat{b}X\pm
t_{n-2}\ \hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(X-\bar{X})^2}{nS_X^2}}
\tag{7.4}
\end{equation}\]</span>
where <span class="math inline">\(t_{n-2}\)</span> is the quantile <span class="math inline">\(1-\alpha/2\)</span> of a Student’s distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, <span class="math inline">\(\bar{X}=(\sum_{i=1}^nX_i)/n\)</span> is the mean of the observed values of <span class="math inline">\(X\)</span> in the dataset that served to fit the model, and <span class="math inline">\(S_X^2=[\sum_{i=1}^n(X_i-\bar{X})^2]/n\)</span> is the empirical variance of the observed values of <span class="math inline">\(X\)</span> in the dataset that served to fit the model.</p>
<p>These expressions call for several remarks. The first is that the difference between the bounds of confidence interval <a href="7-util.html#eq:icind">(7.4)</a> for a tree selected at random and the bounds of confidence interval <a href="7-util.html#eq:icpeup">(7.3)</a> for the mean tree is approximately <span class="math inline">\(t_{n-2}\hat{\sigma}\)</span>. This difference reflects the difference between equations <a href="7-util.html#eq:Yind">(7.2)</a> and <a href="7-util.html#eq:Ypeup">(7.1)</a>, which is due to the residual term <span class="math inline">\(\varepsilon\)</span> whose estimated standard deviation is <span class="math inline">\(\hat{\sigma}\)</span>.</p>
<p>The second is that the width of the confidence interval is not constant, but varies with <span class="math inline">\(X\)</span>. The confidence interval is narrowest when <span class="math inline">\(X=\bar{X}\)</span> but widens as <span class="math inline">\(X\)</span> moves away from <span class="math inline">\(\bar{X}\)</span>.</p>
<p>The third remark is that in order to calculate the confidence interval of a prediction based on a linear regression, we must be in possession of — if not the original data that served to fit the model — at least the mean <span class="math inline">\(\bar{X}\)</span> of the effect variable and its empirical standard deviation <span class="math inline">\(S_X\)</span>. If the original data that served to fit the model are not available, and the values of <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S_X\)</span> have not been documented, we cannot accurately calculate the confidence interval.</p>
<div class="filrouge">
<ol start="33" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:ficlnD" class="exercise"><strong>Red line 7.1  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Confidence interval of <span class="math inline">\(\ln(B)\)</span> predicted by <span class="math inline">\(\ln(D)\)</span></strong></span></p>
</div>
<p>Let us return to the simple linear regression between <span class="math inline">\(\ln(B)\)</span> and <span class="math inline">\(\ln(D)\)</span> that was fitted in red line <a href="6-fit.html#exr:rllnBvD">6.1</a>. Let
<code>m</code> be the object containing the fitted model (see red line <a href="6-fit.html#exr:rllnBvD">6.1</a>). The confidence intervals may be calculated using the <code>predict</code> command. For example, for a tree with a dbh of 20~cm, the 95 % confidence interval for the mean tree is obtained by the command:</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb102-1"><a href="7-util.html#cb102-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,]) <span class="co">## red line 7</span></span>
<span id="cb102-2"><a href="7-util.html#cb102-2"></a><span class="kw">predict</span>(m, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> <span class="dv">20</span>), <span class="dt">interval =</span> <span class="st">&quot;confidence&quot;</span>, <span class="dt">level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##         fit       lwr       upr
## 1 -1.354183 -1.533487 -1.174879</code></pre>
<p>Thus, the fitted model is <span class="math inline">\(\ln(B)=\)</span> -1.3541832 with a 95 % confidence interval of -1.5334873 to -1.174879. For a tree with a dbh of 20 cm selected at random, the confidence interval may be obtained by the command:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb104-1"><a href="7-util.html#cb104-1"></a><span class="kw">predict</span>(</span>
<span id="cb104-2"><a href="7-util.html#cb104-2"></a>    m, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> <span class="dv">20</span>), <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">level =</span> <span class="fl">0.95</span></span>
<span id="cb104-3"><a href="7-util.html#cb104-3"></a>    )</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##         fit       lwr        upr
## 1 -1.354183 -2.305672 -0.4026948</code></pre>
<p>Figure <a href="7-util.html#fig:icD">7.1</a> shows the confidence intervals for the entire range of the data.</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:icD"></span>
<img src="source/figures/iclnD.png" alt="Biomass against dbh (on a log scale) for 42 trees measured in Ghana by Henry et al. (2010) (points), prediction (solid line) of the simple linear regression of \(\ln(B)\) against \(\ln(D)\), and confidence intervals of this prediction for a tree selected at random (green line) and for the mean tree (red line)." width="40%" />
<p class="caption">
Figure 7.1: Biomass against dbh (on a log scale) for 42 trees measured in Ghana by <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry10" role="doc-biblioref">2010</a>)</span> (points), prediction (solid line) of the simple linear regression of <span class="math inline">\(\ln(B)\)</span> against <span class="math inline">\(\ln(D)\)</span>, and confidence intervals of this prediction for a tree selected at random (green line) and for the mean tree (red line).
</p>
</div>
</div>
<div id="prediction-by-multiple-regression" class="section level4 unnumbered">
<h4>Prediction by multiple regression</h4>
<p>The principles laid down in the case of the linear regression also apply to the multiple regression. The confidence interval can be expressed in two different ways: one for the prediction of the mean tree, the other for the prediction of a tree selected at random.</p>
<p>In the case of a multiple regression with estimated coefficients <span class="math inline">\(\hat{\mathbf{a}}={}^{\mathrm{t}}[\hat{a}_0,\ \hat{a}_1,\ \hat{a}_2,\ \ldots,\ \hat{a}_p]\)</span>, the predicted value <span class="math inline">\(\hat{Y}\)</span> of the response variable for a tree whose effect variables are <span class="math inline">\(\mathbf{x}={}^{\mathrm{t}}[1,\ X_1,\ X_2,\ \ldots, X_p]\)</span>, is:
<span class="math display">\[
\hat{Y}={}^{\mathrm{t}}{\mathbf{x}}\ \hat{\mathbf{a}}
\]</span>
and the confidence interval (with a significance level of <span class="math inline">\(\alpha\)</span>) of this prediction is <span class="citation">(Saporta <a href="bibliography.html#ref-saporta90" role="doc-biblioref">1990</a>, 387)</span>:</p>
<ul>
<li><p>for the prediction of the mean tree:
<span class="math display" id="eq:iclm0">\[\begin{equation}
{}^{\mathrm{t}}{\mathbf{x}}\ \hat{\mathbf{a}}\pm t_{n-p-1}\ \hat{\sigma}
\sqrt{{}^{\mathrm{t}}{\mathbf{x}}({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\mathbf{x}}
\tag{7.5}
\end{equation}\]</span></p></li>
<li><p>for the prediction of a tree selected at random:
<span class="math display" id="eq:iclm1">\[\begin{equation}
{}^{\mathrm{t}}{\mathbf{x}}\ \hat{\mathbf{a}}\pm t_{n-p-1}\ \hat{\sigma}
\sqrt{1+{}^{\mathrm{t}}{\mathbf{x}}({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\mathbf{x}}
\tag{7.6}
\end{equation}\]</span></p></li>
</ul>
<p>where <span class="math inline">\(\mathbf{X}\)</span> is is the design matrix constructed using the data that served to fit the multiple regression. In order to calculate the confidence interval of the predictions, we need to be in possession of — if not the original data that served to fit the model — at least the matrix <span class="math inline">\(({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\)</span>. It should be noted that the variance of the prediction in the case <a href="7-util.html#eq:iclm1">(7.6)</a> of a tree selected at random is made up of two terms: a term <span class="math inline">\(\hat{\sigma}^2\)</span> representing the residual error and a term <span class="math inline">\(\hat{\sigma}^2\;{}^{\mathrm{t}}{\mathbf{x}}({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\mathbf{x}\)</span> representing the variability induced by the estimation of the model’s coefficients. When estimating the mean tree, the first term disappears, only the second remains.</p>
<div class="filrouge">
<ol start="34" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:ficlnDH" class="exercise"><strong>Red line 7.2  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Confidence interval of <span class="math inline">\(\ln(B)\)</span> predicted by <span class="math inline">\(\ln(D)\)</span> and <span class="math inline">\(\ln(H)\)</span></strong></span></p>
</div>
<p>Let us return to the multiple linear regression between <span class="math inline">\(\ln(B)\)</span>, <span class="math inline">\(\ln(D)\)</span> and <span class="math inline">\(\ln(H)\)</span> that was fitted in red line <a href="6-fit.html#exr:flnDlnH">6.4</a>. Let <code>m</code> be the object containing the fitted model (see red line <a href="6-fit.html#exr:flnDlnH">6.4</a>). The confidence intervals may be calculated using the <code>predict</code> command. For example, for a tree with a dbh of 20~cm and a height of 20 m, the 95 % confidence interval for the mean tree is obtained by the command:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb106-1"><a href="7-util.html#cb106-1"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(heig)), <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,])</span>
<span id="cb106-2"><a href="7-util.html#cb106-2"></a><span class="kw">predict</span>(</span>
<span id="cb106-3"><a href="7-util.html#cb106-3"></a>    m, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> <span class="dv">20</span>, <span class="dt">heig =</span> <span class="dv">20</span>), </span>
<span id="cb106-4"><a href="7-util.html#cb106-4"></a>    <span class="dt">interval =</span> <span class="st">&quot;confidence&quot;</span>, <span class="dt">level =</span> <span class="fl">0.95</span></span>
<span id="cb106-5"><a href="7-util.html#cb106-5"></a>    )</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##         fit       lwr       upr
## 1 -1.195004 -1.380798 -1.009211</code></pre>
<p>Thus, the model predicts <span class="math inline">\(\ln(B)=\)</span> -1.1950044 with a 95 % confidence interval of -1.3807983 to -1.0092105. For a tree with a dbh of 20 cm and a height of 20 m selected at random, the confidence interval is obtained by the command:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb108-1"><a href="7-util.html#cb108-1"></a><span class="kw">predict</span>(</span>
<span id="cb108-2"><a href="7-util.html#cb108-2"></a>    m, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">dbh =</span> <span class="dv">20</span>, <span class="dt">heig =</span> <span class="dv">20</span>), </span>
<span id="cb108-3"><a href="7-util.html#cb108-3"></a>    <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">level =</span> <span class="fl">0.95</span></span>
<span id="cb108-4"><a href="7-util.html#cb108-4"></a>    )</span></code></pre></div>
<p>which yields:</p>
<pre class="Rout"><code>##         fit       lwr        upr
## 1 -1.195004 -2.046408 -0.3436006</code></pre>
</div>
</div>
</div>
<div id="pnls" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Prediction: case of a non-linear model</h3>
<p>In the general case of a non-linear model as defined by
<span class="math display">\[
Y=f(X_1,\ \ldots,\ X_p;\theta)+\varepsilon
\]</span>
where
<span class="math display">\[
\varepsilon\mathop{\sim}\mathcal{N}(0,\;kX_1^c)
\]</span>
unlike the case of the linear model, there is no exact explicit expression for the confidence intervals of the predictions. But the <span class="math inline">\(\delta\)</span>-method can be used to obtain an approximate (and asymptotically exact) expression of these confidence intervals <span class="citation">(Serfling <a href="bibliography.html#ref-serfling80" role="doc-biblioref">1980</a>)</span>. As before, there are two confidence intervals:</p>
<ul>
<li><p>a confidence interval for the prediction of the mean tree:
<span class="math display" id="eq:icnl0">\[\begin{equation}
f(X_1,\ \ldots,\ X_p;\hat{\theta})\pm t_{n-q}
\sqrt{{}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\;
[\mathrm{d}_{\theta}f(\hat{\theta})]}\tag{7.7}
\end{equation}\]</span></p></li>
<li><p>a confidence interval for the prediction of a tree selected at random:
<span class="math display" id="eq:icnl1">\[\begin{equation}
f(X_1,\ \ldots,\ X_p;\hat{\theta})\pm t_{n-q}
\sqrt{\hat{k}^2X_1^{2\hat{c}}+
{}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\;
[\mathrm{d}_{\theta}f(\hat{\theta})]}\tag{7.8}
\end{equation}\]</span></p></li>
</ul>
<p>where <span class="math inline">\(q\)</span> is the number of coefficients in the model (i.e. the length of vector <span class="math inline">\(\theta\)</span>), <span class="math inline">\(\mathrm{d}_{\theta}f(\hat{\theta})\)</span> is the value in <span class="math inline">\(\theta=\hat{\theta}\)</span> of the differential of <span class="math inline">\(f\)</span> respect to model coefficients, and <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\)</span> is an estimation in <span class="math inline">\(\theta=\hat{\theta}\)</span> of the variance-covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}_{\theta}\)</span> of the estimator of <span class="math inline">\(\theta\)</span>. The differential of <span class="math inline">\(f\)</span> with respect to model coefficients is the vector of length <span class="math inline">\(q\)</span>:
<span class="math display">\[
\mathrm{d}_{\theta}f(\theta)={}^{\mathrm{t}}{\bigg[\bigg(\frac{\partial
f(X_1,\ \ldots,\ X_p;\theta)}{\partial\theta_1}\bigg),\; \ldots,\;
\bigg(\frac{\partial
f(X_1,\ \ldots,\ X_p;\theta)}{\partial\theta_q}\bigg)\bigg]}
\]</span>
where <span class="math inline">\(\theta_i\)</span> is the <span class="math inline">\(i\)</span>th element of vector <span class="math inline">\(\theta\)</span>. If using the estimator of the maximum likelihood of <span class="math inline">\(\theta\)</span>, we can show that asymptotically, when <span class="math inline">\(n\rightarrow\infty\)</span> <span class="citation">(Saporta <a href="bibliography.html#ref-saporta90" role="doc-biblioref">1990</a>, 301)</span>:
<span class="math display">\[
\boldsymbol{\Sigma}_{\theta}\ \mathop{\sim}_{n\rightarrow\infty}\
\mathbf{I}_n(\theta)^{-1}=\frac{1}{n}\; \mathbf{I}_1(\theta)^{-1}
\]</span>
where <span class="math inline">\(\mathbf{I}_n(\theta)\)</span> is the Fisher information matrix provided by a sample of size <span class="math inline">\(n\)</span> on the vector of parameters <span class="math inline">\(\theta\)</span>. This Fisher information matrix has <span class="math inline">\(q\)</span> lines and <span class="math inline">\(q\)</span> columns and is calculated from the second derivative of the log-likelihood of the sample:
<span class="math display">\[
\mathbf{I}_n(\theta)=-\mathrm{E}\bigg[\frac{\partial^2\mathcal{L}(\theta)}
{\partial\theta^2}\bigg]
\]</span>
An approximate estimation of the variance-covariance matrix of the parameters is therefore:
<span class="math display">\[
\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}=-\bigg[\bigg(\frac{\partial^2\mathcal{L}(\theta)}
{\partial\theta^2}\bigg)\bigg|_{\theta=\hat{\theta}}\bigg]^{-1}
\]</span>
In practice, the algorithm that numerically optimizes the log-likelihood of the sample at the same time gives a numerical estimation of the second derivative <span class="math inline">\((\partial^2\mathcal{L}/\partial\theta^2)\)</span>. This provides an immediate numerical estimation of <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\)</span>.</p>
<p>As before, the variance of the predictions in the case <a href="7-util.html#eq:icnl1">(7.8)</a> of a tree selected at random is made up of two terms: a term <span class="math inline">\((\hat{k}X_1^{\hat{c}})^2\)</span> representing the residual error and a term
<span class="math inline">\({}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\;[\mathrm{d}_{\theta}f(\hat{\theta})]\)</span> representing the variability induced by the estimation of the model’s coefficients. When estimating the mean tree, the first term disappears, only the second remains.</p>
</div>
<div id="papp" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Approximated confidence intervals</h3>
<p>The exact calculation of prediction confidence intervals requires information (matrix <span class="math inline">\(\mathbf{X}\)</span> for a linear model, variance-covariance matrix <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\)</span> for a non-linear model) that is only rarely given in papers on volume or biomass tables. Generally, these papers provide only the number <span class="math inline">\(n\)</span> of observations used to fit the model and the residual standard deviation <span class="math inline">\(\hat{\sigma}\)</span> (linear model) or <span class="math inline">\(\hat{k}\)</span> and <span class="math inline">\(\hat{c}\)</span> (non-linear model). Sometimes, even this basic information about the fitting is missing. If <span class="math inline">\(\mathbf{X}\)</span> (linear model) or
<span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{\hat{\theta}}\)</span> (non-linear model) is not provided, it is impossible to use the formulas given above to calculate confidence intervals. In this case we must use an approximate method.</p>
<div id="residual-error-alone" class="section level4 unnumbered">
<h4>Residual error alone</h4>
<p>Very often, only the residual standard deviation <span class="math inline">\(\hat{\sigma}\)</span> (linear model) or <span class="math inline">\(\hat{k}\)</span> and <span class="math inline">\(\hat{c}\)</span> (non-linear model) is given. In this case, an approximate confidence interval with a significance level of <span class="math inline">\(\alpha\)</span> may be determined:</p>
<ul>
<li><p>in the case of a linear regression:
<span class="math display" id="eq:iclm2">\[\begin{equation}
(a_0+a_1X_1+\ldots+a_pX_p)\pm q_{1-\alpha/2}\ \hat{\sigma}
\tag{7.9}
\end{equation}\]</span></p></li>
<li><p>in the case of a non-linear model:
<span class="math display" id="eq:icnl2">\[\begin{equation}
f(X_1,\ \ldots,\ X_p;\theta)\pm
q_{1-\alpha/2}\ \hat{k}X_1^{\hat{c}} \tag{7.10}
\end{equation}\]</span></p></li>
</ul>
<p>where <span class="math inline">\(q_{1-\alpha/2}\)</span> is the quantile <span class="math inline">\(1-\alpha/2\)</span> of the standard normal distribution. This confidence interval is a direct retranscription of the relation <span class="math inline">\(Y=a_0+a_1X_1+\ldots+a_pX_p+\varepsilon\)</span> with <span class="math inline">\(\varepsilon\sim\mathcal{N}(0,\ \hat{\sigma})\)</span> (linear case) or <span class="math inline">\(Y=f(X_1,\ \ldots,\ X_p;\theta)+\varepsilon\)</span> with <span class="math inline">\(\varepsilon\sim\mathcal{N}(0,\ \hat{k}X_1^{\hat{c}})\)</span> (non-linear case), where we have deliberately written the model’s coefficients without a circumflex to underline that these are fixed values. These relations therefore assume implicatively that the model’s coefficients are exactly known and that the only source of variability is the residual error. In other words, these approximate confidence intervals may be interpreted as follows: these confidence intervals <a href="7-util.html#eq:iclm2">(7.9)</a> (linear case) and <a href="7-util.html#eq:icnl2">(7.10)</a> (non-linear case) are those that would be obtained for the prediction of a tree selected at <em>random</em> if sample size was <em>infinite</em>. This is confirmed when <span class="math inline">\(n\rightarrow\infty\)</span>, <span class="math inline">\(t_{n-p-1}\)</span> tends toward <span class="math inline">\(q_{1-\alpha/2}\)</span> and the matrix <span class="math inline">\(({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\)</span> in <a href="7-util.html#eq:iclm1">(7.6)</a> tends toward the null matrix (whose coefficients are all zero). Thus, confidence interval <a href="7-util.html#eq:iclm2">(7.9)</a> is indeed the bound of confidence interval <a href="7-util.html#eq:iclm1">(7.6)</a> when <span class="math inline">\(n\rightarrow\infty\)</span>. The same may be said of <a href="7-util.html#eq:icnl1">(7.8)</a> and <a href="7-util.html#eq:icnl2">(7.10)</a>.</p>
</div>
<div id="confidence-interval-for-the-mean-tree" class="section level4 unnumbered">
<h4>Confidence interval for the mean tree</h4>
<p>If an estimation <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span> is given of the variance-covariance matrix of the parameters, a confidence interval at a significance level of <span class="math inline">\(\alpha\)</span> for the prediction of the mean tree corresponds to:</p>
<ul>
<li><p>for a linear model:
<span class="math display" id="eq:iclm3">\[\begin{equation}
(\hat{a}_0+\hat{a}_1X_1+\ldots+\hat{a}_pX_p)\pm
q_{1-\alpha/2}\sqrt{{}^{\mathrm{t}}{\mathbf{x}}\hat{\boldsymbol{\Sigma}}\mathbf{x}}
\tag{7.11}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{x}\)</span> is the vector <span class="math inline">\({}^{\mathrm{t}}{[X_1,\ \ldots,\ X_p]}\)</span>,</p></li>
<li><p>for a non-linear model:
<span class="math display" id="eq:icnl3">\[\begin{equation}
f(X_1,\ \ldots,\ X_p;\hat{\theta})\pm q_{1-\alpha/2}
\sqrt{{}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;\hat{\boldsymbol{\Sigma}}\;
[\mathrm{d}_{\theta}f(\hat{\theta})]}\tag{7.12}
\end{equation}\]</span></p></li>
</ul>
<p>These confidence intervals consider that all the prediction’s variability stems from the estimation of the model’s coefficients. Also, these confidence intervals are a direct retranscription of the fact that the model’s coefficients follow a multinormal distribution whose mean is their true value and that has a variance-covariance matrix <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span> for in the linear case, if <span class="math inline">\(\hat{\mathbf{a}}={}^{\mathrm{t}}{[\hat{a}_1,\ \ldots,\ \hat{a}_p]}\)</span> follows a multinormal distribution of mean <span class="math inline">\({}^{\mathrm{t}}{[a_1,\ \ldots,\ a_p]}\)</span> and variance-covariance matrix <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span>, then the linear combination <span class="math inline">\({}^{\mathrm{t}}{\mathbf{x}}\ \hat{\mathbf{a}}\)</span> follows a normal distribution of mean <span class="math inline">\({}^{\mathrm{t}}{\mathbf{x}}\ \mathbf{a}\)</span> and variance <span class="math inline">\({}^{\mathrm{t}}{\mathbf{x}}\hat{\boldsymbol{\Sigma}}\mathbf{x}\)</span> <span class="citation">(Saporta <a href="bibliography.html#ref-saporta90" role="doc-biblioref">1990</a>, 85)</span>.</p>
<p>In the case of a linear model, we can show that the variance-covariance matrix of the estimator of the model’s coefficients is <span class="citation">(Saporta <a href="bibliography.html#ref-saporta90" role="doc-biblioref">1990</a>, 380)</span>: <span class="math inline">\(\boldsymbol{\Sigma}=\sigma^2({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\)</span>. Thus, an estimation of this variance-covariance matrix is: <span class="math inline">\(\hat{\boldsymbol{\Sigma}}=\hat{\sigma}^2({}^{\mathrm{t}}{\mathbf{X}}\mathbf{X})^{-1}\)</span>. By integrating this expression in <a href="7-util.html#eq:iclm3">(7.11)</a>, we create an expression similar to <a href="7-util.html#eq:iclm0">(7.5)</a>. Similarly, in the non-linear case, confidence interval <a href="7-util.html#eq:icnl3">(7.12)</a> is an approximation of <a href="7-util.html#eq:icnl0">(7.7)</a>.</p>
<p>In the non-linear case <a href="7-util.html#eq:icnl3">(7.12)</a>, if we want to avoid calculating the partial derivatives of <span class="math inline">\(f\)</span>, we could use the Monte Carlo method. This method is based on a simulation that consists in making <span class="math inline">\(Q\)</span> samplings of coefficients <span class="math inline">\(\theta\)</span> in accordance with a multinormal distribution of mean <span class="math inline">\(\hat{\theta}\)</span> and variance-covariance matrix <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span>, calculating the prediction for each of the simulated values, then calculating the empirical confidence interval of these <span class="math inline">\(Q\)</span> predictions. This method is known in the literature as providing “population prediction intervals” <span class="citation">(Bolker <a href="bibliography.html#ref-bolker08" role="doc-biblioref">2008</a>; Paine et al. <a href="bibliography.html#ref-paine12" role="doc-biblioref">2012</a>)</span>. The pseudo-algorithm is as follows:</p>
<ol style="list-style-type: decimal">
<li>For <span class="math inline">\(k\)</span> of <span class="math inline">\(1\)</span> to <span class="math inline">\(Q\)</span>:
a. draw a vector <span class="math inline">\(\hat{\theta}^{(k)}\)</span> following a multinormal distribution of mean <span class="math inline">\(\hat{\theta}\)</span> and variance-covariance matrix <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span>;
b. calculate the prediction <span class="math inline">\(\hat{Y}^{(k)}=f(X_1,\ \ldots,\ X_p;\hat{\theta}^{(k)})\)</span>.</li>
<li>The confidence interval of the prediction is the empirical confidence interval of the <span class="math inline">\(Q\)</span> values <span class="math inline">\(\hat{Y}^{(1)},\ \ldots,\ \hat{Y}^{(Q)}\)</span>.</li>
</ol>
<p>Very often we do not know the variance-covariance matrix <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span>, but we do have an estimation of the standard deviations of the coefficients. Let <span class="math inline">\(\mathrm{Var}(\hat{a}_i)=\Sigma_i\)</span> (linear case) or <span class="math inline">\(\mathrm{Var}(\hat{\theta}_i)=\Sigma_i\)</span> (non-linear case) be the variance of the model’s <span class="math inline">\(i\)</span>th coefficient. In this case we will ignore the correlation between the model’s coefficients and will approximate the variance-covariance matrix of the coefficients by a diagonal matrix:
<span class="math display">\[
\hat{\boldsymbol{\Sigma}}\approx\left[
\begin{array}{ccc}
\hat{\Sigma}_1 &amp;        &amp; \mathbf{0}\\ %
               &amp; \ddots &amp; \\ %
\mathbf{0}     &amp;        &amp; \hat{\Sigma}_p\\ %
\end{array}
\right]
\]</span></p>
</div>
<div id="confidence-interval-for-a-tree-selected-at-random" class="section level4 unnumbered">
<h4>Confidence interval for a tree selected at random</h4>
<p>The error resulting from the estimation of the model’s coefficients, as described in the last section, can be cumulated with the residual error described in the section before last, in order to construct a prediction confidence interval for a tree selected at random. These are the variances of the predictions that are added one to the other. The confidence interval with a significance level of <span class="math inline">\(\alpha\)</span> is therefore:</p>
<ul>
<li><p>for a linear model:
<span class="math display">\[
(\hat{a}_0+\hat{a}_1X_1+\ldots+\hat{a}_pX_p)\pm
q_{1-\alpha/2}\sqrt{\hat{\sigma}^2+{}^{\mathrm{t}}{\mathbf{x}}\hat{\boldsymbol{\Sigma}}\mathbf{x}}
\]</span>
which is an approximation of <a href="7-util.html#eq:iclm1">(7.6)</a>,</p></li>
<li><p>for a non-linear model:
<span class="math display">\[
f(X_1,\ \ldots,\ X_p;\hat{\theta})\pm q_{1-\alpha/2}
\sqrt{\hat{k}^2X_1^{2\hat{c}}+{}^{\mathrm{t}}{[\mathrm{d}_{\theta}f(\hat{\theta})]}\;
\hat{\boldsymbol{\Sigma}}\; [\mathrm{d}_{\theta}f(\hat{\theta})]}
\]</span>
which is an approximation of <a href="7-util.html#eq:icnl1">(7.8)</a>.</p></li>
</ul>
<p>As before, if we want to avoid a great many calculations, we could employ the Monte Carlo method, using the following pseudo-algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>For <span class="math inline">\(k\)</span> of <span class="math inline">\(1\)</span> to <span class="math inline">\(Q\)</span>:
a. draw a vector <span class="math inline">\(\hat{\theta}^{(k)}\)</span> following a multinormal distribution of mean <span class="math inline">\(\hat{\theta}\)</span> and variance-covariance matrix <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span>;
a. draw a residual <span class="math inline">\(\hat{\varepsilon}^{(k)}\)</span> following a centered normal distribution of standard deviation <span class="math inline">\(\hat{\sigma}\)</span> (linear case) or <span class="math inline">\(\hat{k}X_1^{\hat{c}}\)</span> (non-linear case);
a. calculate the prediction <span class="math inline">\(\hat{Y}^{(k)}=f(X_1,\ \ldots,\ X_p;\hat{\theta}^{(k)})+\hat{\varepsilon}^{(k)}\)</span>.</p></li>
<li><p>The confidence interval of the prediction is the empirical confidence interval of the <span class="math inline">\(Q\)</span> values <span class="math inline">\(\hat{Y}^{(1)},\ \ldots,\ \hat{Y}^{(Q)}\)</span>.</p></li>
</ol>
</div>
<div id="errmes" class="section level4 unnumbered">
<h4>Confidence interval with measurement uncertainties</h4>
<p>Fitting volume and biomass models assumes that the effect variables <span class="math inline">\(X_1,\ \ldots,\ X_p\)</span> are known exactly. In actual fact, this hypothesis is only an approximation as these variables are measured and are therefore subject to measurement error. Warning: measurement error should not be confused with the residual error of the response variable: the first is related to the measuring instrument and in principle may be rendered as small as we wish by using increasingly precise measuring instruments. The second reflects the intrinsic biological variability between individuals. We can take account of the impact of measurement error on the prediction by including it in the prediction confidence interval. Thus, the effect variables <span class="math inline">\(X_1,\ \ldots,\ X_p\)</span>
are no longer considered as fixed, but as being contained within a distribution. Typically, to predict the volume or biomass of a tree with characteristics <span class="math inline">\(X_1,\ \ldots,\ X_p\)</span>, we will consider that the <span class="math inline">\(i\)</span>th characteristic is distributed according to a normal distribution of mean <span class="math inline">\(X_i\)</span> and standard deviation <span class="math inline">\(\tau_i\)</span>. Typically, if <span class="math inline">\(X_i\)</span> is a dbh, then <span class="math inline">\(\tau_i\)</span> will be 3–5~mm; if <span class="math inline">\(X_i\)</span> is a height, <span class="math inline">\(\tau_i\)</span> will be 3 % of <span class="math inline">\(X_i\)</span> for <span class="math inline">\(X_i\leq15\)</span>~m and 1~m for <span class="math inline">\(X_i&gt;15\)</span>~m.</p>
<p>It is difficult to calculate an explicit expression for the prediction confidence interval when the effect variables are considered to be random since this means we must calculate the variances of the products of these random variables, some of which are correlated one with the other. The <span class="math inline">\(\delta\)</span>-method offers an approximate analytical solution <span class="citation">(Serfling <a href="bibliography.html#ref-serfling80" role="doc-biblioref">1980</a>)</span>. Or, more simply, we can again use a Monte Carlo method. In this case, the pseudo-algorithm becomes:</p>
<ol style="list-style-type: decimal">
<li><p>For <span class="math inline">\(k\)</span> of <span class="math inline">\(1\)</span> to <span class="math inline">\(Q\)</span>:
a. for <span class="math inline">\(i\)</span> of 1 to <span class="math inline">\(p\)</span>, draw <span class="math inline">\(\hat{X}_i^{(k)}\)</span> following a normal distribution of mean <span class="math inline">\(X_i\)</span> and standard deviation <span class="math inline">\(\tau_i\)</span>;
a. draw a vector <span class="math inline">\(\hat{\theta}^{(k)}\)</span> following a multinormal distribution of mean <span class="math inline">\(\hat{\theta}\)</span> and variance-covariance matrix <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span>;
a. draw a residual <span class="math inline">\(\hat{\varepsilon}^{(k)}\)</span> following a centered normal distribution of standard deviation <span class="math inline">\(\hat{\sigma}\)</span> (linear case) or <span class="math inline">\(\hat{k}X_1^{\hat{c}}\)</span> (non-linear case);
a. calculate the prediction <span class="math inline">\(\hat{Y}^{(k)}=f(\hat{X}_1^{(k)},\ \ldots,\ \hat{X}_p^{(k)}; \hat{\theta}^{(k)})+\hat{\varepsilon}^{(k)}\)</span>.</p></li>
<li><p>The confidence interval of the prediction is the empirical confidence interval of the <span class="math inline">\(Q\)</span> values <span class="math inline">\(\hat{Y}^{(1)},\ \ldots,\ \hat{Y}^{(Q)}\)</span>.</p></li>
</ol>
<p>This confidence interval corresponds to the prediction for a tree selected at random. To obtain the confidence interval for the mean tree, simply apply the same pseudo-algorithm but replaced step (c) by:</p>
<ol style="list-style-type: decimal">
<li> 
a. (<span class="math inline">\(\ldots\)</span>)
a. (<span class="math inline">\(\ldots\)</span>)
a. with <span class="math inline">\(\hat{\varepsilon}^{(k)}=0\)</span>;
a. (<span class="math inline">\(\ldots\)</span>)</li>
</ol>
</div>
</div>
<div id="invtra" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Inverse variables transformation</h3>
<p>We saw in section <a href="6-fit.html#trans">6.1.5</a> how a variable transformation can render linear a model that initially did not satisfy the hypotheses of the linear model. Variable transformation affects both the mean and the residual error. The same may be said of the inverse transformation, with implications on the calculation of prediction expectation. The log transformation is that most commonly used, but other types of transformation are also available.</p>
<div id="log-transformation" class="section level4 unnumbered">
<h4>Log transformation</h4>
<p>Let us first consider the case of the log transformation of volume or biomass values, which is by far the most common case for volume and biomass models. Let us assume that a log transformation has been applied to biomass <span class="math inline">\(B\)</span> to fit a linear model using effect variables <span class="math inline">\(X_1,\ \ldots,\ X_p\)</span>:
<span class="math display" id="eq:pre">\[\begin{equation}
Y=\ln(B)=a_0+a_1X_1+\ldots+a_pX_p+\varepsilon\tag{7.13}
\end{equation}\]</span>
where
<span class="math display">\[
\varepsilon\mathop{\sim}_{\mathrm{i.i.d.}}\mathcal{N}(0,\ \sigma)
\]</span>
This is equivalent to saying that <span class="math inline">\(\ln(B)\)</span> follows a normal distribution of mean <span class="math inline">\(a_0+a_1X_1+\ldots+a_pX_p\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> or of saying, by definition, that <span class="math inline">\(B\)</span> follows a log-normal distribution of parameters <span class="math inline">\(a_0+a_1X_1+\ldots+a_pX_p\)</span> and <span class="math inline">\(\sigma\)</span>. The expectation of this log-normal distribution is:
<span class="math display">\[
\mbox{E}(B)=\exp\Big(a_0+a_1X_1+\ldots+a_pX_p+\frac{\sigma^2}{2}\Big)
\]</span>
Compared to the inverse model of <a href="7-util.html#eq:pre">(7.13)</a> which corresponds to <span class="math inline">\(B=\exp(a_0+a_1X_1+\ldots+a_pX_p)\)</span>, the inverse transformation of the residual error introduces bias into the prediction which can be corrected by multiplying the prediction
<span class="math inline">\(\exp(a_0+a_1X_1+\ldots+a_pX_p)\)</span> by an correction factor <span class="citation">(Parresol <a href="bibliography.html#ref-parresol99" role="doc-biblioref">1999</a>)</span>:
<span class="math display" id="eq:CF">\[\begin{equation}
\mathrm{CF}=\exp\Big(\frac{\hat{\sigma}^2}{2}\Big)\tag{7.14}
\end{equation}\]</span>
We must however take care as the biomass models reported in the literature, and fitted after log transformation of the biomass, sometimes include this factor and sometimes do not.</p>
<p>If the decimal <span class="math inline">\(\log_{10}\)</span> has been used for variable transformation rather than the natural log, the correction factor is:
<span class="math display">\[
\mathrm{CF}=\exp\Big[\frac{(\hat{\sigma}\ln 10)^2}{2}\Big]\approx
\exp\Big(\frac{\hat{\sigma}^2}{0.3772}\Big)
\]</span></p>
<div class="filrouge">
<ol start="35" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:CFB" class="exercise"><strong>Red line 7.3  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> Correction factor for predicted biomass</strong></span></p>
</div>
<p>Let us return to the biomass model in red line <a href="6-fit.html#exr:fdens">6.25</a> fitted by multiple regression on log-transformed data:
<span class="math display">\[
\ln(B)=-8.38900+0.85715\ln(D^2H)+0.72864\ln(\rho)
\]</span>
If we consider the starting data but use the exponential function (without taking account of the correction factor), we obtain an under-estimated prediction: <span class="math inline">\(B=\exp(-8.38900)\times(D^H)^{0.85715}\rho^{0.72864}=2.274\times10^{-4}(D^2H)^{0.85715}\rho^{0.72864}\)</span>.
Let <code>m</code> be the object containing the fitted model (red line <a href="6-fit.html#exr:fdens">6.25</a>. The adjustment factor <span class="math inline">\(\mathrm{CF}=\exp(\hat{\sigma}^2/2)\)</span> is obtained by the command:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb110-1"><a href="7-util.html#cb110-1"></a>dm &lt;-<span class="st"> </span><span class="kw">tapply</span>(dat<span class="op">$</span>dens, dat<span class="op">$</span>species, mean)</span>
<span id="cb110-2"><a href="7-util.html#cb110-2"></a>dat &lt;-<span class="st"> </span><span class="kw">cbind</span>(dat, <span class="dt">dmoy =</span> dm[<span class="kw">as.character</span>(dat<span class="op">$</span>species)])</span>
<span id="cb110-3"><a href="7-util.html#cb110-3"></a>m &lt;-<span class="st"> </span><span class="kw">lm</span>(</span>
<span id="cb110-4"><a href="7-util.html#cb110-4"></a>    <span class="dt">formula =</span> <span class="kw">log</span>(Btot) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dbh<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>heig)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(dmoy)), </span>
<span id="cb110-5"><a href="7-util.html#cb110-5"></a>    <span class="dt">data =</span> dat[dat<span class="op">$</span>Btot<span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span> ,]</span>
<span id="cb110-6"><a href="7-util.html#cb110-6"></a>    )</span>
<span id="cb110-7"><a href="7-util.html#cb110-7"></a><span class="kw">exp</span>(<span class="kw">summary</span>(m)<span class="op">$</span>sigma<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</span></code></pre></div>
<p>and here is 1.0610348. The correct model is therefore: <span class="math inline">\(B=2.412\times10^{-4}(D^2H)^{0.85715}\rho^{0.72864}\)</span>.</p>
</div>
</div>
<div id="any-transformation" class="section level4 unnumbered">
<h4>Any transformation</h4>
<p>In the general case, let <span class="math inline">\(\psi\)</span> be a variable transformation of the biomass (or volume) such that the response variable <span class="math inline">\(Y=\psi(B)\)</span> may be predicted by a linear regression against the effect variables <span class="math inline">\(X_1,\ \ldots,\ X_p\)</span>. We shall assume that <span class="math inline">\(\psi\)</span> is derivable and invertible. As <span class="math inline">\(\psi(B)\)</span> follows a normal distribution of mean <span class="math inline">\(a_0+a_1X_1+\ldots+a_pX_p\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(B=\psi^{-1}[\psi(B)]\)</span> has an expectation of <span class="citation">(Saporta <a href="bibliography.html#ref-saporta90" role="doc-biblioref">1990</a>, 26)</span>:
<span class="math display" id="eq:EB">\[\begin{equation}
\mathrm{E}(B)=\int\psi^{-1}(x)\;\phi(x)\;\mathrm{d}x\tag{7.15}
\end{equation}\]</span>
where <span class="math inline">\(\phi\)</span> is the probability density of the normal distribution of mean <span class="math inline">\(a_0+a_1X_1+\ldots+a_pX_p\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. This expectation is generally different from <span class="math inline">\(\psi^{-1}(a_0+a_1X_1+\ldots +a_pX_p)\)</span>: the variable transformation induces prediction bias when we return to the starting variable by inverse transformation. The drawback of formula <a href="7-util.html#eq:EB">(7.15)</a> is that it requires the calculation of an integral.</p>
<p>When the residual standard deviation <span class="math inline">\(\sigma\)</span> is small, the <span class="math inline">\(\delta\)</span>-method <span class="citation">(Serfling <a href="bibliography.html#ref-serfling80" role="doc-biblioref">1980</a>)</span> provides an approximate expression of this prediction bias:
<span class="math display">\[\begin{eqnarray*}
\mathrm{E}(\mathrm{B}) &amp;\simeq&amp;
\psi^{-1}[\mathrm{E}(Y)]+\frac{1}{2}\mathrm{Var}(Y)\;
(\psi^{-1})&quot;[\mathrm{E}(Y)]
\\ &amp;\simeq&amp; \psi^{-1}(a_0+a_1X_1+\ldots+a_pX_p)+\frac{\sigma^2}{2}\;
(\psi^{-1})&quot;(a_0+a_1X_1+\ldots+a_pX_p)
\end{eqnarray*}\]</span></p>
</div>
<div id="smearing-estimation" class="section level4 unnumbered">
<h4>“Smearing” estimation</h4>
<p>The “smearing” estimation method is a nonparametric method used to correct prediction bias when an inverse transformation is used on the response variable of a linear model <span class="citation">(Duan <a href="bibliography.html#ref-duan83" role="doc-biblioref">1983</a>; Taylor <a href="bibliography.html#ref-taylor86" role="doc-biblioref">1986</a>; Manning and Mullahy <a href="bibliography.html#ref-manning01" role="doc-biblioref">2001</a>)</span>. Given that we can rewrite biomass (or volume) expectation equation <a href="7-util.html#eq:EB">(7.15)</a> as:
<span class="math display">\[\begin{eqnarray*}
\mathrm{E}(B) &amp;=&amp;
\int\psi^{-1}(x)\;\phi_0(x-a_0-a_1X1-\ldots-a_pX_p)\;\mathrm{d}x
\\ &amp;=&amp;
\int\psi^{-1}(x+a_0+a_1X1+\ldots+a_pX_p)\;\mathrm{d}\Phi_0(x)
\end{eqnarray*}\]</span>
where <span class="math inline">\(\phi_0\)</span> (respectively <span class="math inline">\(\Phi_0\)</span>) is the probability density (respectively the distribution function) of the centered normal distribution of standard deviation <span class="math inline">\(\sigma\)</span>, the smearing method consists in replacing <span class="math inline">\(\Phi_0\)</span> by the empirical distribution function of the residuals from model fitting, i.e.:
<span class="math display">\[\begin{eqnarray*}
B_{\mathrm{smearing}} &amp;=&amp;
\int\psi^{-1}(x+a_0+a_1X_1+\ldots+a_pX_p)\times\frac{1}{n}\sum_{i=1}^n\delta
(x-\hat{\varepsilon}_i)\ \mathrm{d}x
\\ &amp;=&amp; \frac{1}{n}\sum_{i=1}^n\psi^{-1}(a_0+a_1X_1+\ldots+a_pX_p+
\hat{\varepsilon}_i)
\end{eqnarray*}\]</span>
where <span class="math inline">\(\delta\)</span> is Dirac’s function with zero mass and <span class="math inline">\(\hat{\varepsilon}_i\)</span> is the residual of the fitted model for the <span class="math inline">\(i\)</span>th
observation. This method of correcting the prediction bias has the advantage of being both very general and easy to calculate. It has the drawback that the residuals <span class="math inline">\(\hat{\varepsilon}_i\)</span> of model fitting need to be known. This is not a problem when you yourself fit a model to the data, but is a problem when using published volume or biomass tables that are not accompanied by the residuals.</p>
<p>In the particular case of the log transformation, <span class="math inline">\(\psi^{-1}\)</span> is the exponential function, and therefore the smearing estimation of the biomass corresponds to: <span class="math inline">\(\exp(a_0+a_1X_1+\ldots+a_pX_p)\times\mathrm{CF}_{\mathrm{smearing}}\)</span>, where the smearing correction factor is:
<span class="math display">\[
\mathrm{CF}_{\mathrm{smearing}}=\frac{1}{n}\sum_{i=1}^n\exp(\hat{\varepsilon}_i)
\]</span>
Given that <span class="math inline">\(\hat{\sigma}^2=(\sum_{i=1}^n\hat{\varepsilon}_i^2)/(n-p-1)\)</span>, the smearing correction factor is different from correction factor <a href="7-util.html#eq:CF">(7.14)</a>. But, on condition that <span class="math inline">\(\hat{\sigma}\rightarrow0\)</span>, the two adjustment coefficients are equivalent.</p>
<div class="filrouge">
<ol start="36" class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div class="exercise">
<p><span id="exr:smear" class="exercise"><strong>Red line 7.4  </strong></span><span style="color: red;"><strong><span class="math inline">\(\looparrowright\)</span> “Smearing” estimation of biomass</strong></span></p>
</div>
<p>Let us again return to the biomass model in red line <a href="6-fit.html#exr:fdens">6.25</a> fitted by multiple regression on log-transformed data:
<span class="math display">\[
\ln(B)=-8.38900+0.85715\ln(D^2H)+0.72864\ln(\rho)
\]</span>
The smearing correction factor may be obtained by the command:</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r Rchunk"><code class="sourceCode r"><span id="cb111-1"><a href="7-util.html#cb111-1"></a><span class="kw">mean</span>(<span class="kw">exp</span>(<span class="kw">residuals</span>(m)))</span></code></pre></div>
<p>where <code>m</code> is the object containing the fitted model, and in this example is 1.0598586. By way of a comparison, the previously calculated correction factor (red line <a href="7-util.html#exr:CFB">7.3</a>) was 1.061035.</p>
</div>

</div>
</div>
</div>
<div id="predicting-the-volume-or-biomass-of-a-stand" class="section level2">
<h2><span class="header-section-number">7.3</span> Predicting the volume or biomass of a stand</h2>
<p>If we want to predict the volume or biomass of a stand using biomass tables, we cannot measure model entries for all the trees in the stand, only for a sample. The volume or biomass of the trees in this sample will be calculated using the model, then extrapolated to the entire stand. Predicting the volume or biomass of a stand therefore includes two sources of variability: one related to the model’s individual prediction, the other to the sampling of the trees in the stand. If we rigorously take account of these two sources of variability when making predictions on a stand scale, this will cause complex double sampling problems as mentioned in sections <a href="2-samp.html#peup">2.1.2</a> and <a href="2-samp.html#sampeup">2.3</a> <span class="citation">(Parresol <a href="bibliography.html#ref-parresol99" role="doc-biblioref">1999</a>)</span>.</p>
<p>The problem is slightly less complex when the sample of trees used to construct the model is independent of the sample of trees in which the entries were measured. In this case we can consider that the prediction error stemming from the model is independent of the sampling error. Let us assume that <span class="math inline">\(n\)</span> sampling plots of unit area <span class="math inline">\(A\)</span> were selected in a stand whose total area is <span class="math inline">\(\mathcal{A}\)</span>. Let <span class="math inline">\(N_i\)</span> be the number of trees found in the <span class="math inline">\(i\)</span>th plot (<span class="math inline">\(i=1,\ \ldots,\ n\)</span>) and let <span class="math inline">\(X_{ij1},\ \ldots,\ X_{ijp}\)</span> be the <span class="math inline">\(p\)</span> effect variables measured on the <span class="math inline">\(j\)</span>th tree of the <span class="math inline">\(i\)</span>th plot (<span class="math inline">\(j=1,\ \ldots,\ N_i\)</span>). <span class="citation">Cunia (<a href="bibliography.html#ref-cunia65" role="doc-biblioref">1965</a>)</span>; <span class="citation">Cunia (<a href="bibliography.html#ref-cunia87c" role="doc-biblioref">1987</a><a href="bibliography.html#ref-cunia87c" role="doc-biblioref">d</a>)</span> considered the particular case where biomass is predicted by multiple regression based on the <span class="math inline">\(p\)</span> effect variables. The estimation of stand biomass is in this case:
<span class="math display">\[\begin{eqnarray*}
\hat{B} &amp;=&amp;
\frac{\mathcal{A}}{n}\sum_{i=1}^n\frac{1}{A}\sum_{j=1}^{N_i}
(\hat{a}_0+\hat{a}_1X_{ij1}+\ldots+\hat{a}_pX_{ijp})
\\ &amp;=&amp; \hat{a}_0\Big(\frac{\mathcal{A}}{nA}\sum_{i=1}^nN_i\Big)
+\hat{a}_1\Big(\frac{\mathcal{A}}{nA}\sum_{i=1}^n\sum_{j=1}^{N_i}X_{ij1}\Big)
+\ldots+\hat{a}_p\Big(\frac{\mathcal{A}}{nA}\sum_{i=1}^n\sum_{j=1}^{N_i}X_{ijp}\Big)
\end{eqnarray*}\]</span>
where <span class="math inline">\(\hat{a}_0,\ \ldots,\ \hat{a}_p\)</span> are the estimated coefficients of the regression. If <span class="math inline">\(X^{\star}_0=(\mathcal{A}/nA)\sum_{i=1}^nN_i\)</span> and for each <span class="math inline">\(k=1,\ \ldots,\ p\)</span>,
<span class="math display">\[
X^{\star}_k=\frac{\mathcal{A}}{nA}\sum_{i=1}^n\sum_{j=1}^{N_i}X_{ijk}
\]</span>
The estimated biomass of the stand is therefore
<span class="math display">\[
\hat{B}=\hat{a}_0X^{\star}_0+\hat{a}_1X^{\star}_1+\ldots+\hat{a}_pX^{\star}_p
\]</span>
Interestingly, the variability of <span class="math inline">\(\hat{\mathbf{a}}={}^{\mathrm{t}}[\hat{a}_0,\ \ldots,\ \hat{a}_p]\)</span> depends entirely on model fitting, not on stand sampling, while on the other hand, the variability of <span class="math inline">\(\mathbf{x}={}^{\mathrm{t}}[X^{\star}_0,\ \ldots,\ X^{\star}_p]\)</span> depends entirely on the sampling, not on the model. Given that these two errors are independent,
<span class="math display">\[
\mathrm{E}(\hat{B})=\mathrm{E}({}^{\mathrm{t}}{\hat{\mathbf{a}}}\mathbf{x})={}^{\mathrm{t}}{\mathrm{E}(\hat{\mathbf{a}})}\ \mathrm{E}(\mathbf{x})
\]</span>
and
<span class="math display">\[
\mathrm{Var}(\hat{B})={}^{\mathrm{t}}{\mathbf{a}}\boldsymbol{\Sigma}_{\mathbf{x}}
\mathbf{a}+{}^{\mathrm{t}}{\mathbf{x}}\boldsymbol{\Sigma}_{\mathbf{a}}\mathbf{x}
\]</span>
where <span class="math inline">\(\boldsymbol{\Sigma}_{\mathbf{a}}\)</span> is the <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix of the model’s coefficients while <span class="math inline">\(\boldsymbol{\Sigma}_{\mathbf{x}}\)</span> is the <span class="math inline">\((p+1)\times(p+1)\)</span> variance-covariance matrix of the sample of <span class="math inline">\(\mathbf{x}\)</span>. The first matrix is derived from model fitting whereas the second is derived from stand sampling. Thus, the error for the biomass prediction of a stand is the sum of these two terms, one of which is related to the prediction error of the model and the other to stand sampling error.</p>
<p>More generally, the principle is exactly the same as when we considered in section <a href="7-util.html#errmes">Confidence interval with measurement uncertainties</a>, an uncertainty related to the measurement of the effect variables <span class="math inline">\(X_1,\ \ldots,\ X_p\)</span>. A measurement error is not of the same nature as a sampling error. But from a mathematical standpoint, the calculations are the same: in both cases this means considering that the effect variables <span class="math inline">\(X_1,\ \ldots,\ X_p\)</span> are random, not fixed. We may therefore, in this general case, use a Monte Carlo method to estimate stand biomass. The pseudo-algorithm of this Monte Carlo method is the same as that used previously :</p>
<ol style="list-style-type: decimal">
<li>For <span class="math inline">\(k\)</span> from <span class="math inline">\(1\)</span> to <span class="math inline">\(Q\)</span>, where <span class="math inline">\(Q\)</span> is the number of Monte Carlo iterations:
<ul>
<li>1.1. for <span class="math inline">\(i\)</span> from 1 to <span class="math inline">\(p\)</span>, draw <span class="math inline">\(\hat{X}_i^{(k)}\)</span> following a distribution corresponding to the variability of the stand sample (this distribution depends on the type of sampling conducted, the size and number of plots inventoried, etc.);</li>
<li>1.2. draw a vector <span class="math inline">\(\hat{\theta}^{(k)}\)</span> following a multinormal distribution of mean <span class="math inline">\(\hat{\theta}\)</span> and variance-covariance matrix <span class="math inline">\(\hat{\boldsymbol{\Sigma}}\)</span>;</li>
<li>1.3. calculate the prediction <span class="math inline">\(\hat{Y}^{(k)}=f(\hat{X}_1^{(k)},\ \ldots,\ \hat{X}_p^{(k)};\hat{\theta}^{(k)})\)</span>.</li>
</ul></li>
<li>The confidence interval of the prediction is the empirical confidence interval of the <span class="math inline">\(Q\)</span> values <span class="math inline">\(\hat{Y}^{(1)}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\hat{Y}^{(Q)}\)</span>.</li>
</ol>

</div>
<div id="BEF" class="section level2">
<h2><span class="header-section-number">7.4</span> Expanding and converting volume and biomass models</h2>
<p>It may happen that we have a model which predicts a variable that is not exactly the one we need, but is closely related to it. For example, we may have a model that predicts the dry biomass of the trunk, but we are looking to predict the total above-ground biomass of the tree. Or, we may have a model that predicts the volume of the trunk, but we are looking to predict its dry biomass. Rather than deciding not to use a model simply because it does not give us exactly what we want, we can use it by default and adjust it using a factor. We can use <em>conversion</em> factors to convert volume to biomass (and <em>vice versa</em>), <em>expansion</em> factors to extrapolate a part to the whole, or combinations of the two. Using this approach <span class="citation">Henry et al. (<a href="bibliography.html#ref-henry11" role="doc-biblioref">2011</a>)</span> put forward three methods to obtain the total biomass:</p>
<ul>
<li>The biomass of the trunk is the product of its volume and specific wood density <span class="math inline">\(\rho\)</span>;</li>
<li>above-ground biomass is the product of trunk biomass and a biomass expansion factor (BEF);</li>
<li>above-ground biomass is the product of trunk volume and a biomass expansion and conversion factor (BCEF = BEF<span class="math inline">\(\times\rho\)</span>).</li>
</ul>
<p>Tables of these different conversion and expansion factors are available. These values are often very variable as they implicitly include difference sources of variability. Although the default table may be very precise, we often loose the benefit of this precision when using an expansion or correction factor as the prediction error cumulates all the sources of error involved in its calculation.</p>
<p>If we have a table that uses height as entry value, but do not have any height information, we may use a corollary model that predicts height from the entry values we do have (typically a height-dbh relation). Like for conversion and expansion factors, this introduces an additional source of error.</p>

</div>
<div id="arbi" class="section level2">
<h2><span class="header-section-number">7.5</span> Arbitrating between different models</h2>
<p>If we want to predict the volume or biomass of given trees, we often have a choice between different models. For instance, several models may have been fitted in different places for a given species. Or we may have a local model and a pan-tropical model. Arbitrating between the different models available is not always an easy task <span class="citation">(Henry et al. <a href="bibliography.html#ref-henry11" role="doc-biblioref">2011</a>)</span>. For example, should we choose a specific, local model fitted using little data (and therefore unbiased <em>a priori</em> but with high prediction variability) or a pan-tropical, multispecific model fitted with a host of data (therefore potentially biased but with low prediction variability)? This shows how many criteria are potentially involved when making a choice: model quality (the range of its validity, its capacity to extrapolate predictions, etc.), its specificity (with local, monospecific models at one extreme and pan-tropical, multispecific models at the other), the size of the dataset used for model fitting (and therefore implicitly the variability of its predictions). Arbitrating between different models should not be confused with model selection, as described in section <a href="6-fit.html#selmod">6.3.2</a> for when selecting models their coefficients are not yet known, and when we estimate these coefficients we are looking for the model that best fits the data. When arbitrating between models we are dealing with models already fitted and whose coefficients are known.</p>
<p>Arbitration between different models often takes place without any biomass or volume data. But the case we will focus on here is when we have a reference dataset <span class="math inline">\(\mathcal{S}_n\)</span>, with <span class="math inline">\(n\)</span> observations of the response variable (volume of biomass) and the effect variables.</p>
<div id="comparing-using-validation-criteria" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Comparing using validation criteria</h3>
<p>When we have a dataset <span class="math inline">\(\mathcal{S}_n\)</span>, we can compare the different models available on the basis of the validation criteria described in section <a href="7-util.html#Ival">7.1.1</a>, by using <span class="math inline">\(\mathcal{S}_n\)</span> as the validation dataset. Given that the models do not necessarily have the same number <span class="math inline">\(p\)</span> of parameters, and in line with the parsimony principle, we should prefer the validation criteria that depend on <span class="math inline">\(p\)</span> in such a manner as to penalize those models with many parameters.</p>
<p>When comparing a particular, supposedly “best” candidate model with different competitors, we can compare its predictions with those of its competitors. To do this we look to see whether or not the predictions made by its competitors are contained within the confidence interval at a significance level of <span class="math inline">\(\alpha\)</span> of the candidate model.</p>
</div>
<div id="choosing-a-model" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Choosing a model</h3>
<p>A model may be chosen with respect to a “true” model <span class="math inline">\(f\)</span> that although unknown may be assumed to exist. Let <span class="math inline">\(M\)</span> be the number of models available. Let us note as <span class="math inline">\(\hat{f}_m\)</span> the function of the <span class="math inline">\(p\)</span> effect variables that predicts volume or biomass based on the <span class="math inline">\(m\)</span>th model. This function is random as it depends on coefficients that are estimated and therefore have their own distribution. The distribution of <span class="math inline">\(\hat{f}_m\)</span> therefore describes the variability of the predictions based on the model <span class="math inline">\(m\)</span>, as described in section <a href="7-util.html#BVpred">7.2</a>. The <span class="math inline">\(M\)</span> models may have very different forms: the <span class="math inline">\(\hat{f}_1\)</span> model may correspond to a power function, the <span class="math inline">\(\hat{f}_2\)</span> model to a polynomial function, etc. We will also assume that there is a function <span class="math inline">\(f\)</span> of the <span class="math inline">\(p\)</span> effect variables that describes the “true” relation between the response variable (volume or biomass) and these effect variables. We do not know this “true” relation. We do not know what form it has. But each of the <span class="math inline">\(M\)</span> models may be considered to be an approximation of the “true” relation <span class="math inline">\(f\)</span>.</p>
<p>In the models selection theory <span class="citation">(Massart <a href="bibliography.html#ref-massart07" role="doc-biblioref">2007</a>)</span>, the difference between the relation <span class="math inline">\(f\)</span> and a model <span class="math inline">\(\hat{f}_m\)</span> is quantified by a function <span class="math inline">\(\gamma\)</span> that we call the <em>loss</em> function. For example, the loss function may be the norm <span class="math inline">\(L^2\)</span> of the difference between <span class="math inline">\(f\)</span> and <span class="math inline">\(\hat{f}_m\)</span>:
<span class="math display">\[
\gamma(f,\ \hat{f}_m)=\int_{x_1}\ldots\int_{x_p}
[f(x_1,\ \ldots,\ x_p)-\hat{f}_m(x_1,\ \ldots,\ x_p)]^2
\ \mathrm{d}x_1\ldots\mathrm{d}x_p
\]</span>
The expectation of the loss with respect to the distribution of <span class="math inline">\(\hat{f}_m\)</span> is called the <em>risk</em> (written <span class="math inline">\(R\)</span>) i.e. when integrating on the variability of model predictions:
<span class="math display">\[
R=\mathrm{E}[\gamma(f,\ \hat{f}_m)]
\]</span>
The best of the <span class="math inline">\(M\)</span> models available is that which minimizes the risk. The problem is that we do not know the true relation <span class="math inline">\(f\)</span>, therefore, this “best” model is also unknown. In the models selection theory this best model is called an <em>oracle</em>. The model finally chosen will be that such that the risk of the oracle is bounded for a vast family of functions <span class="math inline">\(f\)</span>. Intuitively, the model chosen is that where the difference between it and the “true” relation is limited whatever this “true” relation may be (within the limits of a range of realistic possibilities). We will not go into further detail here concerning this topic as it is outside the scope of this guide.</p>
</div>
<div id="bayesian-model-averaging" class="section level3">
<h3><span class="header-section-number">7.5.3</span> Bayesian model averaging</h3>
<p>Rather than choose one model from the <span class="math inline">\(M\)</span> available, with the danger of not choosing the “best”, an alternative consists in combining the <span class="math inline">\(M\)</span> competitor models to form a new model. This is called “Bayesian model averaging” and although it has been very widely used for weather forecasting models <span class="citation">(Raftery et al. <a href="bibliography.html#ref-raftery05" role="doc-biblioref">2005</a>; Furrer et al. <a href="bibliography.html#ref-furrer07" role="doc-biblioref">2007</a>; Berliner and Kim <a href="bibliography.html#ref-berliner08" role="doc-biblioref">2008</a>; Smith et al. <a href="bibliography.html#ref-smith09" role="doc-biblioref">2009</a>)</span> it is little used for forestry models <span class="citation">(Li, Andersen, and McGaughey <a href="bibliography.html#ref-li08" role="doc-biblioref">2008</a>; Picard et al. <a href="bibliography.html#ref-picard12" role="doc-biblioref">2012</a>)</span>. Let <span class="math inline">\(\mathcal{S}_n=\{(Y_i,\ X_{i1},\ \ldots,\ X_{ip}),\ i=1,\ldots,p\}\)</span> be a reference dataset with <span class="math inline">\(n\)</span> observations of the response variable <span class="math inline">\(Y\)</span> and the <span class="math inline">\(p\)</span> effect variables. Bayesian model averaging considers that the distribution of the response variable <span class="math inline">\(Y\)</span> is a mixture of <span class="math inline">\(M\)</span> distributions:
<span class="math display">\[
g(Y|X_1,\ \ldots,\ X_p)=\sum_{m=1}^Mw_m\ g_m(Y|X_1,\ \ldots,\ X_p)
\]</span>
where <span class="math inline">\(g\)</span> is the distribution density of <span class="math inline">\(Y\)</span>, <span class="math inline">\(g_m\)</span> is the conditional distribution density of <span class="math inline">\(Y\)</span> with model <span class="math inline">\(m\)</span> being the “best”, and <span class="math inline">\(w_m\)</span> is the weight of the <span class="math inline">\(m\)</span>th model in the mixture, that we may interpret as the <em>a posteriori</em> probability that the <span class="math inline">\(m\)</span>th model is the “best”. The <em>a posteriori</em> <span class="math inline">\(w_m\)</span> probabilities are reflections of the quality of the fitting of the models to the data, and their sum is one: <span class="math inline">\(\sum_{m=1}^Mw_m=1\)</span>.</p>
<p>In the same manner as for the model selection mentioned in the previous section, Bayesian model averaging assumes that there is a “true” (though unknown) relation between the response variable and the <span class="math inline">\(p\)</span> effect variables, and that each model differs from this “true” relation according to a normal distribution of standard deviation <span class="math inline">\(\sigma_m\)</span>. In other words, density <span class="math inline">\(g_m\)</span>
is the density of the normal distribution of mean <span class="math inline">\(f_m(x_1,\ \ldots,\ x_p)\)</span> and standard deviation <span class="math inline">\(\sigma_m\)</span>, where <span class="math inline">\(f_m\)</span> is the function of <span class="math inline">\(p\)</span> variables corresponding to the <span class="math inline">\(m\)</span>th model. Thus,
<span class="math display">\[
g(Y|X_1,\ \ldots,\ X_p)=\sum_{m=1}^Mw_m\ \phi(Y;f_m(x_1,\ \ldots,\ x_p),\;\sigma_m)
\]</span>
where <span class="math inline">\(\phi(\cdot;\mu,\ \sigma)\)</span> is the probability density of the normal distribution of expectation <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. The model <span class="math inline">\(f_{\mathrm{mean}}\)</span> resulting from the combination of the <span class="math inline">\(M\)</span> competitor models is defined as the expectation of the mixture model, i.e.:
<span class="math display">\[
f_{\mathrm{mean}}(X_1,\ \ldots,\ X_p)=\mbox{E}(Y|X_1,\ \ldots,\ X_p)=\sum_{m=1}^Mw_m\ 
f_m(X_1,\ \ldots,\ X_p)
\]</span>
Thus, the model resulting from the combination of the <span class="math inline">\(M\)</span> competitor models is a weighted mean of these <span class="math inline">\(M\)</span> models, with the weight of model <span class="math inline">\(m\)</span> being the <em>a posteriori</em> probability that model <span class="math inline">\(m\)</span> is the best. We can also calculate the variance of the predictions made by model <span class="math inline">\(f_{\mathrm{mean}}\)</span> that is a combination of the <span class="math inline">\(M\)</span> competitor models:
<span class="math display">\[\begin{eqnarray*}
\mbox{Var}(Y|X_1,\ \ldots,\ X_p) &amp;=&amp; \sum_{m=1}^Mw_m\Big[
f_m(X_1,\ \ldots,\ X_p)-\sum_{l=1}^Mw_l\ f_l(X_1,\ \ldots,\ X_p)\Big]^2
\\ &amp;&amp; +\sum_{m=1}^Mw_m\ \sigma_m^2
\end{eqnarray*}\]</span>
The first term corresponds to between-model variance and expresses prediction variability from one model to another; the second term corresponds to within-model variance, and expresses the conditional error of the prediction, with this model being the best.</p>
<p>If we want to use model <span class="math inline">\(f_{\mathrm{mean}}\)</span> instead of the <span class="math inline">\(M\)</span> models <span class="math inline">\(f_1,\ \ldots,\ f_M\)</span>, we now have to estimate the weights
<span class="math inline">\(w_1,\ \ldots,\ w_M\)</span> and the within-model standard deviations <span class="math inline">\(\sigma_1,\ \ldots,\ \sigma_M\)</span>. These <span class="math inline">\(2M\)</span> parameters are estimated from reference dataset <span class="math inline">\(\mathcal{S}_n\)</span> using an EM algorithm <span class="citation">(Dempster, Laird, and Rubin <a href="bibliography.html#ref-dempster77" role="doc-biblioref">1977</a>; McLachlan and Krishnan <a href="bibliography.html#ref-mclachlan08" role="doc-biblioref">2008</a>)</span>. The EM algorithm introduces latent variables <span class="math inline">\(z_{im}\)</span> such that <span class="math inline">\(z_{im}\)</span> is the <em>a posteriori</em> probability that model <span class="math inline">\(m\)</span> is the best model for observation <span class="math inline">\(i\)</span> of <span class="math inline">\(\mathcal{S}_n\)</span>. The latent variables <span class="math inline">\(z_{im}\)</span> take values of between 0 and 1. The EM algorithm is iterative and alternates between two steps at each iteration: step E (as in “expectation”) and step M (as “maximisation”). The EM algorithms is as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Choose start values <span class="math inline">\(w^{(0)}_1,\ \ldots,\ w^{(0)}_M\)</span>, <span class="math inline">\(\sigma^{(0)}_1,\ \ldots,\ \sigma^{(0)}_M\)</span> for the <span class="math inline">\(2M\)</span> parameters to be estimated.</p></li>
<li><p>Alternate the two steps:</p>
<ul>
<li>2.1. step E: calculate the value of <span class="math inline">\(z_{im}\)</span> at iteration
<span class="math inline">\(j\)</span> sbased on the values of the parameters at iteration <span class="math inline">\(j-1\)</span>:
<span class="math display">\[
 z_{im}^{(j)}=\frac{w_m^{(j-1)}\ \phi[Y_i;f_m(X_{i1},\ \ldots,\ X_{ip}),
 \ \sigma_m^{(j-1)}]}{\sum_{k=1}^Mw_k^{(j-1)}\ \phi[Y_i;f_k(X_{i1},\ \ldots,\ X_{ip}),
 \ \sigma_k^{(j-1)}]}
 \]</span></li>
<li>2.2. step M: estimate the parameters at iteration <span class="math inline">\(j\)</span> using current values of <span class="math inline">\(z_{im}\)</span> as weight, i.e.:
<span class="math display">\[\begin{eqnarray*}
 w_m^{(j)} &amp;=&amp; \frac{1}{n}\sum_{i=1}^nz_{im}^{(j)}\\ %
 {\sigma_m^{(j)}}^2 &amp;=&amp;
 \frac{\sum_{i=1}^nz_{im}^{(j)}[Y_i-f_m(X_{i1},\ \ldots,\ X_{ip})]^2}
 {\sum_{i=1}^nz_{im}^{(j)}}
 \end{eqnarray*}\]</span>
such as <span class="math inline">\(\sum_{m=1}^M|w_m^{(j)}-w_m^{(j-1)}|+\sum_{m=1}^M|\sigma_m^{(j)}-\sigma_m^{(j-1)}|\)</span> is larger than a fixed infitesimal threshold (for example <span class="math inline">\(10^{-6}\)</span>).</li>
</ul></li>
<li><p>The estimated value of <span class="math inline">\(w_m\)</span> is <span class="math inline">\(w_m^{(j)}\)</span> and the estimated value of <span class="math inline">\(\sigma_m\)</span> is <span class="math inline">\(\sigma_m^{(j)}\)</span>.</p></li>
</ol>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="6-fit.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusions-and-recommendations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": true,
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
